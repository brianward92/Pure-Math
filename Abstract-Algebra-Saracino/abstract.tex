\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{color}
\usepackage{graphicx}
\usepackage[margin = 0.5in]{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{float}
\usepackage{rotating}
\usepackage{url}
\usepackage[font=small]{caption}
\usepackage{setspace}
\usepackage{subfigure}
\usepackage[dvipsnames]{xcolor}
\setcounter{MaxMatrixCols}{10}
\setcounter{tocdepth}{1}
\newtheorem{theorem}{Theorem}
\newtheorem{algorithm}{Algorithm} %[theorem]
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{exercise}{Exercise}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

%\newtheorem{acknowledgement}[theorem]{Acknowledgement}
%\newtheorem{axiom}[theorem]{Axiom}
%\newtheorem{case}[theorem]{Case}
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{conclusion}[theorem]{Conclusion}
%\newtheorem{condition}[theorem]{Condition}
%\newtheorem{conjecture}[theorem]{Conjecture}
%\newtheorem{criterion}[theorem]{Criterion}
%\newtheorem{assumption}[theorem]{Assumption}
%\newtheorem{notation}[theorem]{Notation}
%\newtheorem{problem}[theorem]{Problem}
%\newtheorem{solution}[theorem]{Solution}
%\newtheorem{summary}[theorem]{Summary}
	
\def\N{{\mathbb N}}        % positive integers
\def\Q{{\mathbb Q}}        % rationals
\def\Z{{\mathbb Z}}        % integers
\def\R{{\mathbb R}}        % reals
\def\Rn{{\R^{n}}}          % product of n copies of reals
\def\P{{\mathbb P}}        % probability
\def\E{{\mathbb E}}        % expectation
\def\EQ{{\mathbb E}^{\mathbb Q}}        % expectation
\def\1{{\mathbf 1}}        % indicator
\def\F{{\mathcal F}}        % potential measure
\def\G{{\mathcal G}}        % potential measure
\def\ess{\text{ess}}
\def\var{{\mathop{\mathbf Var}}}    %variance
\def\L{{\mathcal L} \,}
\def\Lhat{{\tilde{\mathcal L} \,}}
\def\setZ{{\mathcal Z}}
\def\setA{{\mathcal A}}
\def\setT{{\mathcal T}}
\def\D{{\mathcal D}}
\def\C{{\mathcal C}}
\def\setE{{\mathcal E}}
\def\Vest{{\mathcal V}}
\def\Cvest{{C}}
\def\Cunvest{{\tilde{C}}}

\def\I{{\mathbf I}}
\def\thetahat{{\hat{\theta}}}
\def\taub{{\hat{\tau}}}
\def\xhat{{\hat{x}}}
\def\xbar{{\bar{x}}}
\def\yhat{{\hat{y}}}
\def\x{{\textcolor[rgb]{0.00,0.00,0.00}{\hat{x}}}}
\def\y{{\textcolor[rgb]{0.00,0.00,0.00}{\hat{y}}}}
\def\v{{\hat{v}}}
\def\Xhat{{\hat{X}}}
\def\G{{\tilde{G}}}
\def\H{{\tilde{H}}}
\def\Yhat{{\hat{Y}}}
\def\Vhat{{\hat{V}}}
\def\Mhat{{\hat{M}}}

%\addtolength{\hoffset}{-1.8cm} \addtolength{\voffset}{-2cm}
%\addtolength{\textheight}{4cm} \addtolength{\textwidth}{3.6cm}

\newcommand{\ward}[1]{\textcolor{red}{#1}}

\numberwithin{theorem}{section}
\numberwithin{equation}{section}
\numberwithin{remark}{section}
\numberwithin{definition}{section}
\numberwithin{theorem}{section}
\numberwithin{lemma}{section}
\numberwithin{example}{section}

\begin{document}
\title{Solutions Guide to Abstract Algebra}
\author{Brian Ward\thanks{Email: {bmw2150@columbia.edu}. Corresponding author. }} 
\maketitle
\abstract{Solutions to the textbook ``Abstract Algebra: A First Course", Second Edition by Dan Saracino.}

\tableofcontents

\newpage

\setcounter{section}{-1}

\section{Sets and Induction}



\subsection{Q1}
With $S = \{2,5,\sqrt{2},25,\pi,5/2\}$ and $T=\{4,25,\sqrt{2},6,3/2\}$, we have 
\begin{align*}
	S\cap T = \{\sqrt{2},25\},
\end{align*}
and 
\begin{align*}
	S\cup T = \{2,5,\sqrt{2},25,\pi,5/2,4,6,3/2\}.
\end{align*}



\subsection{Q2}
For the first equation, the left hand side is
\begin{align*}
	\mathbb{Z} \cap \left(S\cup T \right) = \{2,5,25,4,6\}.
\end{align*}
As for the right hand side, we have $\mathbb{Z}\cap S = \{2,5,25\}$. and $\mathbb{Z}\cap T = \{4,25,6\}$. Thus, 
\begin{align*}
	\left(\mathbb{Z}\cap S\right)\cup\left(\mathbb{Z}\cap T\right) = \{2,5,25\} \cup \{4,25,6\} = \{2,5,25,4,6\}.
\end{align*}
For the second equation, the left hand side is
\begin{align*}
	\mathbb{Z} \cup \left(S\cap T \right) = \mathbb{Z} \cup \{\sqrt{2},25\} = \{\sqrt{2},\ldots,-3,-2,-1,0,1,2,3,\ldots\}.
\end{align*}
As for the right hand side we have 
\begin{align*}
	\mathbb{Z} \cup S = \mathbb{Z} \cup  \{2,5,\sqrt{2},25,\pi,5/2\} = \{\sqrt{2},\pi,5/2,\ldots,-3,-2,-1,0,1,2,3,\ldots\},
\end{align*}
and
\begin{align*}
	\mathbb{Z} \cup T = \mathbb{Z} \cup \{4,25,\sqrt{2},6,3/2\} = \{\sqrt{2},3/2,\ldots,-3,-2,-1,0,1,2,3,\ldots\}.
\end{align*}
Thus,
\begin{align*}
	\left(\mathbb{Z}\cup S\right)\cap\left(\mathbb{Z}\cup T\right) = \{\sqrt{2},\ldots,-3,-2,-1,0,1,2,3,\ldots\}.
\end{align*}



\subsection{Q3}
For the first equation, we prove (i) $S\cap\left(S\cup T\right)\subseteq S$ and (ii) $S\subseteq S\cap\left(S\cup T\right)$. 
\begin{itemize}
	\item[(i)]{Suppose $x\in S\cap\left(S\cup T\right)$. Because an element is in an intersection whenever it is in both sets of the intersection, we have $x\in S$ and $x\in S\cup T$. Of course, the first suffices for $S\cap\left(S\cup T\right)\subseteq S$.}
	\item[(ii)]{Suppose $x\in S$. Then $x\in S \cup T$ as well because an element is in a union if it is in at least one of the two sets in that union. Since $x\in S$ and $x\in S \cup T$, we have $x\in S\cap\left(S\cup T\right)$ so $S\subseteq S\cap\left(S\cup T\right)$.}
\end{itemize}
For the second equation, we prove (iii) $S\cup\left(S\cap T\right)\subseteq S$ and (iv) $S\subseteq S\cup\left(S\cap T\right)$. 
\begin{itemize}
	\item[(iii)]{Suppose $x\in S\cup\left(S\cap T\right)$. Then either (a) $x\in S$ or (b) $x\notin S$. In case (a) we clearly have $S\cup\left(S\cap T\right)\subseteq S$. In case (b) we must have $x\in S\cap T$ (if $x\not \in S\cap T$, then $x$ is in neither $S$ nor $S\cap T$, therefore not in $S\cup\left(S\cap T\right)$, which contradicts our assumption $x\in S\cup\left(S\cap T\right)$.) This implies case (b) is not possible. $x\in S\cap T$ implies $x\in S$ and $x\in T$, contradicting that $x\notin S$. Since cases (a) and (b) are mutually exclusive and exhaustive we have shown $S\cup\left(S\cap T\right)\subseteq S$.}
	\item[(iv)]{Suppose $x\in S$. Then $x\in S\cup\left(S\cap T\right)$ as well because an element is in a union if it is in at least one of the two sets in that union. Thus, we have $S\subseteq S\cup\left(S\cap T\right)$.}
\end{itemize}



\subsection{Q4}
\noindent ($\implies$) 

Suppose that $S \cup T = T$. We must show $S \subseteq T$. Suppose $x\in S$. Then we have $x \in S \cup T$. As $S \cup T = T$, this implies $x\in T$. Thus, $S \cup T = T \implies S \subseteq T$.

\noindent ($\impliedby$)

Suppose that $S \subseteq T$. We must show that $S \cup T = T$. Thus, we show (i) $S \cup T \subseteq T$ and (ii) $T \subseteq S \cup T$.
\begin{itemize}
	\item[(i)]{Suppose $x\in S \cup T$. Then, either (a) $x\in S$ or (b) $x\notin S$. In case (a) because we assume $S \subseteq T$, we have $x \in T$. In case (b) we must have $x \in T$ because otherwise $x\notin S$ and $x\notin T$ so $x$ could not be in $S \cup T$. In both cases we have shown $x\in T$ so we have $S \cup T \subseteq T$.}
	\item[(ii)]{Suppose $x \in T$. Then we know $x \in S \cup T$ (because it is in one of the sets in the union) so $T \subseteq S \cup T$.}
\end{itemize}
Together (i) and (ii) imply $S \cup T = T$ so $S \subseteq T \implies S \cup T = T$.



\subsection{Q5}

We show (i) $A \cap \left(B \cup C\right) \subseteq \left(A \cap B\right) \cup  \left(A \cap C\right)$ and (ii) $\left(A \cap B\right) \cup  \left(A \cap C\right) \subseteq A \cap \left(B \cup C\right)$.
\begin{itemize}
	\item[(i)]{Suppose $x\in A \cap \left(B \cup C\right)$. Then $x\in A$ and $x \in B \cup C$. Either (a) $x\in B$ or (b) $x\notin B$. In case (a) we have $x\in A$ and $x\in B$ so $x\in A \cap B$. In case (b) we must have $x\in C$ (similar to previous arguments) so $x\in A$ and $x \in C$ implying $x \in A \cap C$. In either case we have shown $x$ is in one of the sets of the union $\left(A \cap B\right) \cup  \left(A \cap C\right)$ so $A \cap \left(B \cup C\right) \subseteq \left(A \cap B\right) \cup  \left(A \cap C\right)$.}
	\item[(ii)]{Suppose $x \in \left(A \cap B\right) \cup  \left(A \cap C\right)$. Either (a) $x \in A \cap B$ or (b) $x \notin A \cap B$. In case (a) we have $x\in A$ and $x\in B$. In case (b) we must have $x \in A \cap C$ (similar to previous arguments) so that $x\in A$ and $x\in C$. In either case $x\in A$ and $x$ is either in $B$ or $C$ so that $x\in B \cup C$. Together we have $x\in A \cap \left(B \cup C\right)$ so $\left(A \cap B\right) \cup  \left(A \cap C\right) \subseteq A \cap \left(B \cup C\right)$.}
\end{itemize}



\subsection{Q6}

We show (i) $A \cup \left(B \cap C\right) \subseteq \left(A \cup B\right) \cap  \left(A \cup C\right)$ and (ii) $\left(A \cup B\right) \cap  \left(A \cup C\right) \subseteq A \cup \left(B \cap C\right)$.
\begin{itemize}
	\item[(i)]{Suppose $x\in A \cup \left(B \cap C\right)$. Then either (a) $x\in A$ or (b) $x \notin A$. In case (a) $x\in A$ implies $x\in A \cup B$ and $x \in A \cup C$ so $\left(A \cup B\right) \cap  \left(A \cup C\right)$. In case (b) we must have $x\in B \cap C$ (similar to previous arguments) so $x\in B$ and $x\in C$. That implies $x \in A \cup B$ and $x \in A \cup C$, respectively. In either case, we have $x \in A \cup B$ and $x \in A \cup C$ so $x\in \left(A \cup B\right) \cap  \left(A \cup C\right)$. That means $A \cup \left(B \cap C\right) \subseteq \left(A \cup B\right) \cap  \left(A \cup C\right)$.}
	\item[(ii)]{Suppose $x\in \left(A \cup B\right) \cap  \left(A \cup C\right)$. Then $x\in A \cup B$ and $x\in A \cup C$. Either (a) $x\in A$ or (b) $x\notin A$. In case (a) $x\in A$ implies $x\in A \cup \left(B \cap C\right)$. In case (b) we have $x\notin A$, but $x\in A \cup B$ and $x\in A \cup C$. The last two facts respectively imply $x\in B$ and $x\in C$ (otherwise $x$ could not be in those two unions) so $x\in B \cap C$ so that $x \in A \cup \left(B \cap C\right)$. Thus, $\left(A \cup B\right) \cap  \left(A \cup C\right) \subseteq A \cup \left(B \cap C\right)$.}
\end{itemize}



\subsection{Q7}

The key problem in the proof is the requirement that the subsets overlap. In particular, the book's proof has horses labeled $h_1,h_2,\ldots,h_{m},h_{m+1}$ and considers two subsets of size $m$. Subset 1 is $\{h_1,h_2,\ldots,h_m\}$ and subset 2 is $\{h_2,\ldots,h_m,h_{m+1}\}$. The intersection of these two sets is $S:=\{h_2,\ldots,h_m\}$. We know from the fact that $S$ is in subset 1, that $S$ are all of the same color, say $C_1$. Moreover, this is the color of $h_1$. We also know from the fact that $S$ is in subset 2, that $S$ are all of the same color, say $C_2$. Moreover, this is the color of $h_{m+1}$. Of course, we have just concluded $S$ has color $C_1$ \emph{and} color $C_2$ so $C_1=C_2$. Finally, that indicates $h_1$'s color, $C_1$ must equal that of $h_{m+1}$'s color, $C_2$ and so all $m+1$ horses are the same color. 

However, $S$ is empty when $m=1$ so this first inductive step cannot be carried forward. Intuitively, If I have a group of two horses and I know that all subsets of size less than two are groups of the same color, it does not imply both horses are the same color. For example, if I have one white horse and one black horse then the inductive hypothesis is satisfied by this collection of horses: any subset of size less than two (i.e. a subset of size one) is a group of horses of the same color (pick any individual horse, it is the same color as itself). However, it is obviously not true that the two horses are the same color in spite of the inductive hypothesis holding. 



\subsection{Q8}

When $n=1$, the left hand side is $1^3=1$. The right hand side is $\left(\frac{1(1+1)}{2}\right)^2=\left(\frac{1\cdot2}{2}\right)^2=1^2=1$. Now assume
\begin{align*}
	1^3 + 2^3 + \ldots + n^3 = \left(\frac{n(n+1)}{2}\right)^2,
\end{align*}
then by adding $(n+1)^3$ to both sides we obtain
\begin{align*}
	1^3 + 2^3 + \ldots + n^3 + (n+1)^3&= \left(\frac{n(n+1)}{2}\right)^2 + (n+1)^3.
\end{align*}
We can further simplify the right hand side as
\begin{align*}
	 \left(\frac{n(n+1)}{2}\right)^2 + (n+1)^3 = \left[\left(\frac{n}{2}\right)^2 + (n+1)\right](n+1)^2 & = \frac{1}{4}\left(n^2 + 4(n+1)\right)(n+1)^2 \\
	 & = \frac{1}{4}\left(n^2 + 4n+4)\right)(n+1)^2 \\
	 & = \frac{1}{4}(n+2)^2(n+1)^2\\
	 & = \left(\frac{(n+1)(n+2)}{2}\right)^2,
\end{align*}
which is the right hand side for $n+1$, exactly as required.



\subsection{Q9}

When $n=1$ the left hand side is $1+(2\cdot1+1)=4$. The right hand side is $(1+1)^2=2^2=4$. Now assume
\begin{align*}
	1 + 3 + 5 + \ldots + (2n+1) = (n+1)^2,
\end{align*}
then adding $2(n+1)+1=2n+3$ to both sides we obtain 
\begin{align*}
	1 + 3 + 5 + \ldots + (2n+1) + 2n+3 = (n+1)^2 + 2n+3.
\end{align*}
We can further simplify the right hand side as 
\begin{align*}
	(n+1)^2 + 2n+3 = n^2 + 2n + 1 + 2n + 3 = n^2 + 4n + 4 = (n+2)^2, 
\end{align*}
which is the right hand side for $n+1$, exactly as required. 



\subsection{Q10}

When $n=1$ the left hand side is $2\cdot1=2$. The right hand side is $1\cdot(1+1)=1\cdot2=2$. Now assume
\begin{align*}
	2+4+6+\ldots+2n = n(n+1),
\end{align*}
then adding $2(n+1)=2n+2$ to both sides we obtain 
\begin{align*}
	2+4+6+\ldots+2n + 2n+2 = n(n+1) + 2n+2.
\end{align*}
We can further simplify the right hand side as 
\begin{align*}
	n(n+1) + 2n+2 = n^2 + n + 2n + 2 = n^2 + 3n + 2 = (n+1)(n+2),
\end{align*}
which is the right hand side for $n+1$, exactly as required. 

\vspace{\baselineskip}

\noindent(*) For an alternative proof, note that for $m=2n$, Equation [0.1] on page 5 of the textbook gives
\begin{align*}
	1 + 2 + \ldots + (2n-1) + 2n = \frac{2n(2n+1)}{2}=n(2n+1).
\end{align*}
Let $E:=2+4+6 + \ldots+2n$ and $O:=1 + 3 + 5 + \ldots + (2n+1)$. Then clearly $O-(2n+1)+E=1+2+\ldots+(2n-1)+2n=n(2n+1)$. In Problem 0.9 we proved $O=(n+1)^2$. Thus,
\begin{align*}
	O-(2n+1)+E=n(2n+1) &\implies (n+1)^2-(2n+1)+E=n(2n+1) \\
	&\implies E=n(2n+1)+(2n+1)-(n+1)^2.
\end{align*}
We can further simplify the right hand side as
\begin{align*}
	n(2n+1)+(2n+1)-(n+1)^2=2n^2+n+2n+1-n^2-2n-1=n^2+n=n(n+1), 
\end{align*}
exactly as required. 



\subsection{Q11}

The proof is very similar to the proof of Theorem [0.2]. Suppose $P(n)$ is false for some positive $n$. Then $S:=\{n\in\Z_+:P(n)\text{ is False.}\}$ is a non-empty subset of $\Z_+$. Therefore it has a smallest element, say $n_0$. Observe that $n_0\neq1$ because we know $P(1)$ is true from assumption (i). Thus, $n_0 > 1$ and $n_1:=n_0-1>0$ is a positive integer. We know that $P(k)$ is true for all positive integers $k\le n_1$. If, on the other hand, $P(k)$ were false for some positive $k^\prime\le n_1$, then $k\prime$ would be a member of $S$. However, $k^\prime\le n_1 = n_0 - 1 < n_0$ means $n_0$ is not the least member of $S$, which is a contradiction. 

Now we may apply assumption (ii) for $m=n_0$ as we know for all positive $k \le n_1 = n_0 - 1 < n_0$ that $P(k)$ is true. Assumption (ii) implies $P(n_0)$ is true, which is a contradiction. Thus, the assumption that $P(n)$ is false for some positive $n$ cannot be correct and $P(n)$ is true for all positive $n$. 



\subsection{Q12}

The proof is very similar to the proof of Theorem [0.2]. Suppose $P(n)$ is false for some $n\ge c$. Then $S:=\{n\ge c:P(n)\text{ is False.}\}$ is a non-empty subset of $\Z_+$. Therefore it has a smallest element, say $n_0$. Observe that $n_0\neq c$ because we know $P(c)$ is true from assumption (i). Thus, $n_0 > c$ or $n_0\ge c+1$ and $n_1:=n_0-1\ge c$. We know that $P(n_1)$ is true because otherwise $n_1$ would be a member of $S$. However, $n_1 = n_0 - 1 < n_0$ means $n_0$ is not the least member of $S$, which is a contradiction. 

Now we may apply assumption (ii) for $m=n_1$ as we know $P(n_1)$ is true. Assumption (ii) implies $P(n_1+1)=P(n_0)$ is true, which is a contradiction. Thus, the assumption that $P(n)$ is false for some $n\ge c$ cannot be correct and $P(n)$ is true for all $n\ge c$. 



\subsection{Q13}

The proof is very similar to the proof of Theorem [0.2]. Suppose $P(n)$ is false for some $n\ge c$. Then $S:=\{n\ge c:P(n)\text{ is False.}\}$ is a non-empty subset of $\Z_+$. Therefore it has a smallest element, say $n_0$. Observe that $n_0\neq c$ because we know $P(c)$ is true from assumption (i). Thus, $n_0 > c$ or $n_0\ge c+1$ and $n_1:=n_0-1\ge c$. We know that $P(k)$ is true for all positive integers $k\le n_1$. If, on the other hand, $P(k)$ were false for some positive $k^\prime\le n_1$, then $k^\prime$ would be a member of $S$. However, $k^\prime\le n_1 = n_0 - 1 < n_0$ means $n_0$ is not the least member of $S$, which is a contradiction. 

Now we may apply assumption (ii) for $m=n_0$ as we know for all positive $k \le n_1 = n_0 - 1 < n_0$ that $P(k)$ is true. Assumption (ii) implies $P(n_0)$ is true, which is a contradiction. Thus, the assumption that $P(n)$ is false for some $n\ge c$ cannot be correct and $P(n)$ is true for all $n\ge c$. 

\subsection{Q14}

Having proved the modified version of Theorem [0.2] in Problem 0.12, we can apply it with $c=2$. 

For $n=2$, the left hand side is $1\cdot 2=2$. The right hand side is $\frac{(2-1)\cdot2\cdot(2+1))}{3}=\frac{1\cdot2\cdot3}{3}=2$. Now assume
\begin{align*}
	1 \cdot 2 + 2 \cdot 3 + 3 \cdot 4 + \ldots + (n-1)n = \frac{(n-1)n(n+1)}{3},
\end{align*}
then adding $n(n+1)$ to both sides we obtain 
\begin{align*}
	1 \cdot 2 + 2 \cdot 3 + 3 \cdot 4 + \ldots + (n-1)n + n(n+1) = \frac{(n-1)n(n+1)}{3} + n(n+1).
\end{align*}
We can further simplify the right hand side as 
\begin{align*}
	\frac{(n-1)n(n+1)}{3} + n(n+1) = \frac{(n-1)n(n+1)}{3} + \frac{3n(n+1)}{3} & = \frac{(n-1)n(n+1)+3n(n+1)}{3}\\
	& = \frac{((n-1)+3)n(n+1)}{3}\\
	& = \frac{(n+2)n(n+1)}{3}\\
	& = \frac{n(n+1)(n+2)}{3}\\
\end{align*}
which is the right hand side for $n+1$, exactly as required. 



\subsection{Q15}

When $n=2$ we obtain $\frac{1}{(2-1)\cdot2}=\frac{1}{2}$. For $n=3$ we will add $\frac{1}{(3-1)\cdot3}=\frac{1}{6}$ to that for a total of $\frac{2}{3}$. For $n=4$ we will add $\frac{1}{(4-1)\cdot4}=\frac{1}{12}$ to that for a total of $\frac{3}{4}$. At this point it seems the answer is $\frac{n-1}{n}$. Let us see if this is correct by induction. 

We already know the base case $n=2$ is true from the above calculations. Now assume
\begin{align*}
	\frac{1}{1\cdot 2} + \frac{1}{2\cdot 3} + \frac{1}{3\cdot 4} + \ldots + \frac{1}{(n-1) n} = \frac{n-1}{n},
\end{align*}
then adding $\frac{1}{n(n+1)}$ to both sides we obtain
\begin{align*}
	\frac{1}{1\cdot 2} + \frac{1}{2\cdot 3} + \frac{1}{3\cdot 4} + \ldots + \frac{1}{(n-1) n} + \frac{1}{n(n+1)} = \frac{n-1}{n} + \frac{1}{n(n+1)}.
\end{align*}
We can further simplify the right hand side as 
\begin{align*}
	\frac{n-1}{n} + \frac{1}{n(n+1)} = \frac{(n-1)(n+1)}{n(n+1)} + \frac{1}{n(n+1)} = \frac{(n-1)(n+1) + 1}{n(n+1)} = \frac{n^2-1+1}{n(n+1)} = \frac{n^2}{n(n+1)} = \frac{n}{n+1},\\
\end{align*}
which is the right hand side for $n+1$, exactly as required. 



\subsection{Q16}

For $n=1$ we check if $3$ divides $1^3-1=1-1=0$. As $0 = 3\cdot 0$ we see indeed $3$ divides $0$. For a non-trivial base case we can also check for $n=2$ if $3$ divides $2^3-2=8-2=6$. As $6 = 3\cdot 2$ we see $3$ divides $6$. 

Now assume that $3$ divides $n^3-n$. Consider $(n+1)^3-(n+1)$. Expanding this out, we have  
\begin{align*}
	(n+1)^3-(n+1) = n^3 + 3n^2+3n +1 - n - 1 = \left(n^3-n\right) + 3n^2+3n = \left(n^3-n\right) + 3(n^2+n). 
\end{align*}
By assumption we know $3$ divides $n^3-n$ and therefore, $n^3-n = 3k$ for some integer $k$. Thus, 
\begin{align*}
	(n+1)^3-(n+1) = \left(n^3-n\right) + 3(n^2+n)  = 3k + 3(n^2+n) = 3\left(k+ n^2 + n\right) := 3k^\prime, 
\end{align*}
where $k^\prime := k+ n^2 + n$ is an integer. This demonstrates that $3$ divides $(n+1)^3-(n+1)$, which is exactly the statement for $n+1$.



\subsection{Q17}

We give the proof by induction first as this is in the section on mathematical induction. However, the combinatorial proof is clearer for this particular statement. 

A set $S=\{x\}$ with $n=1$ element has $2^n=2^1=2$ subsets: either $\emptyset$ or $S$ itself. Thus, the base case is true. Now assume a set with $n$ elements has $2^n$ subsets and consider any set with $n+1$ elements. Pick any element, $y$ in the set. There are two cases. Either (a) the subset contains $y$ or (b) the subset does not contain $y$. Thus, the number of subsets of a set of $n$ elements is equal to $Y$, the number of subsets of $S$ containing $y$ plus $N$, the number of subsets of $S$ not containing $y$. 

Each subset of case (a) is formed by taking a union between $\{y\}$ and any subset of $S-\{y\}$. Because $S$ has $n+1$ elements, $S-\{y\}$ has $n$ elements. Thus, there are $2^n$ such subsets and $Y=2^n$. Each subset of case (b) is formed simply by taking a subset of $S-\{y\}$. Again this set has $n$ elements so there are $2^n$ such subsets and $N=2^n$. We conclude that the number of subsets of a set of $n+1$ elements is $Y+N=2^n+2^n=2\cdot2^n=2^{n+1}$, which is exactly the statement for $n+1$

\vspace{\baselineskip}

\noindent (*) For an alternative proof consider directly counting the subsets. For each element in $S$ it is either in the subset or not. Thus, each subset is equivalent to a list of flags 0/1 for whether or not to include the element. E.g. $(0,1,1)$ for a 3 element set indicates to omit the first element and keep the other two. Each element can be $0$ or $1$ so there are two choice for $n$ elements, therefore there are $\underbrace{2\cdot2\cdot2\ldots2}_{n\text{ times}}=2^n$ possible subsets. 



\subsection{Q18}

For $k=1$ we check if $f_{5\cdot1}=f_5$ is divisible by $5$. Indeed $f_5=5$ is divisible by $5$ as $5=5\cdot1$. Now assume that $f_{5n}$ is divisible by $5$. Then, 
\begin{align*}
	f_{5(n+1)} = f_{5n+5} = f_{5n+4} + f_{5n+3} = \left(f_{5n+3} + f_{5n+2}\right) + f_{5n+3} = 2 f_{5n+3} + f_{5n+2} & = 2(f_{5n+2} + f_{5n+1}) + f_{5n+2}\\
	& = 3f_{5n+2} + 2f_{5n+1}\\
	& = 3(f_{5n+1} + f_{5n}) + 2f_{5n+1}\\
	& = 5f_{5n+1} + 3f_{5n}.\\
\end{align*}
Thus, $f_{5(n+1)}=5f_{5n+1} + 3f_{5n}$. By the induction hypothesis we know that $5$ divides $f_{5n}$ so there is an integer $k$ such that $f_{5n} = 5k$. Thus, $f_{5(n+1)}=5f_{5n+1} + 3f_{5n}=5f_{5n+1} + 3\cdot5k=5\left(f_{5n+1}+3k\right):=5k^\prime$, where $k^\prime$ is an integer. We conclude that $5$ divides $f_{5(n+1)}$, which is exactly the statement for $n+1$. 



\subsection{Q19}

When $n=1$ the left hand side is $f_{1+1}^2-f_1f_{1+2}=f_2^2-f_1f_3=1^2-1\cdot2=1-2=-1$. The right hand side is $(-1)^1=-1$. Now assume $f_{n+1}^2-f_nf_{n+2}=(-1)^n$. Then,
\begin{align*}
	f_{n+2}^2-f_{n+1}f_{n+3} = f_{n+2}^2-f_{n+1}(f_{n+2} + f_{n+1}) = f_{n+2}^2-f_{n+1}f_{n+2} - f_{n+1}^2  & = (f_{n+2}-f_{n+1})f_{n+2} - f_{n+1}^2\\
	& = ((f_{n+1} + f_n)-f_{n+1})f_{n+2} - f_{n+1}^2\\
	& = f_nf_{n+2} - f_{n+1}^2\\
	& = -(f_{n+1}^2 - f_nf_{n+2})\\
	& = -(-1)^n = (-1)^{n+1},
\end{align*}
where the second to last equality follows from the induction hypothesis. That demonstrates $f_{n+2}^2-f_{n+1}f_{n+3} = (-1)^{n+1}$, which is exactly the statement for $n+1$.



\subsection{Q20}

Because the Fibonacci Series relies on its prior two values to generate the current value, we need to use the second form of induction from Theorem [0.3]. However, the inductive step will not make sense for $m=2$ because we would be looking at $f_m = f_{m-1} + f_{m-2}$ and there is no $f_{m-2}=f_{2-2}=f_0$ (although traditionally $f_0=0$, it has not been defined in the textbook.) Therefore we simply prove the result for $n=1$ directly as a separate fact first. Then, we rely on the slightly modified version of Theorem [0.3] that we proved in Problem 0.13 to prove this result for all $n\ge2$.

\vspace{\baselineskip}

\noindent ($n=1$) When $n=1$ the left hand side is $f_1 = 1$. The right hand side is
\begin{align*}
	\frac{\alpha^1-\beta^1}{\sqrt{5}}=\frac{\frac{1+\sqrt{5}}{2}-\frac{1-\sqrt{5}}{2}}{\sqrt{5}}=\frac{\frac{1+\sqrt{5}-1+\sqrt{5}}{2}}{\sqrt{5}}=\frac{\frac{2\sqrt{5}}{2}}{\sqrt{5}}=\frac{\sqrt{5}}{\sqrt{5}}=1.
\end{align*}

\vspace{\baselineskip}

\noindent ($n\ge2$) When $n=2$, the left hand side is $f_2 = 1$. The right hand side is 
\begin{align*}
	\frac{\alpha^2-\beta^2}{\sqrt{5}}=\frac{\left(\frac{1+\sqrt{5}}{2}\right)^2-\left(\frac{1-\sqrt{5}}{2}\right)^2}{\sqrt{5}}=\frac{\frac{\left(1+\sqrt{5}\right)^2}{4}-\frac{\left(1-\sqrt{5}\right)^2}{4}}{\sqrt{5}}&=\frac{\frac{1+2\sqrt{5}+5}{4}-\frac{1-2\sqrt{5}+5}{4}}{\sqrt{5}}\\
	&=\frac{\frac{1+2\sqrt{5}+5-1+2\sqrt{5}-5}{4}}{\sqrt{5}}\\
	&=\frac{\frac{4\sqrt{5}}{4}}{\sqrt{5}}=\frac{\sqrt{5}}{\sqrt{5}}=1.
\end{align*}
Now assume for all $2\le k<m$ that $f_k = \frac{\alpha^k-\beta^k}{\sqrt{5}}.$ Then, 
\begin{align*}
	f_m = f_{m-1} + f_{m-2} = \frac{\alpha^{m-1}-\beta^{m-1}}{\sqrt{5}} +  \frac{\alpha^{m-2}-\beta^{m-2}}{\sqrt{5}} & = \frac{\alpha^{m-1}-\beta^{m-1}+\alpha^{m-2}-\beta^{m-2}}{\sqrt{5}} \\
	& = \frac{\alpha^{m-2}(\alpha+1)-\beta^{m-2}(\beta+1)}{\sqrt{5}}.
\end{align*}
Next, observe that $\alpha+1=\frac{1+\sqrt{5}}{2}+1=\frac{3+\sqrt{5}}{2}$. Moreover,
\begin{align*}
	\alpha^2=\left(\frac{1+\sqrt{5}}{2}\right)^2=\frac{\left(1+\sqrt{5}\right)^2}{4}=\frac{1+2\sqrt{5}+5}{4}=\frac{6+2\sqrt{5}}{4}=\frac{3+\sqrt{5}}{2}=\alpha+1. 
\end{align*}
Similarly, $\beta+1=\frac{1-\sqrt{5}}{2}+1=\frac{3-\sqrt{5}}{2}$ and 
\begin{align*}
	\beta^2=\left(\frac{1-\sqrt{5}}{2}\right)^2=\frac{\left(1-\sqrt{5}\right)^2}{4}=\frac{1-2\sqrt{5}+5}{4}=\frac{6-2\sqrt{5}}{4}=\frac{3-\sqrt{5}}{2}=\beta+1. 
\end{align*}
Continuing the above equalities we have
\begin{align*}
	f_m  = \frac{\alpha^{m-2}(\alpha+1)-\beta^{m-2}(\beta+1)}{\sqrt{5}} = \frac{\alpha^{m-2}\alpha^2-\beta^{m-2}\beta^2}{\sqrt{5}} = \frac{\alpha^{m}-\beta^{m}}{\sqrt{5}},
\end{align*}
which is exactly the statement for $m$. 

\vspace{\baselineskip}

\noindent (*) There is one final technicality worth noting. With $c=2$, assumption (ii) of the variant of Theorem [0.3] from Problem 0.13 says \emph{``for every $m>2$, if $P(k)$ is true for all $k$ such that $2\le k < m$ then $P(m)$ is true."} In truth, we have verified this assumption only for $m>3$ because then $m-2>1$ or $m-2\ge 2$. That is required for us to be able to use the Fibonacci Series definition which recurses twice backwards in the series ($f_m = f_{m-1} + f_{m-2}$) and still have two indices $\ge 2$ for which we know the result to be true.

The problem I am getting at is that when $m=3$ the set of $k$ such that $2\le k < m$ is simply $\{2\}$ and not $\{1,2\}$ but calculating $f_3$ relies on $f_2$ and $f_1$. However, we can treat the statement here for $n=1$ as a separate fact that is just \emph{always} true regardless of any induction. In this case, then for $m=3$ it is sufficient to assume $P(2)$ \emph{only} to get $P(3)$ to be true inductively as we can call on this separate fact (i.e. $P(1)$) as needed to complete our proof. 



\subsection{Q21}

When $n=1$ the left hand side is $F_0F_1\ldots F_{n-1}=F_0=2^{2^0}+1=2^1+1=2+1=3$. The right hand side is $F_1 - 2 = 2^{2^{1}}+1-2=2^2-1=4-1=3$. Now assume $F_0F_1\ldots F_{n-1}=F_n-2$, then mulitplying both sides by $F_n$ we have $F_0F_1\ldots F_{n-1}F_n=F_n^2-2F_n$. We can simplify the right hand side as 
\begin{align*}
	F_n^2-2F_n = \left(2^{2^n}+1\right)^2-2\left(2^{2^n}+1\right)=\left(2^{2^n}\right)^2+2\cdot 2^{2^n}+1-2\cdot 2^{2^n} - 2 = \left(2^{2^n}\right)^2-1 & = 2^{2^n}\cdot 2^{2^n}-1\\
	& = 2^{2\cdot2^n}-1\\
	& = 2^{2^{n+1}}+1-2\\
	& = F_{n+1}-2\\
\end{align*}
which is the right hand side for $n+1$, exactly as required. 



\subsection{Q22}

We prove this by induction on $n$. When $n=1$ we have a $2^{n}\times2^{n}=2^{1}\times2^1=2\times2$ checkerboard. In Figure \ref{fig:problem_0.22_0} we show a $2\times2$ checkerboard. Consider removing any of the 4 squares in the checkerboard, what is remaining? No matter what square is chosen, there are 3 squares left, and they are always shaped like an ``L" or reverse ``L" (henceforth, an "L-shaped region"). Thus, the base case holds. 

\begin{figure}
	\begin{center}
		\includegraphics[width=2in]{fig/problem_0.22_0.png}
		\caption{Graphic for the base case in Problem 0.22.}
		\label{fig:problem_0.22_0}
	\end{center}
\end{figure}

Now assume the statement to be true for $n\ge 1$ and consider a $2^{n+1}\times2^{n+1}$ checkerboard. In fact, this checkerboard is actually just four $2^{n}\times2^{n}$ sub-checkerboards arranged two by two. As an example, we demonstrate this for $n=1$ in Figure \ref{fig:problem_0.22_1}. This is a $2^{n+1}\times 2^{n+1} = 2^2\times 2^2=4\times4$ checkerboard with the bolded outlines delineating the four $2^{n}\times 2^{n} = 2^1\times 2^1=2\times2$ sub-checkerboards. 

Consider removing any square. This square will be in one of the four sub-checkerboards of size $2^n\times2^n$ and we know, by the inductive hypothesis, that this can be broken up into L-shaped regions of three squares. How about the other three sub-checkerboards of size $2^n\times2^n$? We know if we remove any one square from each of these (three in total) that the inductive hypothesis guarantees we can break each of them up into L-shaped regions of three squares. However, we cannot just remove any squares we want as we must break up this remaining figure into L-shaped regions of three squares. In fact, the centermost squares (where all three sub-checkerboards meet) constitue an L-shaped region and better yet, removing this (which we can) removes exactly one square from each of the three sub-checkerboards. At this point, we know we can break up what remains in L-shaped regions of three squares as we have three $2^n\times2^n$ checkerboards with one square removed from each. Thus, any $2^{n+1}\times 2^{n+1}$ checkerboard with exactly one square removed can be broken up into L-shaped regions of three squares. 

\begin{figure}
	\begin{center}
		\includegraphics[width=2in]{fig/problem_0.22_1.png}
		\caption{Graphic for the inductive step in Problem 0.22.}
		\label{fig:problem_0.22_1}
	\end{center}
\end{figure}



\newpage

\section{Binary Operations}

\subsection{Q1}

\begin{itemize}
	\item[(a)]{With $S = \{2,5,\sqrt{2},25,\pi,5/2\}$ and $T=\{4,25,\sqrt{2},6,3/2\}$, we have
		\begin{align*}
			S-T = \{2,5,\pi,5/2\},
		\end{align*}
		and
		\begin{align*}
			T-S = \{4,6,3/2\},
		\end{align*}
		so that 
		\begin{align*}
			S\triangle T = \left(S - T\right) \cup \left(T - S\right) = \{2,5,\pi,5/2\} \cup \{4,6,3/2\} = \{2,5,\pi,5/2,4,6,3/2\}.
		\end{align*}
		As a check, recall from Problem 0.1 that $S\cap T = \{\sqrt{2},25\}$ and $S\cup T = \{2,5,\sqrt{2},25,\pi,5/2,4,6,3/2\}$. Thus, we can also obtain $S\triangle T$ via
		\begin{align*}
			S\triangle T = \left(S \cup T\right) - \left(S \cap T\right)= \{2,5,\sqrt{2},25,\pi,5/2,4,6,3/2\} - \{\sqrt{2},25\} = \{2,5,\pi,5/2,4,6,3/2\},
		\end{align*}
		which verifies our prior computation.}
	\item[(b)]{With $S=\left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}\right\}$ and $T=\left\{\begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\}$, we have
		\begin{align*}
			S-T & = \left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}\right\} - \left\{\begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\} \\
 			& = \left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}\right\},
 		\end{align*}
 		and
		\begin{align*}
			T-S & = \left\{\begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\} - \left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}\right\} \\
			& = \left\{\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\},
		\end{align*}
		so that 
		\begin{align*}
			S\triangle T = \left(S - T\right) \cup \left(T - S\right) = \left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}\right\} \cup \left\{\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\} = \left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}, \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\}.
		\end{align*}
		In a similar way, one may also verify this with the formula $S\triangle T = \left(S\cup T\right) - \left(S\cap T\right)$. Of course, $S\cup T = \left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}, \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\}$ and $S\cap T = \left\{\begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}\right\}$. Then, $\left(S\cup T\right) - \left(S\cap T\right)$ is exactly as above. }
\end{itemize}



\subsection{Q2}

In Figure \ref{fig:problem_1.2_0}, the blue $x$'s mark $\left(A\triangle B\right)\triangle C$. 

To understand why, first consider $A\triangle B$. The purple $x$'s mark $A\triangle B$: those elements of exactly one of $A$ or $B$. The green $x$ is in $A\cap B$ so cannot be in $A\triangle B$. It is also not in $C$ either so cannot be in $\left(A\triangle B\right)\triangle C$ hence it is not colored blue. The sub-regions of $A\triangle B$ with purple $x$'s but without blue $x$'s overlap with $C$ so they cannot be in $\left(A\triangle B\right)\triangle C$ and hence are not colored blue either. The trickiest region is the very center, which \emph{is} in $\left(A\triangle B\right)\triangle C$. That is because it is in $A\cap B$ so is not in $A\triangle B$, but it \emph{is} in $C$ so is in exactly one of $A\triangle B$ or $C$ and hence in $\left(A\triangle B\right)\triangle C$. The remaining regions (the outermost regions with their red text letters) are all in exactly one of $A\triangle B$ or $C$. 

\begin{figure}
	\begin{center}
		\includegraphics[width=2in]{fig/problem_1.2.png}
		\caption{Graphic for the solution to Problem 1.2.}
		\label{fig:problem_1.2_0}
	\end{center}
\end{figure}



\subsection{Q3}

To ensure an operation is a binary operation we require (i) the operation can be computed for any element of $S\times S$ (i.e. any ordered pair of elements of $S$) and (ii) the operation returns an element of $S$. 

\begin{itemize}
	\item[(a)]{Yes, for any pair of integers $(a,b)$ one can compute $b^2$ and obtain an integer. Then we have $a$ and $b^2$ as two integers and the sum of two integers is also an integer.}
	\item[(b)]{Yes, for any pair of integers $(a,b)$ one can compute $a^2$ and $b^3$ and obtain integers. Then we have $a^2$ and $b^3$ as two integers and the product of two integers is also an integer.}
	\item[(c)]{No, the operation cannot be computed for the ordered pair $(0,0)$.}
	\item[(d)]{No, the operation cannot be computed for the ordered pair $(0,0)$.}
	\item[(e)]{Yes, for any pair of integers $(a,b)$ one can compute $-a\cdot b$ and obtain an integer. Then we have $a$, $b$, and $-a\cdot b$ as three integers and the sum of three integers is also an integer.}
	\item[(f)]{Yes, for any pair of real numbers $(a,b)$ one can simply return $b$ and this yields another real number.}
	\item[(g)]{No, $1*-4=|-4|=4\notin$S.}
	\item[(h)]{No, $3*3=3\cdot3=9\notin S$}
	\item[(i)]{Yes, for any pair of $2\times2$ matrices of real numbers $(a,b)$ computing $a*b$ is tantamount to computing 4 sums of pairs of real numbers. Of course, the sum of real numbers yields another real number so $a*b$ is a $2\times2$ matrix of real numbers.}
	\item[(j)]{Yes, certainly for any pair of subsets of $X$, $(A,B)$ we can compute $A\triangle B$ as the set of elements of $A$ or $B$, but not in both $A$ and $B$. However, we must check that $\left(A\triangle B\right)\triangle B$ is a subset of $X$. Let $x\in \left(A\triangle B\right)\triangle B$. Then (using the result proved in the remark in Problem 1.7) we know either (a) $x\in A\triangle B$, $x\notin B$ or (b) $x\notin A \triangle B$, $x\in B$. In case (a) we know that $x\in A \triangle B$ further expands to two subcases. Thus, we know either (a1) $x\in A$, $x\notin B$ or (a2) $x\notin A$, $x\in B$. Recall in case (a) that $x\notin B$ so only case (a1) is possible. However, because $x\in A\subseteq X$ we know $x\in X$ in case (a). In case (b) we know $x\in B \subseteq X$ so we know $x\in X$ in case (b). Thus, $\left(A\triangle B\right)\triangle B \subseteq X$.} 
\end{itemize}



\subsection{Q4}

Let $a*b:=a/b$ for any pair of $(a,b)\in\R^+\times\R^+$. Then, $*$ is not commutative. For example, $1*2=1/2=0.5\neq2=2/1=2*1$. It is also not associative. For example, 
\begin{align*}
	(3*1)*2=(3/1)*2=3*2=3/2=1.5,
\end{align*}
while
\begin{align*}
	3*(1*2)=3*(1/2)=3*(0.5)=3/0.5=6.
\end{align*}



\subsection{Q5}

Recall that for any $A\subseteq X$ and $B\subseteq X$, the definition of $A\cap B$ is $\{x\in X: x\in A,\text{ and } x\in B\}$. Then, let $A * B:= A \cap B$. We will show $*$ is a binary operation on $S$ (the set of all subsets of $X$) that is both commutative and associative. 

\vspace{\baselineskip}

\noindent ($*$ is a binary operation on $S$) To show that $*$ is a binary operation we need to show (i) the operation can be computed for any element of $S\times S$ and (ii) the operation returns an element of $S$. The first property is clear from the definition: we simply need to return the set of elements that are in both $A$ and $B$ to compute $A * B$. For (ii), suppose $x\in A * B := A \cap B$. Then, $x\in A \subseteq X \implies x \in X$. This is sufficient to show that $A * B \subseteq X$. 

\vspace{\baselineskip}

\noindent ($*$ is commutative) We need to show for any $(A,B)\in S\times S$, that $A * B = B * A$ or $A \cap B = B \cap A$. This is almost trivial though. Suppose $x\in A \cap B$, then $x \in A$ and $x \in B$. Re-ordering, we have $x\in B$ and $x \in A$ so $x \in B \cap A$. Thus, $A \cap B \subseteq B \cap A$. For the reverse, suppose $x \in B \cap A$, then $x \in B$ and $x \in A$. Re-ordering, we have $x \in A$ and $x\in B$ so $x \in A \cap B$. Thus, $B \cap A \subseteq A \cap B$ as well and the proof is complete. 

\vspace{\baselineskip}

\noindent ($*$ is associative) We need to show for any $A, B, C \in S$ that $(A * B) * C = A * (B * C)$ or $(A \cap B) \cap C = A \cap (B \cap C)$. This is again almost trivial. Suppose $x \in (A \cap B) \cap C$, then $x \in A \cap B$ and $x \in C$. The former implies $x \in A$ and $x \in B$. Since $x \in B$ and $x \in C$, we know $x \in B \cap C$. Thus, since $x \in A$ and $x \in B \cap C$, we have $x \in A \cap (B \cap C)$. Thus, $(A \cap B) \cap C \subseteq A \cap (B \cap C)$. For the reverse, suppose $x \in A \cap (B \cap C)$, then $x \in A$ and $x \in B \cap C$. The latter implies $x \in B$ and $x \in C$. Since $x \in A$ and $x \in B$, we know $x \in A \cap B$. Thus since $x \in A \cap B$ and $x \in C$, we have $x \in (A \cap B) \cap C$. Thus, $A \cap (B \cap C) \subseteq (A \cap B) \cap C$ as well and the proof is complete. 



\subsection{Q6}



\begin{itemize}
	\item[(a)]{It is neither commutative nor associative. Let $a=1, b=2, c=3$ then $a*b=1*2=1+2^2=5\neq3=2+1^2=2*1=b*a$. Moreover, $(a*b)*c=(1*2)*3=(1+2^2)*3=5*3=5+3^2=14$, while $a*(b*c)=1*(2*3)=1*(2+3^2)=1*11=1+11^2=122$.}
	\item[(b)]{It is neither commutative nor associative. Let $a=1, b=2, c=3$ then $a*b=1*2=1^2\cdot2^3=1\cdot8=8$, while $b*a=2*1=2^2\cdot1^3=4\cdot1=4$. Moreover, $(a*b)*c=(1*2)*3=(1^2\cdot2^3)*3=8*3=8^2\cdot3^3=64\cdot27=1728$, while $a*(b*c)=1*(2*3)=1*(2^2\cdot3^3)=1*108=1^2\cdot108^3=1259712$.}
	\item[(e)]{It is both commutative and associative. Observe that $a*b=a+b-ab=b+a-ba=b*a$ so $*$ is commutative. Moreover, $(a*b)*c=(a+b-ab)*c=(a+b-ab)+c-(a+b-ab)c=a+b+c-ab-ac-bc+abc$ and $a*(b*c)=a*(b+c-bc)=a+(b+c-bc)-a(b+c-bc)=a+b+c-bc-ab-ac+abc$ so $*$ is associative.}
	\item[(f)]{It is associative but not commutative. Let $a=1, b=2, c=3$ then $a*b=1*2=2\neq 1 = 2*1=b*a$. Observe that $(a*b)*c=b*c=c$ and $a*(b*c)=a*c=c$ so $*$ is associative.}
	\item[(i)]{It is both commutative and associative. Let $a,b,c\in S$, where $a=\begin{pmatrix} d_1 & d_2 \\ d_3 & d_4 \end{pmatrix}$, $b=\begin{pmatrix} e_1 & e_2 \\ e_3 & e_4 \end{pmatrix}$, and $c=\begin{pmatrix} f_1 & f_2 \\ f_3 & f_4 \end{pmatrix}$. Then
		\begin{align*}
			a*b=\begin{pmatrix} d_1+e_1 & d_2+e_2 \\ d_3+e_3 & d_4+e_4 \end{pmatrix}=\begin{pmatrix} e_1+d_1 & e_2+d_2 \\ e_3+d_3 & e_4+d_4 \end{pmatrix}=b*a,
		\end{align*}
	so $*$ is associative. Moreover,
		\begin{align*}
			(a*b)*c=\begin{pmatrix} d_1+e_1 & d_2+e_2 \\ d_3+e_3 & d_4+e_4 \end{pmatrix}*c&=\begin{pmatrix} (d_1+e_1)+f_1 & (d_2+e_2)+f_2 \\ (d_3+e_3)+f_3 & (d_4+e_4)+f_4 \end{pmatrix}\\
			&=\begin{pmatrix} d_1+(e_1+f_1) & d_2+(e_2+f_2) \\ d_3+(e_3+f_3) & d_4+(e_4+f_4) \end{pmatrix},
		\end{align*}
	where the final equality uses associativity of real number addition. Finally, 
		\begin{align*}
			a*(b*c)=a*\begin{pmatrix} e_1+f_1 & e_2+f_2 \\ e_3+f_3 & e_4+f_4 \end{pmatrix}=\begin{pmatrix} d_1+(e_1+f_1) & d_2+(e_2+f_2) \\ d_3+(e_3+f_3) & d_4+(e_4+f_4) \end{pmatrix},
		\end{align*}
	which is the right hand side of $(a*b)*c$ calculated above, so $*$ is associative.}
	\item[(j)]{It is associative but not commutative. In Problem 1.8 we verify that $\triangle$ is an associative binary operation. That implies that $(A\triangle B)\triangle B=A\triangle (B \triangle B)$. However, $B \triangle B= B \cup B - B\cap B = B - B = \emptyset$. Thus, $A*B=A\triangle (B \triangle B)=A \triangle \emptyset = (A\cup \emptyset)-A\cap\emptyset=A-\emptyset=A$. Now consider $A,B,C\subseteq$ we have $(A*B)*C=A*C=A$ and $A*(B*C)=A*B=A$ so $*$ is associative.}
\end{itemize}

\subsection{Q7}

\begin{remark}
	This exercise and the one following it concern the symmetric difference, $\triangle$. Throughout these two exercises we make use of the following. 
	\begin{center}
		For any sets, $C$ and $D$, $x\in C \triangle D$ if and only if either (a) $x\in C$, $x\notin D$ or (b) $x\notin C$, $x\in D$.
	\end{center}
	We start with the definition from the text ($C \triangle D = (C-D)\cup(D-C)$) and then prove the above equivalent definition. 
	
	\vspace{\baselineskip}
	
	\noindent ($\implies$) Suppose $x \in C \triangle D = (C-D)\cup(D-C)$. Then either (a) $x\in C-D$ or (b) $x\notin C-D$. In case (a), by definition of set difference $x\in C$ and $x\notin D$. In case (b), we must have $x\in D-C$ (similar to previous arguments). By definition of set difference, $x\in D$ and $x\notin C$. Thus, if $x\in C \triangle D$ then either (a) $x\in C$, $x\notin D$ or (b) $x\notin C$, $x\in D$.
	
	\vspace{\baselineskip}
	
	\noindent ($\impliedby$) Suppose either (a) $x\in C$, $x\notin D$ or (b) $x\notin C$, $x\in D$. In case (a), by definition of set difference, $x\in C - D$. It follows that $x \in (C - D) \cup (D - C)$ because $x$ is in at least one set of this union. In case (b), by definition of set difference, $x \in D - C$. It follows that $x \in (C - D) \cup (D - C)$ because $x$ is in at least one set of this union. Thus if either (a) $x\in C$, $x\notin D$ or (b) $x\notin C$, $x\in D$, then $x\in (C - D) \cup (D - C) = C \triangle D$. 
	
\end{remark} 

Now to solve Problem 1.7. To show $\triangle$ is commutative, we must show $A \triangle B = B \triangle A$. To that end, we show (i) $A \triangle B \subseteq B \triangle A$ and (ii) $B \triangle A \subseteq A \triangle B$.
\begin{itemize}
	\item[(i)]{Suppose $x\in A\triangle B$. Then our fact tells us either (a) $x\in A$, $x\notin B$ or (b) $x\notin A$, $x \in B$. In case (a), the second condition of the above fact holds when we take $C = B$ and $D = A$. That is, $x \notin C = B$ and $x \in D = A$. Thus, the fact implies $x \in C \triangle  D = B \triangle A$. In case (b), the first condition of the above fact holds when we again take $C = B$ and $D = A$. That is, $x \in C = B$ and $x \notin D = A$. Thus, the fact implies $x \in C \triangle D = B \triangle A$. Thus, we have $A \triangle B \subseteq B \triangle A$.}
	\item[(ii)]{Suppose $x\in B\triangle A$. Then our fact tells us either (a) $x\in B$, $x\notin A$ or (b) $x\notin B$, $x \in A$. In case (a), the second condition of the above fact holds when we take $C = A$ and $D = B$. That is, $x \notin C = A$ and $x \in D = B$. Thus, the fact implies $x \in C \triangle  D = A \triangle B$. In case (b), the first condition of the above fact holds when we again take $C = A$ and $D = B$. That is, $x \in C = A$ and $x \notin D = B$. Thus, the fact implies $x \in C \triangle D = A \triangle B$. Thus, we have $B \triangle A \subseteq A \triangle B$.}
\end{itemize}

\subsection{Q8}

\begin{remark}
	For any element $x$ and sets $C$ and $D$ there are four possibilities:
	\begin{itemize}
		\item[(a)]{$x\in C$, $x\notin D$,}
		\item[(b)]{$x\notin C$, $x\in D$,}
		\item[(c)]{$x\in C$, $x\in D$,}
		\item[(d)]{$x\notin C$, $x\notin D$.}
	\end{itemize}
	Cases (a) and (b) comprise the two possibilities for $x\in C\triangle D$ as demonstrated in the prior remark. Therefore $x\notin C\triangle D$ is equivalent to the two possibilities (c) and (d). This fact will be used in the proof below. 
\end{remark}

The full proof started on page 13. The only thing remaining is the reverse inclusion $A \triangle (B \triangle C) \subseteq (A \triangle B) \triangle C$. If we prove this, it then follows that $A \triangle (B \triangle C) = (A \triangle B) \triangle C$. That is precisely what it means for $\triangle$ to be an associative operation. Suppose $x \in A \triangle (B \triangle C)$. Then either (a) $x\in A$, $x \notin B \triangle C$ or (b) $x\notin A$, $x \in B \triangle C$. 

In case (a), there are two subcases (a1) $x\in A$, $x\in B$ and $x\in C$ or (a2) $x\notin A$, $x\notin B$, and $x\notin C$. In case (a1), $x\in A$ and $x\in B$ implies (cf. case (c) of the above remark) that $x\notin A\triangle B$. Thus, $x\notin A \triangle B$ and $x\in C$ so $x\in (A\triangle B) \triangle C$. In case (a2) $x\notin A$, $x\notin B$ implies (cf. case (d) of the above remark) that $x\notin A\triangle B$. Once again, $x\notin A\triangle B$ and $x\in C$ so $x\in (A\triangle B) \triangle C$. Therefore, in case (a) $x\in (A\triangle B) \triangle C$.

In case (b), there are two subcases (b1) $x\notin A$, $x\in B$ and $x\notin C$ or (b2) $x\notin A$, $x\notin B$, and $x\in C$. In case (b1), $x\notin A$ and $x\in B$ implies (cf. case (b) of the above remark) that $x\in A\triangle B$. Thus, $x\in A \triangle B$ and $x\notin C$ so $x\in (A\triangle B) \triangle C$. In case (b2) $x\notin A$, $x\notin B$ implies (cf. case (d) of the above remark) that $x\notin A\triangle B$. Thus, $\notin A\triangle B$ and $x\in C$ so $x\in (A\triangle B) \triangle C$. Therefore, in case (b) $x\in (A\triangle B) \triangle C$.

Thus, in both case (a) and (b) we have shown $x\in (A\triangle B) \triangle C$ which implies $A \triangle (B \triangle C) \subseteq (A \triangle B) \triangle C$. 



\subsection{Q9}

For the operation to be commutative we need this table to be symmetric. That is sufficient because then we know the value in ``row $s_1$, column $s_2$" equals the value in ``row $s_2$, column $s_1$." However, the table is constructed such that the respective values are $s_1 * s_2$ and $s_2 * s_1$. Thus, a symmetric table implies $s_1*s_2=s_2*s_1$, which means $*$ is commutative. In this case, by inspection, the table is symmetric so it is indeed commutative. 

However, it is not associative. For it to be associative we would need $(a*b)*b=a*(b*b)$. The left hand side is $(a*b)*b=c*b=d$. The right hand side is $a*(b*b)=a*a=a$. 



\subsection{Q10}

A binary operation is simply a map from $S \times S$ back to $S$. For each pair $(a,b)\in S\times S$ we need to assign another value $c\in S$. There are $n^2$ such pairs $(a,b)$ and each of them can be assigned one of $n$ values. That means there are $n^{n^2}$ binary operations on a set of $n$ elements. 

For the operation to be commutative, we require $a * b = b * a$. This is automatically satisfied when $a=b$, so we only need to worry about $a\neq b$. We know there are $n^2$ pairs $(a,b)\in S \times S$, $n$ of which have $a=b$. That leaves $n^2-n=n(n-1)$ pairs where $a\neq b$. When we count the number of commutative binary operations, we know that given the value of $a * b = c$ we ``force" the value of $b * a = c$. Thus, we only need to count the number of possible choices for half of the $n(n-1)$ pairs where $a\neq b$ as the other half are ``forced." There are $\frac{1}{2}n(n-1)$ such pairs and each one can get one of $n$ possible values so there are $n^{\frac{1}{2}n(n-1)}$ possibilities. We must multiply this by the number of possible assignments to the $n$ pairs of the form $(a,a)$. Because there are $n$ elements with $n$ possible values, this is $n^n$. The result is that there are
\begin{align*}
	n^{\frac{1}{2}n(n-1)}\cdot n^n = n^{\frac{1}{2}n^2 - \frac{1}{2}n + n} = n^{\frac{1}{2}n^2 + \frac{1}{2}n} = n^{\frac{1}{2}n(n+1)}
\end{align*}
commutative binary operations on a set of $n$ elements. 

\vspace{\baselineskip}

\noindent (*) For an alternative proof, consider the representation of a binary operation from Problem 1.9. In that problem, the binary operation is represented by an $n\times n$ table with the value in ``row $a$, column $b$" being the value assigned to $a * b$. The question is how many such tables are there? Since there are $n^2$ entries in the table, with $n$ possible values for each entry, the total number is $\underbrace{n\cdot n \ldots n}_{n^2\text{ times}}=n^{n^2}$.

\vspace{\baselineskip}

\noindent We can use this representation to count the number of commutative binary operations as well. In this case we need only concern ourselves with assigning a value to the ``lower left half" or ``lower triangle" part of the table. Given an entry below the diagonal, we know the value of the corresponding entry in the ``upper right half" or "upper triangle" part of the table. Thus, we need to know how many ways to assign values to the diagonal plus the lower half. There are $n + \frac{1}{2}(n^2-n) = n + \frac{1}{2}n^2 - \frac{1}{2}n = \frac{1}{2}n^2+\frac{1}{2}n = \frac{1}{2}n(n+1)$ entries to be assigned, each of which can have one of $n$ values. The result is $n^{\frac{1}{2}n(n+1)}$ possible tables. 



\newpage

\section{Groups}

\subsection{Q1}

\begin{itemize}
	\item[(a)]{Yes, adding any two non-negative real numbers results in a non-negative real number so this is a binary operation. It is associative because addition of real numbers is associative. $0$ serves as an identity element because $0*x=0+x=x$ and $x*0=x+0=x$ for all $x\in\R^+$. Finally, we can define $x^{-1}=\frac{1}{x}$, which is }
	\item[(b)]{}
	\item[(c)]{No there can be no identity element. If there were it would satisfy $x*e=x$ for all $x$ or $|xe|=x$. However, if $x<0$ then the right hand side is $<0$ but the left hand side is \emph{always} $\ge 0$.}
	\item[(d)]{Yes, multiplying any pair of elements in $\{-1,1\}$ returns an element in $\{-1,1\}$ so this is a binary operation. It is associative because multiplication of real numbers is associative. $1$ serves as an identity element because $1*x=1\cdot x=x$ and $x*1=x\cdot 1=x$ for all $x\in\{-1,1\}$. Finally $1$ is its own inverse, and $-1$ is its own inverse: $1*1=1\cdot1=1$ and $(-1)*(-1)=(-1)\cdot(-1)=1$.}
	\item[(e)]{}
	\item[(f)]{}
	\item[(g)]{}
	\item[(h)]{}
	\item[(i)]{}
\end{itemize}

\subsection{Q2}

\subsection{Q3}

\subsection{Q4}

\begin{itemize}
	\item[(a)]{The table is as follows, each element can be computed directly.

\begin{align*}
	\begin{tabular}{c|cccc}
		& 0 & 1 & 2 & 3 \\
		\hline
		0 & 0 & 1 & 2 & 3 \\
		1 & 1 & 2 & 3 & 0 \\
		2 & 2 & 3 & 0 & 1 \\
		3 & 3 & 0 & 1 & 2 \\
	\end{tabular}
\end{align*}
	
}
	\item[(b)]{The table is as follows, each element can be computed directly.

\begin{align*}
	\begin{tabular}{c|ccccc}
		& 0 & 1 & 2 & 3 & 4\\
		\hline
		0 & 0 & 1 & 2 & 3 & 4\\
		1 & 1 & 2 & 3 & 4 & 0\\
		2 & 2 & 3 & 4 & 0 & 1\\
		3 & 3 & 4 & 0 & 1 & 2\\
		4 & 4 & 0 & 1 & 2 & 3\\
	\end{tabular}	
\end{align*}
	
}
	\item[(c)]{The table is as follows, each element can be computed directly.

\begin{align*}
	\begin{tabular}{c|cccccc}
		& 0 & 1 & 2 & 3 & 4 & 5\\
		\hline
		0 & 0 & 1 & 2 & 3 & 4 & 5\\
		1 & 1 & 2 & 3 & 4 & 5 & 0\\
		2 & 2 & 3 & 4 & 5 & 0 & 1\\
		3 & 3 & 4 & 5 & 0 & 1 & 2\\
		4 & 4 & 5 & 0 & 1 & 2 & 3\\
		5 & 5 & 0 & 1 & 2 & 3 & 4\\
	\end{tabular}
\end{align*}

	
}
\end{itemize}



\subsection{Q5}

No it is not a group. Observe that $a$ is an identity element because (1) $a*a=a$, (2) $a*b=b*a=b$, and (3) $a*c=c*a=c$. However, although $a$ is its own inverse, $b$ and $c$ do not have inverses (there do not exist $x$ such that $x*b=b*x=a$ and $x*c=c*x=a$).



\subsection{Q6}

Not it is not a group. This operation is not associative because $b*(c*b)=b*b=a$ and $(b*c)*b=c*b=b$.



\subsection{Q7}

We already know that by writing down a table with elements from the original set, we have a binary operation.\footnote{The table represents the value of $a*b$ for each $(a,b)\in S \times S$, so it is computable. Moreover, as long as each entry in the table is a member of $S$, we know $*$ always returns a member of $S$.} Consider the table below. Frankly, it was chosen because it is ``nice" in that it is pretty simple and seems reasonable since both $a$ and $b$ can be returned. 

\begin{align*}
\begin{tabular}{c|cc}
	& $a$ & $b$ \\
	\hline
	$a$ & $a$ & $b$\\
	$b$ & $b$ & $a$\\
\end{tabular}
\end{align*}

In any case, an identity must satisfy $a*e=e*a=a$ and $b*e=e*b=b$. Observe that $a*a=a$ so $e=a$ meets the condition that $a*e=e*a=a$ (they are both the same condition when $e=a$). Moreover, $b*a=a*b=b$ so $e=a$ meets the condition that $b*e=e*b=b$. Thus, $e$ serves as an identify. Furthermore, we see that $a$ is its own inverse since $a*a=a$. As for $b$ we need an $x$ such that $b*x=x*b=a$. We know $b*b=a$ from the table so $b$ is also its own inverse (again, the required conditions are the same when $x=b$). Finally for associativity, we need to check for all $x,y,z\in \{a,b\}$ that $(x*y)*z=x*(z*y)$. That is fairly manageable to check directly.

\begin{align*}
	(a*a)*a=a*(a*a) \iff a*a=a*a \iff a = a\\
	(a*a)*b=a*(a*b) \iff a*b=a*b \iff b = b\\
	(a*b)*a=a*(b*a) \iff b*a=a*b \iff b = b\\
	(a*b)*b=a*(b*b) \iff b*b=a*a \iff a = a\\
	(b*a)*a=b*(a*a) \iff b*a=b*a \iff b = b\\
	(b*a)*b=b*(a*b) \iff b*b=b*b \iff a = a\\
	(b*b)*a=b*(b*a) \iff a*a=b*b \iff a = a\\
	(b*b)*b=b*(b*b) \iff a*b=b*a \iff b = b\\
\end{align*}



\subsection{Q8}

Yes this is a group. First, the binary operation can be computed for each pair of functions $(f,g)$ as we simply need to return that function whose value at each point $x\in \R$ is $f(x)\cdot g(x)$, which can be performed by computing the product of real numbers $f(x)$ and $g(x)$. Moreover, this defines a function from $\R$ to $\R$. Moreover, $\times$ is associative because for any $f,g,h\in G$, $(f\times g)\times h=f\times(g\times h)$ if and only if for all $x$ 
\begin{align*}
	(f\times g)(x)\cdot h(x) = f(x)\cdot (g\times h)(x). 
\end{align*}
Of course, this is true because $(f\times g)(x)=f(x)\cdot g(x)$ and $(g\times h)(x)=g(x)\cdot h(x)$. Thus, both the left and right hand sides are $f(x)\cdot g(x) \cdot h(x)$ so $\times$ is associative. 

For there to be an identity element $e$ we would need for all $f\in G$ that $f\times e = f$. That means for all $x\in \R$ that $(f\times e)(x)=f(x)$ or $f(x)\cdot e(x)=f(x)$. Since we require $f(x)\neq 0$ for all $x\in\R$, we obtain that $e(x)=1$ for all $x\in\R$. To completely verify that $e$ is an identity we must also check that $e\times f = f$ for all $f\in G$ (for our candidate $e$). This is so because for all $x\in \R$ we have $f(x)=f(x)$. The left hand side can be re-written as $1\cdot f(x)=f(x)$ and again as $e(x)\cdot f(x)=f(x)$, for all $x\in \R$. This is the meaning of $e\times f = f$ so $e$ is indeed an identity. 

Finally, for each $f\in G$ we need an inverse function $f^\prime$ such that $f\times f^\prime = f^\prime \times f = e$. A natural guess for $f^\prime$ is that function defined such that $f^\prime(x)=\frac{1}{f(x)}$ for all $x\in \R$ (which is well-defined because $f(x)\neq0$ for all $x\in \R$). For this $f^\prime$ we have $f(x)\cdot f^\prime(x)=f(x)\cdot\frac{1}{f(x)}=1=e(x)$, for all $x\in \R$. Moreover, we have $f^\prime(x)\cdot f(x)=\frac{1}{f(x)}\cdot f(x)=1=e(x)$, for all $x\in \R$. Thus, $f^\prime$ is an inverse for each $x$. 



\subsection{Q9}

\begin{itemize}
	\item[(a)]{Let $A=\begin{pmatrix}a_1 & a_2\\ a_3 & a_4 \end{pmatrix}$ and $B=\begin{pmatrix}b_1 & b_2\\ b_3 & b_4 \end{pmatrix}$. Then of course, $\text{det}(A)=a_1a_4-a_2a_3$ and $\text{det}(B)=b_1b_4-b_2b_3$ and
		\begin{align*}
			A\cdot B = \begin{pmatrix}a_1 & a_2\\ a_3 & a_4 \end{pmatrix}\cdot \begin{pmatrix}b_1 & b_2\\ b_3 & b_4 \end{pmatrix} = \begin{pmatrix}a_1 b_1+a_2b_3& a_1b_2+a_2b_4\\ a_3b_1+a_4b_3 & a_3b_2+a_4b_4 \end{pmatrix}.
		\end{align*}
	Thus, 	
		\begin{align*}
			\text{det}(A\cdot B) & = (a_1 b_1+a_2b_3)(a_3b_2+a_4b_4) - (a_1b_2+a_2b_4) (a_3b_1+a_4b_3) \\
			& = a_1b_1a_3b_2+a_1b_1a_4b_4+a_2b_3a_3b_2+a_2b_3a_4b_4-a_1b_2a_3b_1-a_1b_2a_4b_3-a_2b_4a_3b_1-a_2b_4a_4b_3\\
			& = (a_1a_4b_1b_4-a_1a_4b_2b_3)+(a_2a_3b_2b_3-a_2a_3b_1b_4)\\
			&\qquad\qquad\qquad\qquad\qquad\qquad+(a_1a_3b_1b_2-a_1a_3b_1b_2)+(a_2a_4b_3b_4-a_2a_4b_3b_4)\\
			& = a_1a_4(b_1b_4-b_2b_3)-a_2a_3(b_1b_4-b_2b_3)\\
			& = (a_1a_4-a_2a_3)(b_1b_4-b_2b_3)\\
			& = \text{det}(A)\cdot\text{det}(B),\\
		\end{align*}
	exactly as desired.}
\end{itemize}



\subsection{Q10}

\subsection{Q11}

\subsection{Q12}

\subsection{Q13}

\subsection{Q14}





\newpage

\section{Fundamental Theorems about Groups}

\subsection{Q1}

In $(\Z_n,\oplus)$, the inverse of $x\neq0$ is $n-x$ and the identity is $0$ (cf. page 22 of the textbook.) We use this fact to cancel the constants on either side of $x$ in the given equation, $2 \oplus x \oplus 7 = 1$. Recalling that we can group the operations in any way we like (cf. Problem 2.13), we begin by canceling the $2$ by adding $10$ to both sides of the equation on the left as
\begin{align*}
	10 \oplus 2 \oplus x \oplus 7 = 10 \oplus 1 \iff 0 \oplus x \oplus 7 = 11 \iff x \oplus 7 = 11.
\end{align*}
Then, we can cancel the $7$ by adding $5$ to both sides of the equation on the right as 
\begin{align*}
	x \oplus 7 \oplus 5 = 11 \oplus 5 \iff x \oplus 0 = 4 \iff x = 4.
\end{align*}
As a check, $2 \oplus 4 \oplus 7 = 6 \oplus 7 = 1$. 



\subsection{Q2}

In $\left(P(X),\triangle\right)$, the inverse of $A$ is $A$ and the identity is $\emptyset$ (cf. page 20 of the textbook.) Using this fact, we can can cancel $A$ from the left hand side of the given equation, $A*x=B$ as
\begin{align*}
	A*A*x=A*B\iff\emptyset * x= A*B\iff x = A*B & = A \triangle B \\
	& = A \cup B - A \cap B \\
	& = \left\{1,4,5,7,8\right\}\cup\left\{2,4,6\right\} - \left\{1,4,5,7,8\right\}\cap\left\{2,4,6\right\} \\
	& = \left\{1,2,4,5,6,7,8\right\}- \left\{4\right\}\\
	& = \left\{1,2,5,6,7,8\right\}.
\end{align*}
As a check 
\begin{align*}
	A*\left\{1,2,5,6,7,8\right\} = A\triangle\left\{1,2,5,6,7,8\right\}  & = A\cup\left\{1,2,5,6,7,8\right\} - A\cap\left\{1,2,5,6,7,8\right\}\\
	& = \left\{1,4,5,7,8\right\}\cup\left\{1,2,5,6,7,8\right\} - \left\{1,4,5,7,8\right\}\cap\left\{1,2,5,6,7,8\right\}\\
	& = \left\{1,2,4,5,6,7,8\right\} - \left\{1,5,7,8\right\}\\
	& = \left\{2,4,6\right\} = B.\\
\end{align*}



\subsection{Q3}

Consider $A = \begin{pmatrix}1 & 2 \\3 & 4\end{pmatrix}$, $B = \begin{pmatrix}0 & 1\\1 & 0\end{pmatrix}$ and $C = \begin{pmatrix}4 & 3 \\2 & 1\end{pmatrix}$. Then,
\begin{align*}
	AB = \begin{pmatrix}1 & 2 \\3 & 4\end{pmatrix} \cdot \begin{pmatrix}0 & 1\\1 & 0\end{pmatrix} = \begin{pmatrix}2 & 1\\4 & 3\end{pmatrix},
\end{align*}
and
\begin{align*}
	BC = \begin{pmatrix}0 & 1\\1 & 0\end{pmatrix} \cdot \begin{pmatrix}4 & 3 \\2 & 1\end{pmatrix}= \begin{pmatrix}2 & 1\\4 & 3\end{pmatrix}.
\end{align*}
Thus, for these three matrices $A,B,C\in GL(2\R)$ we have $AB=BC$, yet $A\neq C$. 

\vspace{\baselineskip}

\noindent (*) One may wonder how to ``derive" these matrices from first principles. There is no direct way to do this, but we can motivate how to think about the above. First, we know $B$ is the most important matrix to get the property $AB=BC$ to hold so we focus on that. To keep it simple, one first considers matrices with 0s and 1s as entries. The identity matrix immediately comes to mind. However, this is not a choice for $B$ because then $AB=BC$ \emph{implies} $A=C$. To try to retain the simplicity, we try flipping and using a matrix with 1s only on the off diagonal (i.e. $B$ above). Finally, if one considers arbitrary matrices $A$ and $C$ and explicitly writes out the four equations derived from $AB=BC$ we find a set of conditions on $C$, given the values of $A$. Specifically, $a_1=c_4$, $a_2=c_3$, $a_3=c_2$, and $a_4=c_1$.  Thus, $A$ can be any matrix you like, and you know immediately what $C$ must be. 



\subsection{Q4}

Because $G$ is a group, each element $x\in G$ has an inverse, $x^{-1}$. Thus, we can left multiply both sides of the equation $x*g=x$ by $x^{-1}$ to obtain 
\begin{align*}
	x^{-1}*x*g=x^{-1}*x\iff e*g=e \iff g = e. 
\end{align*}



\subsection{Q5}

First, we apply Theorem 3.4 with $x:=x$ and $y:=y*z$. We have
\begin{align*}
	\left(x*y*z\right)^{-1}=\left(x*(y*z)\right)^{-1}=\left(y*z\right)^{-1}*x^{-1}.
\end{align*}
Second, applying Theorem 3.4 with $x:=y$ and $y:=z$. We have
\begin{align*}
	\left(y*z\right)^{-1}=z^{-1}*y^{-1}.
\end{align*}
Finally, putting these two equations together we have
\begin{align*}
	\left(x*y*z\right)^{-1}=\left(y*z\right)^{-1}*x^{-1}=\left(z^{-1}*y^{-1}\right)*x^{-1}=z^{-1}*y^{-1}*x^{-1}.
\end{align*}



\subsection{Q6}

Let $(G,*)$ be a group and $x,y,z\in G$. To prove (i) we assume $x*y=x*z$ and we must show $y=z$. Because $x\in G$ there exists $x^{-1}$ such that $x*x^{-1}=x^{-1}*x=e$, where $e$ is the identity element in $G$. From $x*y=x*z$ we can left multiply both sides by $x^{-1}$ to obtain $x^{-1}*(x*y)=x^{-1}*(x*z)$. Using associativity we obtain $(x^{-1}*x)*y=(x^{-1}*x)*z$. That implies $e*y=e*z$, which in turn implies $y=z$, exactly as desired. To prove (ii) we assume $y*x=z*x$. This time, we can right multiply both sides by $x^{-1}$ to obtain $(y*x)*x^{-1}=(z*x)*x^{-1}$. Using associativity we obtain $y*(x*x^{-1})=z*(x*x^{-1})$. That implies $y*e=z*e$, which in turn implies $y=z$, exactly as desired. 



\subsection{Q7}

\subsection{Q8}

\subsection{Q9}

$\left(\implies\right)$ 

We know from Theorem 3.4 that for any $x,y\in G$, $\left(x*y\right)^{-1}=y^{-1}*x^{-1}$. Since $x^{-1},y^{-1}\in G$, the fact that $(G,*)$ is abelian implies $y^{-1}*x^{-1}=x^{-1}*y^{-1}$. Thus, the fact that $(G,*)$ is abelian implies for all $x,y\in G$, $\left(x*y\right)^{-1}=y^{-1}*x^{-1}=x^{-1}*y^{-1}.$

\noindent $\left(\impliedby\right)$ 

Let $x,y\in G$ then we have $\left(x*y\right)^{-1}=x^{-1}*y^{-1}$ (by assumption). However, by Theorem 3.4, the left hand side expands as $\left(x*y\right)^{-1}=y^{-1}*x^{-1}$. We conclude that for all $x,y\in G$ that $x^{-1}*y^{-1}=y^{-1}*x^{-1}$ (both being equal to $\left(x*y\right)^{-1}$.) By Theorem 3.2, the inverse is unique so we can take the inverse of both sides to obtain 
\begin{align*}
	\left(x^{-1}*y^{-1}\right)^{-1}=\left(y^{-1}*x^{-1}\right)^{-1} & \implies \left(y^{-1}\right)^{-1}* \left(x^{-1}\right)^{-1}=\left(x^{-1}\right)^{-1}* \left(y^{-1}\right)^{-1}\\
	&\implies y*x = x*y.
\end{align*}
The first implication is due to Theorem 3.4, while the second is due to Theorem  3.3. Thus, we have shown for all $x,y\in G$ we have $x*y=y*x$. I.e. $(G,*)$ is abelian. 

\subsection{Q10}

Let $(G,*)$ be a group and $g\in G$ be a fixed element. Denote $\widetilde{G}:=\{g*x:x\in G\}$. We will show $G=\widetilde{G}$ by showing (i) $G\subseteq\widetilde{G}$ and  (ii) $\widetilde{G}\subseteq G$. 
\begin{itemize}
	\item[(i)]{Suppose $y\in G$. To show that $y\in\widetilde{G}$, we must find an $x\in G$ such that $y=g*x$. Because $g\in G$, we know there is a $g^{-1}\in G$ such that $g*g^{-1}=g^{-1}*g=e$, where $e$ is the identity element in $G$. Left multiplying the equation $y=g*x$ by $g^{-1}$ yields $g^{-1}*y=g^{-1}*(g*x)=(g^{-1}*g)*x=e*x=x$. Thus, for $x=g^{-1}*y$, we can write $y=g*x$. Thus, $y\in\widetilde{G}$.}
	\item[(ii)]{Suppose $y\in\widetilde{G}$. By definition of $\widetilde{G}$, there is an $x\in G$ such that $y=x*g$. However, because $*$ is a binary operation and $x,g\in G$ we know $y=x*g\in G$.}
\end{itemize}



\subsection{Q11}

Let $x,y\in G$. Then, by definition of $G$ we have $x*x=y*y=e$. Observe that this fact immediately implies $x^{-1}=x$ and $y^{-1}=y$. To show that $x*y=y*x$ it is equivalent to show that $(x*y)^{-1}*(y*x)=e$. By Theorem 3.4, for all $x,y\in G$, $(x*y)^{-1}=y^{-1}*x^{-1}$. Thus, we can expand the left hand side as $(x*y)^{-1}*(y*x)=(y^{-1}*x^{-1})*(y*x)=(y*x)*(y*x)=(y*x)^2=e$. The final equality holds because $*$ is a binary operation so $z:=x*y\in G$ and by definition of $G$, $z^2=(y*x)^2=e$.



\subsection{Q12}

\subsection{Q13}

\subsection{Q14}

\subsection{Q15}

\subsection{Q16}

\subsection{Q17}





\end{document}