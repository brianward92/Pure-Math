\documentclass[12pt]{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage{color}
\usepackage{graphicx}
\usepackage[margin = 0.5in]{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{float}
\usepackage{rotating}
\usepackage{url}
\usepackage[font=small]{caption}
\usepackage{setspace}
\usepackage{subfigure}
\usepackage[dvipsnames]{xcolor}
\setcounter{MaxMatrixCols}{10}
\setcounter{tocdepth}{1}
\newtheorem{theorem}{Theorem}
\newtheorem{algorithm}{Algorithm} %[theorem]
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{exercise}{Exercise}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{remark}{Remark}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

%\newtheorem{acknowledgement}[theorem]{Acknowledgement}
%\newtheorem{axiom}[theorem]{Axiom}
%\newtheorem{case}[theorem]{Case}
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{conclusion}[theorem]{Conclusion}
%\newtheorem{condition}[theorem]{Condition}
%\newtheorem{conjecture}[theorem]{Conjecture}
%\newtheorem{criterion}[theorem]{Criterion}
%\newtheorem{assumption}[theorem]{Assumption}
%\newtheorem{notation}[theorem]{Notation}
%\newtheorem{problem}[theorem]{Problem}
%\newtheorem{solution}[theorem]{Solution}
%\newtheorem{summary}[theorem]{Summary}
	
\def\N{{\mathbb N}}        % positive integers
\def\Q{{\mathbb Q}}        % rationals
\def\Z{{\mathbb Z}}        % integers
\def\R{{\mathbb R}}        % reals
\def\Rn{{\R^{n}}}          % product of n copies of reals
\def\P{{\mathbb P}}        % probability
\def\E{{\mathbb E}}        % expectation
\def\EQ{{\mathbb E}^{\mathbb Q}}        % expectation
\def\1{{\mathbf 1}}        % indicator
\def\F{{\mathcal F}}        % potential measure
\def\G{{\mathcal G}}        % potential measure
\def\ess{\text{ess}}
\def\var{{\mathop{\mathbf Var}}}    %variance
\def\L{{\mathcal L} \,}
\def\Lhat{{\tilde{\mathcal L} \,}}
\def\setZ{{\mathcal Z}}
\def\setA{{\mathcal A}}
\def\setT{{\mathcal T}}
\def\D{{\mathcal D}}
\def\C{{\mathcal C}}
\def\setE{{\mathcal E}}
\def\Vest{{\mathcal V}}
\def\Cvest{{C}}
\def\Cunvest{{\tilde{C}}}

\def\I{{\mathbf I}}
\def\thetahat{{\hat{\theta}}}
\def\taub{{\hat{\tau}}}
\def\xhat{{\hat{x}}}
\def\xbar{{\bar{x}}}
\def\yhat{{\hat{y}}}
\def\x{{\textcolor[rgb]{0.00,0.00,0.00}{\hat{x}}}}
\def\y{{\textcolor[rgb]{0.00,0.00,0.00}{\hat{y}}}}
\def\v{{\hat{v}}}
\def\Xhat{{\hat{X}}}
\def\G{{\tilde{G}}}
\def\H{{\tilde{H}}}
\def\Yhat{{\hat{Y}}}
\def\Vhat{{\hat{V}}}
\def\Mhat{{\hat{M}}}

%\addtolength{\hoffset}{-1.8cm} \addtolength{\voffset}{-2cm}
%\addtolength{\textheight}{4cm} \addtolength{\textwidth}{3.6cm}

\newcommand{\ward}[1]{\textcolor{red}{#1}}

\numberwithin{theorem}{section}
\numberwithin{equation}{section}
\numberwithin{remark}{section}
\numberwithin{definition}{section}
\numberwithin{theorem}{section}
\numberwithin{lemma}{section}
\numberwithin{example}{section}

\begin{document}
\title{Solutions Guide to Abstract Algebra}
\author{Brian Ward\thanks{Email: {bmw2150@columbia.edu}. Corresponding author. }} 
\maketitle
\abstract{Solutions to the textbook ``Abstract Algebra: A First Course", Second Edition by Dan Saracino.}

\tableofcontents

\newpage

\setcounter{section}{-1}

\section{Sets and Induction}



\subsection{Q1}
With $S = \{2,5,\sqrt{2},25,\pi,5/2\}$ and $T=\{4,25,\sqrt{2},6,3/2\}$, we have 
\begin{align*}
	S\cap T = \{\sqrt{2},25\},
\end{align*}
and 
\begin{align*}
	S\cup T = \{2,5,\sqrt{2},25,\pi,5/2,4,6,3/2\}.
\end{align*}



\subsection{Q2}
For the first equation, the left hand side is
\begin{align*}
	\mathbb{Z} \cap \left(S\cup T \right) = \{2,5,25,4,6\}.
\end{align*}
As for the right hand side, we have $\mathbb{Z}\cap S = \{2,5,25\}$. and $\mathbb{Z}\cap T = \{4,25,6\}$. Thus, 
\begin{align*}
	\left(\mathbb{Z}\cap S\right)\cup\left(\mathbb{Z}\cap T\right) = \{2,5,25\} \cup \{4,25,6\} = \{2,5,25,4,6\}.
\end{align*}
For the second equation, the left hand side is
\begin{align*}
	\mathbb{Z} \cup \left(S\cap T \right) = \mathbb{Z} \cup \{\sqrt{2},25\} = \{\sqrt{2},\ldots,-3,-2,-1,0,1,2,3,\ldots\}.
\end{align*}
As for the right hand side we have 
\begin{align*}
	\mathbb{Z} \cup S = \mathbb{Z} \cup  \{2,5,\sqrt{2},25,\pi,5/2\} = \{\sqrt{2},\pi,5/2,\ldots,-3,-2,-1,0,1,2,3,\ldots\},
\end{align*}
and
\begin{align*}
	\mathbb{Z} \cup T = \mathbb{Z} \cup \{4,25,\sqrt{2},6,3/2\} = \{\sqrt{2},3/2,\ldots,-3,-2,-1,0,1,2,3,\ldots\}.
\end{align*}
Thus,
\begin{align*}
	\left(\mathbb{Z}\cup S\right)\cap\left(\mathbb{Z}\cup T\right) = \{\sqrt{2},\ldots,-3,-2,-1,0,1,2,3,\ldots\}.
\end{align*}



\subsection{Q3}
For the first equation, we prove (i) $S\cap\left(S\cup T\right)\subseteq S$ and (ii) $S\subseteq S\cap\left(S\cup T\right)$. 
\begin{itemize}
	\item[(i)]{Suppose $x\in S\cap\left(S\cup T\right)$. Because an element is in an intersection whenever it is in both sets of the intersection, we have $x\in S$ and $x\in S\cup T$. Of course, the first suffices for $S\cap\left(S\cup T\right)\subseteq S$.}
	\item[(ii)]{Suppose $x\in S$. Then $x\in S \cup T$ as well because an element is in a union if it is in at least one of the two sets in that union. Since $x\in S$ and $x\in S \cup T$, we have $x\in S\cap\left(S\cup T\right)$ so $S\subseteq S\cap\left(S\cup T\right)$.}
\end{itemize}
For the second equation, we prove (iii) $S\cup\left(S\cap T\right)\subseteq S$ and (iv) $S\subseteq S\cup\left(S\cap T\right)$. 
\begin{itemize}
	\item[(iii)]{Suppose $x\in S\cup\left(S\cap T\right)$. Then either (a) $x\in S$ or (b) $x\notin S$. In case (a) we clearly have $S\cup\left(S\cap T\right)\subseteq S$. In case (b) we must have $x\in S\cap T$ (if $x\not \in S\cap T$, then $x$ is in neither $S$ nor $S\cap T$, therefore not in $S\cup\left(S\cap T\right)$, which contradicts our assumption $x\in S\cup\left(S\cap T\right)$.) This implies case (b) is not possible. $x\in S\cap T$ implies $x\in S$ and $x\in T$, contradicting that $x\notin S$. Since cases (a) and (b) are mutually exclusive and exhaustive we have shown $S\cup\left(S\cap T\right)\subseteq S$.}
	\item[(iv)]{Suppose $x\in S$. Then $x\in S\cup\left(S\cap T\right)$ as well because an element is in a union if it is in at least one of the two sets in that union. Thus, we have $S\subseteq S\cup\left(S\cap T\right)$.}
\end{itemize}



\subsection{Q4}
\noindent ($\implies$) 

Suppose that $S \cup T = T$. We must show $S \subseteq T$. Suppose $x\in S$. Then we have $x \in S \cup T$. As $S \cup T = T$, this implies $x\in T$. Thus, $S \cup T = T \implies S \subseteq T$.

\noindent ($\impliedby$)

Suppose that $S \subseteq T$. We must show that $S \cup T = T$. Thus, we show (i) $S \cup T \subseteq T$ and (ii) $T \subseteq S \cup T$.
\begin{itemize}
	\item[(i)]{Suppose $x\in S \cup T$. Then, either (a) $x\in S$ or (b) $x\notin S$. In case (a) because we assume $S \subseteq T$, we have $x \in T$. In case (b) we must have $x \in T$ because otherwise $x\notin S$ and $x\notin T$ so $x$ could not be in $S \cup T$. In both cases we have shown $x\in T$ so we have $S \cup T \subseteq T$.}
	\item[(ii)]{Suppose $x \in T$. Then we know $x \in S \cup T$ (because it is in one of the sets in the union) so $T \subseteq S \cup T$.}
\end{itemize}
Together (i) and (ii) imply $S \cup T = T$ so $S \subseteq T \implies S \cup T = T$.



\subsection{Q5}

We show (i) $A \cap \left(B \cup C\right) \subseteq \left(A \cap B\right) \cup  \left(A \cap C\right)$ and (ii) $\left(A \cap B\right) \cup  \left(A \cap C\right) \subseteq A \cap \left(B \cup C\right)$.
\begin{itemize}
	\item[(i)]{Suppose $x\in A \cap \left(B \cup C\right)$. Then $x\in A$ and $x \in B \cup C$. Either (a) $x\in B$ or (b) $x\notin B$. In case (a) we have $x\in A$ and $x\in B$ so $x\in A \cap B$. In case (b) we must have $x\in C$ (similar to previous arguments) so $x\in A$ and $x \in C$ implying $x \in A \cap C$. In either case we have shown $x$ is in one of the sets of the union $\left(A \cap B\right) \cup  \left(A \cap C\right)$ so $A \cap \left(B \cup C\right) \subseteq \left(A \cap B\right) \cup  \left(A \cap C\right)$.}
	\item[(ii)]{Suppose $x \in \left(A \cap B\right) \cup  \left(A \cap C\right)$. Either (a) $x \in A \cap B$ or (b) $x \notin A \cap B$. In case (a) we have $x\in A$ and $x\in B$. In case (b) we must have $x \in A \cap C$ (similar to previous arguments) so that $x\in A$ and $x\in C$. In either case $x\in A$ and $x$ is either in $B$ or $C$ so that $x\in B \cup C$. Together we have $x\in A \cap \left(B \cup C\right)$ so $\left(A \cap B\right) \cup  \left(A \cap C\right) \subseteq A \cap \left(B \cup C\right)$.}
\end{itemize}



\subsection{Q6}

We show (i) $A \cup \left(B \cap C\right) \subseteq \left(A \cup B\right) \cap  \left(A \cup C\right)$ and (ii) $\left(A \cup B\right) \cap  \left(A \cup C\right) \subseteq A \cup \left(B \cap C\right)$.
\begin{itemize}
	\item[(i)]{Suppose $x\in A \cup \left(B \cap C\right)$. Then either (a) $x\in A$ or (b) $x \notin A$. In case (a) $x\in A$ implies $x\in A \cup B$ and $x \in A \cup C$ so $\left(A \cup B\right) \cap  \left(A \cup C\right)$. In case (b) we must have $x\in B \cap C$ (similar to previous arguments) so $x\in B$ and $x\in C$. That implies $x \in A \cup B$ and $x \in A \cup C$, respectively. In either case, we have $x \in A \cup B$ and $x \in A \cup C$ so $x\in \left(A \cup B\right) \cap  \left(A \cup C\right)$. That means $A \cup \left(B \cap C\right) \subseteq \left(A \cup B\right) \cap  \left(A \cup C\right)$.}
	\item[(ii)]{Suppose $x\in \left(A \cup B\right) \cap  \left(A \cup C\right)$. Then $x\in A \cup B$ and $x\in A \cup C$. Either (a) $x\in A$ or (b) $x\notin A$. In case (a) $x\in A$ implies $x\in A \cup \left(B \cap C\right)$. In case (b) we have $x\notin A$, but $x\in A \cup B$ and $x\in A \cup C$. The last two facts respectively imply $x\in B$ and $x\in C$ (otherwise $x$ could not be in those two unions) so $x\in B \cap C$ so that $x \in A \cup \left(B \cap C\right)$. Thus, $\left(A \cup B\right) \cap  \left(A \cup C\right) \subseteq A \cup \left(B \cap C\right)$.}
\end{itemize}



\subsection{Q7}

The key problem in the proof is the requirement that the subsets overlap. In particular, the book's proof has horses labeled $h_1,h_2,\ldots,h_{m},h_{m+1}$ and considers two subsets of size $m$. Subset 1 is $\{h_1,h_2,\ldots,h_m\}$ and subset 2 is $\{h_2,\ldots,h_m,h_{m+1}\}$. The intersection of these two sets is $S:=\{h_2,\ldots,h_m\}$. We know from the fact that $S$ is in subset 1, that $S$ are all of the same color, say $C_1$. Moreover, this is the color of $h_1$. We also know from the fact that $S$ is in subset 2, that $S$ are all of the same color, say $C_2$. Moreover, this is the color of $h_{m+1}$. Of course, we have just concluded $S$ has color $C_1$ \emph{and} color $C_2$ so $C_1=C_2$. Finally, that indicates $h_1$'s color, $C_1$ must equal that of $h_{m+1}$'s color, $C_2$ and so all $m+1$ horses are the same color. 

However, $S$ is empty when $m=1$ so this first inductive step cannot be carried forward. Intuitively, If I have a group of two horses and I know that all subsets of size less than two are groups of the same color, it does not imply both horses are the same color. For example, if I have one white horse and one black horse then the inductive hypothesis is satisfied by this collection of horses: any subset of size less than two (i.e. a subset of size one) is a group of horses of the same color (pick any individual horse, it is the same color as itself). However, it is obviously not true that the two horses are the same color in spite of the inductive hypothesis holding. 



\subsection{Q8}

When $n=1$, the left hand side is $1^3=1$. The right hand side is $\left(\frac{1(1+1)}{2}\right)^2=\left(\frac{1\cdot2}{2}\right)^2=1^2=1$. Now assume
\begin{align*}
	1^3 + 2^3 + \ldots + n^3 = \left(\frac{n(n+1)}{2}\right)^2,
\end{align*}
then by adding $(n+1)^3$ to both sides we obtain
\begin{align*}
	1^3 + 2^3 + \ldots + n^3 + (n+1)^3&= \left(\frac{n(n+1)}{2}\right)^2 + (n+1)^3.
\end{align*}
We can further simplify the right hand side as
\begin{align*}
	 \left(\frac{n(n+1)}{2}\right)^2 + (n+1)^3 = \left[\left(\frac{n}{2}\right)^2 + (n+1)\right](n+1)^2 & = \frac{1}{4}\left(n^2 + 4(n+1)\right)(n+1)^2 \\
	 & = \frac{1}{4}\left(n^2 + 4n+4)\right)(n+1)^2 \\
	 & = \frac{1}{4}(n+2)^2(n+1)^2\\
	 & = \left(\frac{(n+1)(n+2)}{2}\right)^2,
\end{align*}
which is the right hand side for $n+1$, exactly as required.



\subsection{Q9}

When $n=1$ the left hand side is $1+(2\cdot1+1)=4$. The right hand side is $(1+1)^2=2^2=4$. Now assume
\begin{align*}
	1 + 3 + 5 + \ldots + (2n+1) = (n+1)^2,
\end{align*}
then adding $2(n+1)+1=2n+3$ to both sides we obtain 
\begin{align*}
	1 + 3 + 5 + \ldots + (2n+1) + 2n+3 = (n+1)^2 + 2n+3.
\end{align*}
We can further simplify the right hand side as 
\begin{align*}
	(n+1)^2 + 2n+3 = n^2 + 2n + 1 + 2n + 3 = n^2 + 4n + 4 = (n+2)^2, 
\end{align*}
which is the right hand side for $n+1$, exactly as required. 



\subsection{Q10}

When $n=1$ the left hand side is $2\cdot1=2$. The right hand side is $1\cdot(1+1)=1\cdot2=2$. Now assume
\begin{align*}
	2+4+6+\ldots+2n = n(n+1),
\end{align*}
then adding $2(n+1)=2n+2$ to both sides we obtain 
\begin{align*}
	2+4+6+\ldots+2n + 2n+2 = n(n+1) + 2n+2.
\end{align*}
We can further simplify the right hand side as 
\begin{align*}
	n(n+1) + 2n+2 = n^2 + n + 2n + 2 = n^2 + 3n + 2 = (n+1)(n+2),
\end{align*}
which is the right hand side for $n+1$, exactly as required. 

\vspace{\baselineskip}

\noindent(*) For an alternative proof, note that for $m=2n$, Equation [0.1] on page 5 of the textbook gives
\begin{align*}
	1 + 2 + \ldots + (2n-1) + 2n = \frac{2n(2n+1)}{2}=n(2n+1).
\end{align*}
Let $E:=2+4+6 + \ldots+2n$ and $O:=1 + 3 + 5 + \ldots + (2n+1)$. Then clearly $O-(2n+1)+E=1+2+\ldots+(2n-1)+2n=n(2n+1)$. In Problem 0.9 we proved $O=(n+1)^2$. Thus,
\begin{align*}
	O-(2n+1)+E=n(2n+1) &\implies (n+1)^2-(2n+1)+E=n(2n+1) \\
	&\implies E=n(2n+1)+(2n+1)-(n+1)^2.
\end{align*}
We can further simplify the right hand side as
\begin{align*}
	n(2n+1)+(2n+1)-(n+1)^2=2n^2+n+2n+1-n^2-2n-1=n^2+n=n(n+1), 
\end{align*}
exactly as required. 



\subsection{Q11}

The proof is very similar to the proof of Theorem [0.2]. Suppose $P(n)$ is false for some positive $n$. Then $S:=\{n\in\Z_+:P(n)\text{ is False.}\}$ is a non-empty subset of $\Z_+$. Therefore it has a smallest element, say $n_0$. Observe that $n_0\neq1$ because we know $P(1)$ is true from assumption (i). Thus, $n_0 > 1$ and $n_1:=n_0-1>0$ is a positive integer. We know that $P(k)$ is true for all positive integers $k\le n_1$. If, on the other hand, $P(k)$ were false for some positive $k^\prime\le n_1$, then $k\prime$ would be a member of $S$. However, $k^\prime\le n_1 = n_0 - 1 < n_0$ means $n_0$ is not the least member of $S$, which is a contradiction. 

Now we may apply assumption (ii) for $m=n_0$ as we know for all positive $k \le n_1 = n_0 - 1 < n_0$ that $P(k)$ is true. Assumption (ii) implies $P(n_0)$ is true, which is a contradiction. Thus, the assumption that $P(n)$ is false for some positive $n$ cannot be correct and $P(n)$ is true for all positive $n$. 



\subsection{Q12}

The proof is very similar to the proof of Theorem [0.2]. Suppose $P(n)$ is false for some $n\ge c$. Then $S:=\{n\ge c:P(n)\text{ is False.}\}$ is a non-empty subset of $\Z_+$. Therefore it has a smallest element, say $n_0$. Observe that $n_0\neq c$ because we know $P(c)$ is true from assumption (i). Thus, $n_0 > c$ or $n_0\ge c+1$ and $n_1:=n_0-1\ge c$. We know that $P(n_1)$ is true because otherwise $n_1$ would be a member of $S$. However, $n_1 = n_0 - 1 < n_0$ means $n_0$ is not the least member of $S$, which is a contradiction. 

Now we may apply assumption (ii) for $m=n_1$ as we know $P(n_1)$ is true. Assumption (ii) implies $P(n_1+1)=P(n_0)$ is true, which is a contradiction. Thus, the assumption that $P(n)$ is false for some $n\ge c$ cannot be correct and $P(n)$ is true for all $n\ge c$. 



\subsection{Q13}

The proof is very similar to the proof of Theorem [0.2]. Suppose $P(n)$ is false for some $n\ge c$. Then $S:=\{n\ge c:P(n)\text{ is False.}\}$ is a non-empty subset of $\Z_+$. Therefore it has a smallest element, say $n_0$. Observe that $n_0\neq c$ because we know $P(c)$ is true from assumption (i). Thus, $n_0 > c$ or $n_0\ge c+1$ and $n_1:=n_0-1\ge c$. We know that $P(k)$ is true for all positive integers $k\le n_1$. If, on the other hand, $P(k)$ were false for some positive $k^\prime\le n_1$, then $k^\prime$ would be a member of $S$. However, $k^\prime\le n_1 = n_0 - 1 < n_0$ means $n_0$ is not the least member of $S$, which is a contradiction. 

Now we may apply assumption (ii) for $m=n_0$ as we know for all positive $k \le n_1 = n_0 - 1 < n_0$ that $P(k)$ is true. Assumption (ii) implies $P(n_0)$ is true, which is a contradiction. Thus, the assumption that $P(n)$ is false for some $n\ge c$ cannot be correct and $P(n)$ is true for all $n\ge c$. 

\subsection{Q14}

Having proved the modified version of Theorem [0.2] in Problem 0.12, we can apply it with $c=2$. 

For $n=2$, the left hand side is $1\cdot 2=2$. The right hand side is $\frac{(2-1)\cdot2\cdot(2+1))}{3}=\frac{1\cdot2\cdot3}{3}=2$. Now assume
\begin{align*}
	1 \cdot 2 + 2 \cdot 3 + 3 \cdot 4 + \ldots + (n-1)n = \frac{(n-1)n(n+1)}{3},
\end{align*}
then adding $n(n+1)$ to both sides we obtain 
\begin{align*}
	1 \cdot 2 + 2 \cdot 3 + 3 \cdot 4 + \ldots + (n-1)n + n(n+1) = \frac{(n-1)n(n+1)}{3} + n(n+1).
\end{align*}
We can further simplify the right hand side as 
\begin{align*}
	\frac{(n-1)n(n+1)}{3} + n(n+1) = \frac{(n-1)n(n+1)}{3} + \frac{3n(n+1)}{3} & = \frac{(n-1)n(n+1)+3n(n+1)}{3}\\
	& = \frac{((n-1)+3)n(n+1)}{3}\\
	& = \frac{(n+2)n(n+1)}{3}\\
	& = \frac{n(n+1)(n+2)}{3}\\
\end{align*}
which is the right hand side for $n+1$, exactly as required. 



\subsection{Q15}

When $n=2$ we obtain $\frac{1}{(2-1)\cdot2}=\frac{1}{2}$. For $n=3$ we will add $\frac{1}{(3-1)\cdot3}=\frac{1}{6}$ to that for a total of $\frac{2}{3}$. For $n=4$ we will add $\frac{1}{(4-1)\cdot4}=\frac{1}{12}$ to that for a total of $\frac{3}{4}$. At this point it seems the answer is $\frac{n-1}{n}$. Let us see if this is correct by induction. 

We already know the base case $n=2$ is true from the above calculations. Now assume
\begin{align*}
	\frac{1}{1\cdot 2} + \frac{1}{2\cdot 3} + \frac{1}{3\cdot 4} + \ldots + \frac{1}{(n-1) n} = \frac{n-1}{n},
\end{align*}
then adding $\frac{1}{n(n+1)}$ to both sides we obtain
\begin{align*}
	\frac{1}{1\cdot 2} + \frac{1}{2\cdot 3} + \frac{1}{3\cdot 4} + \ldots + \frac{1}{(n-1) n} + \frac{1}{n(n+1)} = \frac{n-1}{n} + \frac{1}{n(n+1)}.
\end{align*}
We can further simplify the right hand side as 
\begin{align*}
	\frac{n-1}{n} + \frac{1}{n(n+1)} = \frac{(n-1)(n+1)}{n(n+1)} + \frac{1}{n(n+1)} = \frac{(n-1)(n+1) + 1}{n(n+1)} = \frac{n^2-1+1}{n(n+1)} = \frac{n^2}{n(n+1)} = \frac{n}{n+1},\\
\end{align*}
which is the right hand side for $n+1$, exactly as required. 



\subsection{Q16}

For $n=1$ we check if $3$ divides $1^3-1=1-1=0$. As $0 = 3\cdot 0$ we see indeed $3$ divides $0$. For a non-trivial base case we can also check for $n=2$ if $3$ divides $2^3-2=8-2=6$. As $6 = 3\cdot 2$ we see $3$ divides $6$. 

Now assume that $3$ divides $n^3-n$. Consider $(n+1)^3-(n+1)$. Expanding this out, we have  
\begin{align*}
	(n+1)^3-(n+1) = n^3 + 3n^2+3n +1 - n - 1 = \left(n^3-n\right) + 3n^2+3n = \left(n^3-n\right) + 3(n^2+n). 
\end{align*}
By assumption we know $3$ divides $n^3-n$ and therefore, $n^3-n = 3k$ for some integer $k$. Thus, 
\begin{align*}
	(n+1)^3-(n+1) = \left(n^3-n\right) + 3(n^2+n)  = 3k + 3(n^2+n) = 3\left(k+ n^2 + n\right) := 3k^\prime, 
\end{align*}
where $k^\prime := k+ n^2 + n$ is an integer. This demonstrates that $3$ divides $(n+1)^3-(n+1)$, which is exactly the statement for $n+1$.



\subsection{Q17}

We give the proof by induction first as this is in the section on mathematical induction. However, the combinatorial proof is clearer for this particular statement. 

A set $S=\{x\}$ with $n=1$ element has $2^n=2^1=2$ subsets: either $\emptyset$ or $S$ itself. Thus, the base case is true. Now assume a set with $n$ elements has $2^n$ subsets and consider any set with $n+1$ elements. Pick any element, $y$ in the set. There are two cases. Either (a) the subset contains $y$ or (b) the subset does not contain $y$. Thus, the number of subsets of a set of $n$ elements is equal to $Y$, the number of subsets of $S$ containing $y$ plus $N$, the number of subsets of $S$ not containing $y$. 

Each subset of case (a) is formed by taking a union between $\{y\}$ and any subset of $S-\{y\}$. Because $S$ has $n+1$ elements, $S-\{y\}$ has $n$ elements. Thus, there are $2^n$ such subsets and $Y=2^n$. Each subset of case (b) is formed simply by taking a subset of $S-\{y\}$. Again this set has $n$ elements so there are $2^n$ such subsets and $N=2^n$. We conclude that the number of subsets of a set of $n+1$ elements is $Y+N=2^n+2^n=2\cdot2^n=2^{n+1}$, which is exactly the statement for $n+1$

\vspace{\baselineskip}

\noindent (*) For an alternative proof consider directly counting the subsets. For each element in $S$ it is either in the subset or not. Thus, each subset is equivalent to a list of flags 0/1 for whether or not to include the element. E.g. $(0,1,1)$ for a 3 element set indicates to omit the first element and keep the other two. Each element can be $0$ or $1$ so there are two choice for $n$ elements, therefore there are $\underbrace{2\cdot2\cdot2\ldots2}_{n\text{ times}}=2^n$ possible subsets. 



\subsection{Q18}

For $k=1$ we check if $f_{5\cdot1}=f_5$ is divisible by $5$. Indeed $f_5=5$ is divisible by $5$ as $5=5\cdot1$. Now assume that $f_{5n}$ is divisible by $5$. Then, 
\begin{align*}
	f_{5(n+1)} = f_{5n+5} = f_{5n+4} + f_{5n+3} = \left(f_{5n+3} + f_{5n+2}\right) + f_{5n+3} = 2 f_{5n+3} + f_{5n+2} & = 2(f_{5n+2} + f_{5n+1}) + f_{5n+2}\\
	& = 3f_{5n+2} + 2f_{5n+1}\\
	& = 3(f_{5n+1} + f_{5n}) + 2f_{5n+1}\\
	& = 5f_{5n+1} + 3f_{5n}.\\
\end{align*}
Thus, $f_{5(n+1)}=5f_{5n+1} + 3f_{5n}$. By the induction hypothesis we know that $5$ divides $f_{5n}$ so there is an integer $k$ such that $f_{5n} = 5k$. Thus, $f_{5(n+1)}=5f_{5n+1} + 3f_{5n}=5f_{5n+1} + 3\cdot5k=5\left(f_{5n+1}+3k\right):=5k^\prime$, where $k^\prime$ is an integer. We conclude that $5$ divides $f_{5(n+1)}$, which is exactly the statement for $n+1$. 



\subsection{Q19}

When $n=1$ the left hand side is $f_{1+1}^2-f_1f_{1+2}=f_2^2-f_1f_3=1^2-1\cdot2=1-2=-1$. The right hand side is $(-1)^1=-1$. Now assume $f_{n+1}^2-f_nf_{n+2}=(-1)^n$. Then,
\begin{align*}
	f_{n+2}^2-f_{n+1}f_{n+3} = f_{n+2}^2-f_{n+1}(f_{n+2} + f_{n+1}) = f_{n+2}^2-f_{n+1}f_{n+2} - f_{n+1}^2  & = (f_{n+2}-f_{n+1})f_{n+2} - f_{n+1}^2\\
	& = ((f_{n+1} + f_n)-f_{n+1})f_{n+2} - f_{n+1}^2\\
	& = f_nf_{n+2} - f_{n+1}^2\\
	& = -(f_{n+1}^2 - f_nf_{n+2})\\
	& = -(-1)^n = (-1)^{n+1},
\end{align*}
where the second to last equality follows from the induction hypothesis. That demonstrates $f_{n+2}^2-f_{n+1}f_{n+3} = (-1)^{n+1}$, which is exactly the statement for $n+1$.



\subsection{Q20}

Because the Fibonacci Series relies on its prior two values to generate the current value, we need to use the second form of induction from Theorem [0.3]. However, the inductive step will not make sense for $m=2$ because we would be looking at $f_m = f_{m-1} + f_{m-2}$ and there is no $f_{m-2}=f_{2-2}=f_0$ (although traditionally $f_0=0$, it has not been defined in the textbook.) Therefore we simply prove the result for $n=1$ directly as a separate fact first. Then, we rely on the slightly modified version of Theorem [0.3] that we proved in Problem 0.13 to prove this result for all $n\ge2$.

\vspace{\baselineskip}

\noindent ($n=1$) When $n=1$ the left hand side is $f_1 = 1$. The right hand side is
\begin{align*}
	\frac{\alpha^1-\beta^1}{\sqrt{5}}=\frac{\frac{1+\sqrt{5}}{2}-\frac{1-\sqrt{5}}{2}}{\sqrt{5}}=\frac{\frac{1+\sqrt{5}-1+\sqrt{5}}{2}}{\sqrt{5}}=\frac{\frac{2\sqrt{5}}{2}}{\sqrt{5}}=\frac{\sqrt{5}}{\sqrt{5}}=1.
\end{align*}

\vspace{\baselineskip}

\noindent ($n\ge2$) When $n=2$, the left hand side is $f_2 = 1$. The right hand side is 
\begin{align*}
	\frac{\alpha^2-\beta^2}{\sqrt{5}}=\frac{\left(\frac{1+\sqrt{5}}{2}\right)^2-\left(\frac{1-\sqrt{5}}{2}\right)^2}{\sqrt{5}}=\frac{\frac{\left(1+\sqrt{5}\right)^2}{4}-\frac{\left(1-\sqrt{5}\right)^2}{4}}{\sqrt{5}}&=\frac{\frac{1+2\sqrt{5}+5}{4}-\frac{1-2\sqrt{5}+5}{4}}{\sqrt{5}}\\
	&=\frac{\frac{1+2\sqrt{5}+5-1+2\sqrt{5}-5}{4}}{\sqrt{5}}\\
	&=\frac{\frac{4\sqrt{5}}{4}}{\sqrt{5}}=\frac{\sqrt{5}}{\sqrt{5}}=1.
\end{align*}
Now assume for all $2\le k<m$ that $f_k = \frac{\alpha^k-\beta^k}{\sqrt{5}}.$ Then, 
\begin{align*}
	f_m = f_{m-1} + f_{m-2} = \frac{\alpha^{m-1}-\beta^{m-1}}{\sqrt{5}} +  \frac{\alpha^{m-2}-\beta^{m-2}}{\sqrt{5}} & = \frac{\alpha^{m-1}-\beta^{m-1}+\alpha^{m-2}-\beta^{m-2}}{\sqrt{5}} \\
	& = \frac{\alpha^{m-2}(\alpha+1)-\beta^{m-2}(\beta+1)}{\sqrt{5}}.
\end{align*}
Next, observe that $\alpha+1=\frac{1+\sqrt{5}}{2}+1=\frac{3+\sqrt{5}}{2}$. Moreover,
\begin{align*}
	\alpha^2=\left(\frac{1+\sqrt{5}}{2}\right)^2=\frac{\left(1+\sqrt{5}\right)^2}{4}=\frac{1+2\sqrt{5}+5}{4}=\frac{6+2\sqrt{5}}{4}=\frac{3+\sqrt{5}}{2}=\alpha+1. 
\end{align*}
Similarly, $\beta+1=\frac{1-\sqrt{5}}{2}+1=\frac{3-\sqrt{5}}{2}$ and 
\begin{align*}
	\beta^2=\left(\frac{1-\sqrt{5}}{2}\right)^2=\frac{\left(1-\sqrt{5}\right)^2}{4}=\frac{1-2\sqrt{5}+5}{4}=\frac{6-2\sqrt{5}}{4}=\frac{3-\sqrt{5}}{2}=\beta+1. 
\end{align*}
Continuing the above equalities we have
\begin{align*}
	f_m  = \frac{\alpha^{m-2}(\alpha+1)-\beta^{m-2}(\beta+1)}{\sqrt{5}} = \frac{\alpha^{m-2}\alpha^2-\beta^{m-2}\beta^2}{\sqrt{5}} = \frac{\alpha^{m}-\beta^{m}}{\sqrt{5}},
\end{align*}
which is exactly the statement for $m$. 

\vspace{\baselineskip}

\noindent (*) There is one final technicality worth noting. With $c=2$, assumption (ii) of the variant of Theorem [0.3] from Problem 0.13 says \emph{``for every $m>2$, if $P(k)$ is true for all $k$ such that $2\le k < m$ then $P(m)$ is true."} In truth, we have verified this assumption only for $m>3$ because then $m-2>1$ or $m-2\ge 2$. That is required for us to be able to use the Fibonacci Series definition which recurses twice backwards in the series ($f_m = f_{m-1} + f_{m-2}$) and still have two indices $\ge 2$ for which we know the result to be true.

The problem I am getting at is that when $m=3$ the set of $k$ such that $2\le k < m$ is simply $\{2\}$ and not $\{1,2\}$ but calculating $f_3$ relies on $f_2$ and $f_1$. However, we can treat the statement here for $n=1$ as a separate fact that is just \emph{always} true regardless of any induction. In this case, then for $m=3$ it is sufficient to assume $P(2)$ \emph{only} to get $P(3)$ to be true inductively as we can call on this separate fact (i.e. $P(1)$) as needed to complete our proof. 



\subsection{Q21}

When $n=1$ the left hand side is $F_0F_1\ldots F_{n-1}=F_0=2^{2^0}+1=2^1+1=2+1=3$. The right hand side is $F_1 - 2 = 2^{2^{1}}+1-2=2^2-1=4-1=3$. Now assume $F_0F_1\ldots F_{n-1}=F_n-2$, then mulitplying both sides by $F_n$ we have $F_0F_1\ldots F_{n-1}F_n=F_n^2-2F_n$. We can simplify the right hand side as 
\begin{align*}
	F_n^2-2F_n = \left(2^{2^n}+1\right)^2-2\left(2^{2^n}+1\right)=\left(2^{2^n}\right)^2+2\cdot 2^{2^n}+1-2\cdot 2^{2^n} - 2 = \left(2^{2^n}\right)^2-1 & = 2^{2^n}\cdot 2^{2^n}-1\\
	& = 2^{2\cdot2^n}-1\\
	& = 2^{2^{n+1}}+1-2\\
	& = F_{n+1}-2\\
\end{align*}
which is the right hand side for $n+1$, exactly as required. 



\subsection{Q22}

We prove this by induction on $n$. When $n=1$ we have a $2^{n}\times2^{n}=2^{1}\times2^1=2\times2$ checkerboard. In Figure \ref{fig:problem_0.22_0} we show a $2\times2$ checkerboard. Consider removing any of the 4 squares in the checkerboard, what is remaining? No matter what square is chosen, there are 3 squares left, and they are always shaped like an ``L" or reverse ``L" (henceforth, an ``L-shaped region"). Thus, the base case holds. 

\begin{figure}
	\begin{center}
		\includegraphics[width=2in]{fig/problem_0.22_0.png}
		\caption{Graphic for the base case in Problem 0.22.}
		\label{fig:problem_0.22_0}
	\end{center}
\end{figure}

Now assume the statement to be true for $n\ge 1$ and consider a $2^{n+1}\times2^{n+1}$ checkerboard. In fact, this checkerboard is actually just four $2^{n}\times2^{n}$ sub-checkerboards arranged two by two. As an example, we demonstrate this for $n=1$ in Figure \ref{fig:problem_0.22_1}. This is a $2^{n+1}\times 2^{n+1} = 2^2\times 2^2=4\times4$ checkerboard with the bolded outlines delineating the four $2^{n}\times 2^{n} = 2^1\times 2^1=2\times2$ sub-checkerboards. 

Consider removing any square. This square will be in one of the four sub-checkerboards of size $2^n\times2^n$ and we know, by the inductive hypothesis, that this can be broken up into L-shaped regions of three squares. How about the other three sub-checkerboards of size $2^n\times2^n$? We know if we remove any one square from each of these (three in total) that the inductive hypothesis guarantees we can break each of them up into L-shaped regions of three squares. However, we cannot just remove any squares we want as we must break up this remaining figure into L-shaped regions of three squares. In fact, the centermost squares (where all three sub-checkerboards meet) constitue an L-shaped region and better yet, removing this (which we can) removes exactly one square from each of the three sub-checkerboards. At this point, we know we can break up what remains in L-shaped regions of three squares as we have three $2^n\times2^n$ checkerboards with one square removed from each. Thus, any $2^{n+1}\times 2^{n+1}$ checkerboard with exactly one square removed can be broken up into L-shaped regions of three squares. 

\begin{figure}
	\begin{center}
		\includegraphics[width=2in]{fig/problem_0.22_1.png}
		\caption{Graphic for the inductive step in Problem 0.22.}
		\label{fig:problem_0.22_1}
	\end{center}
\end{figure}



\newpage

\section{Binary Operations}

\subsection{Q1}

\begin{itemize}
	\item[(a)]{With $S = \{2,5,\sqrt{2},25,\pi,5/2\}$ and $T=\{4,25,\sqrt{2},6,3/2\}$, we have
		\begin{align*}
			S-T = \{2,5,\pi,5/2\},
		\end{align*}
		and
		\begin{align*}
			T-S = \{4,6,3/2\},
		\end{align*}
		so that 
		\begin{align*}
			S\triangle T = \left(S - T\right) \cup \left(T - S\right) = \{2,5,\pi,5/2\} \cup \{4,6,3/2\} = \{2,5,\pi,5/2,4,6,3/2\}.
		\end{align*}
		As a check, recall from Problem 0.1 that $S\cap T = \{\sqrt{2},25\}$ and $S\cup T = \{2,5,\sqrt{2},25,\pi,5/2,4,6,3/2\}$. Thus, we can also obtain $S\triangle T$ via
		\begin{align*}
			S\triangle T = \left(S \cup T\right) - \left(S \cap T\right)= \{2,5,\sqrt{2},25,\pi,5/2,4,6,3/2\} - \{\sqrt{2},25\} = \{2,5,\pi,5/2,4,6,3/2\},
		\end{align*}
		which verifies our prior computation.}
	\item[(b)]{With $S=\left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}\right\}$ and $T=\left\{\begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\}$, we have
		\begin{align*}
			S-T & = \left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}\right\} - \left\{\begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\} \\
 			& = \left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}\right\},
 		\end{align*}
 		and
		\begin{align*}
			T-S & = \left\{\begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\} - \left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}\right\} \\
			& = \left\{\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\},
		\end{align*}
		so that 
		\begin{align*}
			S\triangle T = \left(S - T\right) \cup \left(T - S\right) = \left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}\right\} \cup \left\{\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\} = \left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}, \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\}.
		\end{align*}
		In a similar way, one may also verify this with the formula $S\triangle T = \left(S\cup T\right) - \left(S\cap T\right)$. Of course, $S\cup T = \left\{\begin{pmatrix} 1 & 3 \\ 4 & 6 \end{pmatrix}, \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}, \begin{pmatrix} 1 & 1 \\ 1 & \pi \end{pmatrix}, \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}\right\}$ and $S\cap T = \left\{\begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}, \begin{pmatrix} 5 & 8 \\ 0 & -1 \end{pmatrix}\right\}$. Then, $\left(S\cup T\right) - \left(S\cap T\right)$ is exactly as above. }
\end{itemize}



\subsection{Q2}

In Figure \ref{fig:problem_1.2_0}, the blue $x$'s mark $\left(A\triangle B\right)\triangle C$. 

To understand why, first consider $A\triangle B$. The purple $x$'s mark $A\triangle B$: those elements of exactly one of $A$ or $B$. The green $x$ is in $A\cap B$ so cannot be in $A\triangle B$. It is also not in $C$ either so cannot be in $\left(A\triangle B\right)\triangle C$ hence it is not colored blue. The sub-regions of $A\triangle B$ with purple $x$'s but without blue $x$'s overlap with $C$ so they cannot be in $\left(A\triangle B\right)\triangle C$ and hence are not colored blue either. The trickiest region is the very center, which \emph{is} in $\left(A\triangle B\right)\triangle C$. That is because it is in $A\cap B$ so is not in $A\triangle B$, but it \emph{is} in $C$ so is in exactly one of $A\triangle B$ or $C$ and hence in $\left(A\triangle B\right)\triangle C$. The remaining regions (the outermost regions with their red text letters) are all in exactly one of $A\triangle B$ or $C$. 

\begin{figure}
	\begin{center}
		\includegraphics[width=2in]{fig/problem_1.2.png}
		\caption{Graphic for the solution to Problem 1.2.}
		\label{fig:problem_1.2_0}
	\end{center}
\end{figure}



\subsection{Q3}

To ensure an operation is a binary operation we require (i) the operation can be computed for any element of $S\times S$ (i.e. any ordered pair of elements of $S$) and (ii) the operation returns an element of $S$. 

\begin{itemize}
	\item[(a)]{Yes, for any pair of integers $(a,b)$ one can compute $b^2$ and obtain an integer. Then we have $a$ and $b^2$ as two integers and the sum of two integers is also an integer.}
	\item[(b)]{Yes, for any pair of integers $(a,b)$ one can compute $a^2$ and $b^3$ and obtain integers. Then we have $a^2$ and $b^3$ as two integers and the product of two integers is also an integer.}
	\item[(c)]{No, the operation cannot be computed for the ordered pair $(0,0)$.}
	\item[(d)]{No, the operation cannot be computed for the ordered pair $(0,0)$.}
	\item[(e)]{Yes, for any pair of integers $(a,b)$ one can compute $-a\cdot b$ and obtain an integer. Then we have $a$, $b$, and $-a\cdot b$ as three integers and the sum of three integers is also an integer.}
	\item[(f)]{Yes, for any pair of real numbers $(a,b)$ one can simply return $b$ and this yields another real number.}
	\item[(g)]{No, $1*-4=|-4|=4\notin$S.}
	\item[(h)]{No, $3*3=3\cdot3=9\notin S$}
	\item[(i)]{Yes, for any pair of $2\times2$ matrices of real numbers $(a,b)$ computing $a*b$ is tantamount to computing 4 sums of pairs of real numbers. Of course, the sum of real numbers yields another real number so $a*b$ is a $2\times2$ matrix of real numbers.}
	\item[(j)]{Yes, certainly for any pair of subsets of $X$, $(A,B)$ we can compute $A\triangle B$ as the set of elements of $A$ or $B$, but not in both $A$ and $B$. However, we must check that $\left(A\triangle B\right)\triangle B$ is a subset of $X$. Let $x\in \left(A\triangle B\right)\triangle B$. Then (using the result proved in the remark in Problem 1.7) we know either (a) $x\in A\triangle B$, $x\notin B$ or (b) $x\notin A \triangle B$, $x\in B$. In case (a) we know that $x\in A \triangle B$ further expands to two subcases. Thus, we know either (a1) $x\in A$, $x\notin B$ or (a2) $x\notin A$, $x\in B$. Recall in case (a) that $x\notin B$ so only case (a1) is possible. However, because $x\in A\subseteq X$ we know $x\in X$ in case (a). In case (b) we know $x\in B \subseteq X$ so we know $x\in X$ in case (b). Thus, $\left(A\triangle B\right)\triangle B \subseteq X$.} 
\end{itemize}



\subsection{Q4}

Let $a*b:=a/b$ for any pair of $(a,b)\in\R^+\times\R^+$. Then, $*$ is not commutative. For example, $1*2=1/2=0.5\neq2=2/1=2*1$. It is also not associative. For example, 
\begin{align*}
	(3*1)*2=(3/1)*2=3*2=3/2=1.5,
\end{align*}
while
\begin{align*}
	3*(1*2)=3*(1/2)=3*(0.5)=3/0.5=6.
\end{align*}



\subsection{Q5}

Recall that for any $A\subseteq X$ and $B\subseteq X$, the definition of $A\cap B$ is $\{x\in X: x\in A,\text{ and } x\in B\}$. Then, let $A * B:= A \cap B$. We will show $*$ is a binary operation on $S$ (the set of all subsets of $X$) that is both commutative and associative. 

\vspace{\baselineskip}

\noindent ($*$ is a binary operation on $S$) To show that $*$ is a binary operation we need to show (i) the operation can be computed for any element of $S\times S$ and (ii) the operation returns an element of $S$. The first property is clear from the definition: we simply need to return the set of elements that are in both $A$ and $B$ to compute $A * B$. For (ii), suppose $x\in A * B := A \cap B$. Then, $x\in A \subseteq X \implies x \in X$. This is sufficient to show that $A * B \subseteq X$. 

\vspace{\baselineskip}

\noindent ($*$ is commutative) We need to show for any $(A,B)\in S\times S$, that $A * B = B * A$ or $A \cap B = B \cap A$. This is almost trivial though. Suppose $x\in A \cap B$, then $x \in A$ and $x \in B$. Re-ordering, we have $x\in B$ and $x \in A$ so $x \in B \cap A$. Thus, $A \cap B \subseteq B \cap A$. For the reverse, suppose $x \in B \cap A$, then $x \in B$ and $x \in A$. Re-ordering, we have $x \in A$ and $x\in B$ so $x \in A \cap B$. Thus, $B \cap A \subseteq A \cap B$ as well and the proof is complete. 

\vspace{\baselineskip}

\noindent ($*$ is associative) We need to show for any $A, B, C \in S$ that $(A * B) * C = A * (B * C)$ or $(A \cap B) \cap C = A \cap (B \cap C)$. This is again almost trivial. Suppose $x \in (A \cap B) \cap C$, then $x \in A \cap B$ and $x \in C$. The former implies $x \in A$ and $x \in B$. Since $x \in B$ and $x \in C$, we know $x \in B \cap C$. Thus, since $x \in A$ and $x \in B \cap C$, we have $x \in A \cap (B \cap C)$. Thus, $(A \cap B) \cap C \subseteq A \cap (B \cap C)$. For the reverse, suppose $x \in A \cap (B \cap C)$, then $x \in A$ and $x \in B \cap C$. The latter implies $x \in B$ and $x \in C$. Since $x \in A$ and $x \in B$, we know $x \in A \cap B$. Thus since $x \in A \cap B$ and $x \in C$, we have $x \in (A \cap B) \cap C$. Thus, $A \cap (B \cap C) \subseteq (A \cap B) \cap C$ as well and the proof is complete. 



\subsection{Q6}



\begin{itemize}
	\item[(a)]{It is neither commutative nor associative. Let $a=1, b=2, c=3$ then $a*b=1*2=1+2^2=5\neq3=2+1^2=2*1=b*a$. Moreover, $(a*b)*c=(1*2)*3=(1+2^2)*3=5*3=5+3^2=14$, while $a*(b*c)=1*(2*3)=1*(2+3^2)=1*11=1+11^2=122$.}
	\item[(b)]{It is neither commutative nor associative. Let $a=1, b=2, c=3$ then $a*b=1*2=1^2\cdot2^3=1\cdot8=8$, while $b*a=2*1=2^2\cdot1^3=4\cdot1=4$. Moreover, $(a*b)*c=(1*2)*3=(1^2\cdot2^3)*3=8*3=8^2\cdot3^3=64\cdot27=1728$, while $a*(b*c)=1*(2*3)=1*(2^2\cdot3^3)=1*108=1^2\cdot108^3=1259712$.}
	\item[(e)]{It is both commutative and associative. Observe that $a*b=a+b-ab=b+a-ba=b*a$ so $*$ is commutative. Moreover, $(a*b)*c=(a+b-ab)*c=(a+b-ab)+c-(a+b-ab)c=a+b+c-ab-ac-bc+abc$ and $a*(b*c)=a*(b+c-bc)=a+(b+c-bc)-a(b+c-bc)=a+b+c-bc-ab-ac+abc$ so $*$ is associative.}
	\item[(f)]{It is associative but not commutative. Let $a=1, b=2, c=3$ then $a*b=1*2=2\neq 1 = 2*1=b*a$. Observe that $(a*b)*c=b*c=c$ and $a*(b*c)=a*c=c$ so $*$ is associative.}
	\item[(i)]{It is both commutative and associative. Let $a,b,c\in S$, where $a=\begin{pmatrix} d_1 & d_2 \\ d_3 & d_4 \end{pmatrix}$, $b=\begin{pmatrix} e_1 & e_2 \\ e_3 & e_4 \end{pmatrix}$, and $c=\begin{pmatrix} f_1 & f_2 \\ f_3 & f_4 \end{pmatrix}$. Then
		\begin{align*}
			a*b=\begin{pmatrix} d_1+e_1 & d_2+e_2 \\ d_3+e_3 & d_4+e_4 \end{pmatrix}=\begin{pmatrix} e_1+d_1 & e_2+d_2 \\ e_3+d_3 & e_4+d_4 \end{pmatrix}=b*a,
		\end{align*}
	so $*$ is associative. Moreover,
		\begin{align*}
			(a*b)*c=\begin{pmatrix} d_1+e_1 & d_2+e_2 \\ d_3+e_3 & d_4+e_4 \end{pmatrix}*c&=\begin{pmatrix} (d_1+e_1)+f_1 & (d_2+e_2)+f_2 \\ (d_3+e_3)+f_3 & (d_4+e_4)+f_4 \end{pmatrix}\\
			&=\begin{pmatrix} d_1+(e_1+f_1) & d_2+(e_2+f_2) \\ d_3+(e_3+f_3) & d_4+(e_4+f_4) \end{pmatrix},
		\end{align*}
	where the final equality uses associativity of real number addition. Finally, 
		\begin{align*}
			a*(b*c)=a*\begin{pmatrix} e_1+f_1 & e_2+f_2 \\ e_3+f_3 & e_4+f_4 \end{pmatrix}=\begin{pmatrix} d_1+(e_1+f_1) & d_2+(e_2+f_2) \\ d_3+(e_3+f_3) & d_4+(e_4+f_4) \end{pmatrix},
		\end{align*}
	which is the right hand side of $(a*b)*c$ calculated above, so $*$ is associative.}
	\item[(j)]{It is associative but not commutative. In Problem 1.8 we verify that $\triangle$ is an associative binary operation. That implies that $(A\triangle B)\triangle B=A\triangle (B \triangle B)$. However, $B \triangle B= B \cup B - B\cap B = B - B = \emptyset$. Thus, $A*B=A\triangle (B \triangle B)=A \triangle \emptyset = (A\cup \emptyset)-A\cap\emptyset=A-\emptyset=A$. Now consider $A,B,C\subseteq$ we have $(A*B)*C=A*C=A$ and $A*(B*C)=A*B=A$ so $*$ is associative.}
\end{itemize}

\subsection{Q7}

\begin{remark}
	This exercise and the one following it concern the symmetric difference, $\triangle$. Throughout these two exercises we make use of the following. 
	\begin{center}
		For any sets, $C$ and $D$, $x\in C \triangle D$ if and only if either (a) $x\in C$, $x\notin D$ or (b) $x\notin C$, $x\in D$.
	\end{center}
	We start with the definition from the text ($C \triangle D = (C-D)\cup(D-C)$) and then prove the above equivalent definition. 
	
	\vspace{\baselineskip}
	
	\noindent ($\implies$) Suppose $x \in C \triangle D = (C-D)\cup(D-C)$. Then either (a) $x\in C-D$ or (b) $x\notin C-D$. In case (a), by definition of set difference $x\in C$ and $x\notin D$. In case (b), we must have $x\in D-C$ (similar to previous arguments). By definition of set difference, $x\in D$ and $x\notin C$. Thus, if $x\in C \triangle D$ then either (a) $x\in C$, $x\notin D$ or (b) $x\notin C$, $x\in D$.
	
	\vspace{\baselineskip}
	
	\noindent ($\impliedby$) Suppose either (a) $x\in C$, $x\notin D$ or (b) $x\notin C$, $x\in D$. In case (a), by definition of set difference, $x\in C - D$. It follows that $x \in (C - D) \cup (D - C)$ because $x$ is in at least one set of this union. In case (b), by definition of set difference, $x \in D - C$. It follows that $x \in (C - D) \cup (D - C)$ because $x$ is in at least one set of this union. Thus if either (a) $x\in C$, $x\notin D$ or (b) $x\notin C$, $x\in D$, then $x\in (C - D) \cup (D - C) = C \triangle D$. 
	
\end{remark} 

Now to solve Problem 1.7. To show $\triangle$ is commutative, we must show $A \triangle B = B \triangle A$. To that end, we show (i) $A \triangle B \subseteq B \triangle A$ and (ii) $B \triangle A \subseteq A \triangle B$.
\begin{itemize}
	\item[(i)]{Suppose $x\in A\triangle B$. Then our fact tells us either (a) $x\in A$, $x\notin B$ or (b) $x\notin A$, $x \in B$. In case (a), the second condition of the above fact holds when we take $C = B$ and $D = A$. That is, $x \notin C = B$ and $x \in D = A$. Thus, the fact implies $x \in C \triangle  D = B \triangle A$. In case (b), the first condition of the above fact holds when we again take $C = B$ and $D = A$. That is, $x \in C = B$ and $x \notin D = A$. Thus, the fact implies $x \in C \triangle D = B \triangle A$. Thus, we have $A \triangle B \subseteq B \triangle A$.}
	\item[(ii)]{Suppose $x\in B\triangle A$. Then our fact tells us either (a) $x\in B$, $x\notin A$ or (b) $x\notin B$, $x \in A$. In case (a), the second condition of the above fact holds when we take $C = A$ and $D = B$. That is, $x \notin C = A$ and $x \in D = B$. Thus, the fact implies $x \in C \triangle  D = A \triangle B$. In case (b), the first condition of the above fact holds when we again take $C = A$ and $D = B$. That is, $x \in C = A$ and $x \notin D = B$. Thus, the fact implies $x \in C \triangle D = A \triangle B$. Thus, we have $B \triangle A \subseteq A \triangle B$.}
\end{itemize}

\subsection{Q8}

\begin{remark}
	For any element $x$ and sets $C$ and $D$ there are four possibilities:
	\begin{itemize}
		\item[(a)]{$x\in C$, $x\notin D$,}
		\item[(b)]{$x\notin C$, $x\in D$,}
		\item[(c)]{$x\in C$, $x\in D$,}
		\item[(d)]{$x\notin C$, $x\notin D$.}
	\end{itemize}
	Cases (a) and (b) comprise the two possibilities for $x\in C\triangle D$ as demonstrated in the prior remark. Therefore $x\notin C\triangle D$ is equivalent to the two possibilities (c) and (d). This fact will be used in the proof below. 
\end{remark}

The full proof started on page 13. The only thing remaining is the reverse inclusion $A \triangle (B \triangle C) \subseteq (A \triangle B) \triangle C$. If we prove this, it then follows that $A \triangle (B \triangle C) = (A \triangle B) \triangle C$. That is precisely what it means for $\triangle$ to be an associative operation. Suppose $x \in A \triangle (B \triangle C)$. Then either (a) $x\in A$, $x \notin B \triangle C$ or (b) $x\notin A$, $x \in B \triangle C$. 

In case (a), there are two subcases (a1) $x\in A$, $x\in B$ and $x\in C$ or (a2) $x\notin A$, $x\notin B$, and $x\notin C$. In case (a1), $x\in A$ and $x\in B$ implies (cf. case (c) of the above remark) that $x\notin A\triangle B$. Thus, $x\notin A \triangle B$ and $x\in C$ so $x\in (A\triangle B) \triangle C$. In case (a2) $x\notin A$, $x\notin B$ implies (cf. case (d) of the above remark) that $x\notin A\triangle B$. Once again, $x\notin A\triangle B$ and $x\in C$ so $x\in (A\triangle B) \triangle C$. Therefore, in case (a) $x\in (A\triangle B) \triangle C$.

In case (b), there are two subcases (b1) $x\notin A$, $x\in B$ and $x\notin C$ or (b2) $x\notin A$, $x\notin B$, and $x\in C$. In case (b1), $x\notin A$ and $x\in B$ implies (cf. case (b) of the above remark) that $x\in A\triangle B$. Thus, $x\in A \triangle B$ and $x\notin C$ so $x\in (A\triangle B) \triangle C$. In case (b2) $x\notin A$, $x\notin B$ implies (cf. case (d) of the above remark) that $x\notin A\triangle B$. Thus, $\notin A\triangle B$ and $x\in C$ so $x\in (A\triangle B) \triangle C$. Therefore, in case (b) $x\in (A\triangle B) \triangle C$.

Thus, in both case (a) and (b) we have shown $x\in (A\triangle B) \triangle C$ which implies $A \triangle (B \triangle C) \subseteq (A \triangle B) \triangle C$. 



\subsection{Q9}

For the operation to be commutative we need this table to be symmetric. That is sufficient because then we know the value in ``row $s_1$, column $s_2$" equals the value in ``row $s_2$, column $s_1$." However, the table is constructed such that the respective values are $s_1 * s_2$ and $s_2 * s_1$. Thus, a symmetric table implies $s_1*s_2=s_2*s_1$, which means $*$ is commutative. In this case, by inspection, the table is symmetric so it is indeed commutative. 

However, it is not associative. For it to be associative we would need $(a*b)*b=a*(b*b)$. The left hand side is $(a*b)*b=c*b=d$. The right hand side is $a*(b*b)=a*a=a$. 



\subsection{Q10}

A binary operation is simply a map from $S \times S$ back to $S$. For each pair $(a,b)\in S\times S$ we need to assign another value $c\in S$. There are $n^2$ such pairs $(a,b)$ and each of them can be assigned one of $n$ values. That means there are $n^{n^2}$ binary operations on a set of $n$ elements. 

For the operation to be commutative, we require $a * b = b * a$. This is automatically satisfied when $a=b$, so we only need to worry about $a\neq b$. We know there are $n^2$ pairs $(a,b)\in S \times S$, $n$ of which have $a=b$. That leaves $n^2-n=n(n-1)$ pairs where $a\neq b$. When we count the number of commutative binary operations, we know that given the value of $a * b = c$ we ``force" the value of $b * a = c$. Thus, we only need to count the number of possible choices for half of the $n(n-1)$ pairs where $a\neq b$ as the other half are ``forced." There are $\frac{1}{2}n(n-1)$ such pairs and each one can get one of $n$ possible values so there are $n^{\frac{1}{2}n(n-1)}$ possibilities. We must multiply this by the number of possible assignments to the $n$ pairs of the form $(a,a)$. Because there are $n$ elements with $n$ possible values, this is $n^n$. The result is that there are
\begin{align*}
	n^{\frac{1}{2}n(n-1)}\cdot n^n = n^{\frac{1}{2}n^2 - \frac{1}{2}n + n} = n^{\frac{1}{2}n^2 + \frac{1}{2}n} = n^{\frac{1}{2}n(n+1)}
\end{align*}
commutative binary operations on a set of $n$ elements. 

\vspace{\baselineskip}

\noindent (*) For an alternative proof, consider the representation of a binary operation from Problem 1.9. In that problem, the binary operation is represented by an $n\times n$ table with the value in ``row $a$, column $b$" being the value assigned to $a * b$. The question is how many such tables are there? Since there are $n^2$ entries in the table, with $n$ possible values for each entry, the total number is $\underbrace{n\cdot n \ldots n}_{n^2\text{ times}}=n^{n^2}$.

\vspace{\baselineskip}

\noindent We can use this representation to count the number of commutative binary operations as well. In this case we need only concern ourselves with assigning a value to the ``lower left half" or ``lower triangle" part of the table. Given an entry below the diagonal, we know the value of the corresponding entry in the ``upper right half" or "upper triangle" part of the table. Thus, we need to know how many ways to assign values to the diagonal plus the lower half. There are $n + \frac{1}{2}(n^2-n) = n + \frac{1}{2}n^2 - \frac{1}{2}n = \frac{1}{2}n^2+\frac{1}{2}n = \frac{1}{2}n(n+1)$ entries to be assigned, each of which can have one of $n$ values. The result is $n^{\frac{1}{2}n(n+1)}$ possible tables. 



\newpage

\section{Groups}

\subsection{Q1}

\begin{itemize}
	\item[(a)]{No this is not a group because there can be no identity. An identity element $e$ must satisfy $x*e=x$ for all $x\in\R^+$. Thus, $x+e=x$ which implies $e=0$, which is not in $\R^+$.}
	\item[(b)]{Yes, let $x,y\in3\Z$. Then there exist $k_x,k_y\in\Z$ such that $x=3k_x$ and $y=3k_y$. Thus, $x*y=x+y=3k_x+3k_y=3(k_x+k_y)$, which is a multiple of 3 so is in $3\Z$. Thus, we have a binary operation on $3\Z$. Associativity follows because addition of integers is associative. $0$ serves as an identity because for any $x\in3\Z$ we have $0*x=0+x=x$ and $x*0=x+0=x$. Finally, for any $x\in3\Z$, $-x\in3\Z$. That is because there is a $k_x\in\Z$ such that $x=3k_x$. Thus, $-x=-3k_x=3(-k_x)$ is in $3\Z$. Moreover, $-x$ serves as an inverse because $x*(-x)=x+(-x)=x-x=0$ and $(-x)*x=-x+x=0$.}
	\item[(c)]{No this is not a group because there can be no identity. An identity element $e$ must satisfy $x*e=x$ for all $x\in\R-\{0\}$. Thus, we must have $|xe|=x$. However, if $x<0$ then the right hand side is $<0$ but the left hand side is \emph{always} $\ge 0$ so this equation can never be satisfied for any $x<0$.}
	\item[(d)]{Yes, multiplying any pair of elements in $\{-1,1\}$ returns an element in $\{-1,1\}$ so this is a binary operation. It is associative because multiplication of real numbers is associative. $1$ serves as an identity element because $1*x=1\cdot x=x$ and $x*1=x\cdot 1=x$ for all $x\in\{-1,1\}$. Finally $1$ is its own inverse, and $-1$ is its own inverse: $1*1=1\cdot1=1$ and $(-1)*(-1)=(-1)\cdot(-1)=1$.}
	\item[(e)]{Yes, let $q_1,q_2$ be two positive rational numbers and $r_1,r_2$ be their rational square roots. Thus, $r_1^2=q_1$ and $r_2^2=q_2$. Moreover, $q_1*q_2=q_1\cdot q_2$, which has the rational square root $r_1\cdot r_2$. To show this, first note that $r_1\cdot r_2$ is rational because the rational numbers are closed under multiplication. Moreover, $(r_1\cdot r_2)^2=r_1^2\cdot r_2^2=q_1\cdot q_2$. It is associative because multiplication of rational numbers is associative. The number $1$ is a positive rational number with rational square root $1$ and serves as an identity. In particular, for any positive rational $q$ with a rational square root, $q*1=q\cdot1=q$ and $1*q=1\cdot q=q$. Finally, if $q$ is a rational number with rational square root $r$ then $r^2=q$. Thus, $\frac{1}{q}$ has square root $\frac{1}{r}$ because $\left(\frac{1}{r}\right)^2=\frac{1}{r^2}=\frac{1}{q}$. Moreover, both $\frac{1}{q}$ and $\frac{1}{r}$ are rational. That is because if $x$ is a rational number, it can be expressed as $\frac{p}{q}$ for integers, $p$ and $q$. It follows that $\frac{1}{x}=\frac{q}{p}$ is also a rational number. Finally, $\frac{1}{q}$ serves as an inverse for any $q$. That is because $q*\frac{1}{q}=q\cdot\frac{1}{q}=1$ and $\frac{1}{q}*q=\frac{1}{q}\cdot q=1$. Note also that $\frac{1}{q}$ is well-defined for any rational $q$ because $q$ is positive, hence non-zero (and similarly for $\frac{1}{r}$).}
	\item[(f)]{No this is not a group because there can be no identity. An identity element $e:=(e_1,e_2)$ must satisfy $x*e=x$ for all pairs of real numbers $x:=(x_1,x_2)$. Thus, $(x_1,x_2)*(e_1,e_2)=(x_1,x_2)$ which implies $(x_1+e_1,x_2-e_2)=(x_1,x_2)$. Componentwise we must have $x_1+e_1=x_1$ and $x_2-e_2=x_2$. That implies $e_1=e_2=0$. However, $(0,0)*x=(0,0)*(x_1,x_2)=(0+x_1,0-x_2)=(x_1,-x_2)$ which does not equal $(x_1,x_2)$ for all $x$. In particular, whenever $x_2\neq0$ we cannot have this equality.}
	\item[(g)]{Yes, consider $(a_1,b_1),(a_2,b_2)$ with $b_1\neq0$ and $b_2\neq0$. We have $(a_1,b_1)*(a_2,b_2)=(a_1+a_2,b_1\cdot b_2)$. For this to be in the group we must have $b_1\cdot b_2\neq0$. That is indeed the case because both multiplicants are non-zero. Thus, we have a binary operation. Consider also $(a_3,b_3)$ then,
		\begin{align*}
			\left[(a_1,b_1)*(a_2,b_2)\right]*(a_3,b_3)=(a_1+a_2,b_1\cdot b_2)*(a_3,b_3)=(a_1+a_2+a_3,b_1\cdot b_2\cdot b_3)
		\end{align*}
	and
		\begin{align*}
			(a_1,b_1)*\left[(a_2,b_2)*(a_3,b_3)\right]=(a_1,b_1)*(a_2 + a_3,b_2\cdot b_3)=(a_1+a_2+a_3,b_1\cdot b_2\cdot b_3).
		\end{align*}
	Thus, $*$ is associative\footnote{Note we can unambiguously write $a_1+a_2+a_3$ and $b_1\cdot b_2\cdot b_3$ because addition and multiplication of real numbers are associative operations.}. For an element $e:=(e_1,e_2)$ to be an identity we must have $(a,b)*(e_1,e_2)=(a,b)$ for all pairs of real numbers $a$ and $b$. Thus, $(a+e_1,b\cdot e_2)=(a,b)$. Componentwise we must have $a+e_1=a$ and $b\cdot e_2=b$ which implies $e_1=0$ and $e_2=1$ (the latter follows from $b\neq0$.) Indeed $(a,b)*(0,1)=(a+0,b\cdot1)=(a,b)$ and $(0,1)*(a,b)=(0+a,1\cdot b)=(a,b)$. One may observe that this group is simply pairs of elements of the groups $(\R,+)$ and $(\R-\{0\},\cdot)$ from examples $1$ and $2$ in the textbook. For this reason, we immediately suggest that the inverse element of $(a,b)$ must be $\left(-a,\frac{1}{b}\right)$. Indeed $(a,b)*\left(-a,\frac{1}{b}\right)=\left(a-a,b\cdot\frac{1}{b}\right)=(0,1)$ and $\left(-a,\frac{1}{b}\right)*(a,b)=\left(-a+a,\frac{1}{b}\cdot b\right)=(0,1)$.}
	\item[(h)]{Yes, consider $a,b\in\R-\{1\}$ then we must show $a*b\in\R-\{1\}$. Suppose to the contrary that $a*b=1$ for some $a,b$. Then we have $a+b-ab=1$, which implies $(1-a)b=1-a$. Because $a\neq1$, we can divide both sides by $1-a$ and we obtain $b=1$, which is a contradiction since $b\in\R-\{1\}$. Thus, it can never be the case that $a*b=1$ so $*$ is indeed a binary operation. Consider also $c\in\R-\{1\}$ then,
		\begin{align*}
			(a*b)*c=(a+b-ab)*c=(a+b-ab) + c - (a+b-ab)\cdot c = a + b + c - ab -ac - bc + abc,
		\end{align*}
	and
		\begin{align*}
			a*(b*c)=a*(b+c-bc)=a + (b+c-bc) - a\cdot (b+c-bc) = a + b + c - ab - ac - bc + abc.
		\end{align*}
	Thus, $*$ is associative\footnote{Similar to the prior footnote, addition and multiplication of real numbers are associative so we can be ambiguous about the way we write the sum $a+b+c$ and the product $abc$.}. An identity element $e$ must satisfy $x*e=x$ for all $x\in\R-\{1\}$. Thus, $x+e-xe=x$ which implies $e=xe$. For this to hold we need $e=0$. Indeed $x*0=x+0-x\cdot0=x$ and $0*x=0+x-0\cdot x=x$ for all $x\in\R-\{1\}$. Let $x^{-1}$ denote an inverse element. We must have $x*x^{-1}=0$ which implies $x+x^{-1}-x\cdot x^{-1}=0$ or $x^{-1}=\frac{-x}{1-x}$. Note this element is well defined because $x\neq1$. Moreover, if $x^{-1}=1$ we would have $\frac{-x}{1-x}=1$, which implies $-x=1-x\implies0=1$, which is of course a contradiction so $x^{-1}\neq1$ for any $x$. Thus, $x^{-1}\in\R-\{1\}$. Finally, we have $x^{-1}*x=\left(\frac{-x}{1-x}\right)*x=\frac{-x}{1-x}+x-\frac{-x}{1-x}\cdot x=\frac{-x}{1-x}+\frac{x-x^2}{1-x}+\frac{x^2}{1-x}=0$.}
	\item[(i)]{Yes, consider $a,b\in\Z$. Then $a*b=a+b-1\in\Z$ so $*$ is a binary operation. Consider also $c\in\Z$ then,
		\begin{align*}
			(a*b)*c=(a+b-1)*c=(a+b-1)+c-1=a+b+c-2
		\end{align*}
	and
		\begin{align*}
			a*(b*c)=a*(b+c-1)=a+(b+c-1)-1=a+b+c-2.
		\end{align*}
	Thus, $*$ is associative\footnote{For the final time on this problem, we can be ambiguous about $a+b+c$ because addition of integers is associative.}. An identity element $e$ must satisfy $x*e=x$ for all $x\in\Z$. Thus, $x+e-1=x$, which implies $e=1$. Indeed $x*1=x+1-1=x$ and $1*x=1+x-1=x$ for all $x\in\Z$. Let $x^{-1}$ denote an inverse element. We must have $x*x^{-1}=1$ which implies $x+x^{-1}-1=1$ or $x^{-1}=2-x$ and this is an element of $\Z$. Indeed $x*x^{-1}=x*(2-x)=x+(2-x)-1=1$ and $x^{-1}*x=(2-x)*x=(2-x)+x-1=1$.}
\end{itemize}

\subsection{Q2}

\begin{itemize}
	\item[(a)]{They are all Abelian.
		\begin{itemize}
			\item[(b)]{Let $a,b\in3\Z$. Then $a*b=a+b=b+a=b*a$, where the second equality follows because addition of integers is commutative.}
			\item[(d)]{Let $a,b\in\{-1,1\}$. Then $a*b=a\cdot b = b\cdot a=b*a$, where the second equality follows because multiplication of integers is commutative.}
			\item[(e)]{Let $a,b\in\Q^+$ be two positive rational numbers with rational square roots. Then $a*b=a\cdot b = b\cdot a = b*a$, where the second equality follows because multiplication of rational numbers is commutative.}
			\item[(g)]{Let $(a_1,b_1),(a_2,b_2)$ be two pairs of real numbers. Then,
				\begin{align*}
					(a_1,b_1)*(a_2,b_2)=(a_1+a_2,b_1\cdot b_2)=(a_2+a_1,b_2\cdot b_1)=(a_2,b_2)*(a_1,b_1),
				\end{align*}
			where the second equality follows because addition and multiplication of real numbers are commutative operations.}
			\item[(h)]{Let $a,b\in\R-\{1\}$. Then $a*b=a+b-ab=b+a-ba=b*a$, where the second equality follows because addition and multiplication of real numbers are commutative operations.}
			\item[(i)]{Let $a,b\in\Z$. Then, $a*b=a+b-1=b+a-1=b*a$, where the second equality follows because addition of integers is commutative.}
		\end{itemize}}
	\item[(b)]{Each example is actually multiple examples. We address only the examples that are actually groups and skip the non-groups as identified on pages 17-20 of the textbook.
		\begin{itemize}
			\item[1.]{All three of $(\Z,+)$, $(\Q,+)$ and $(\R,+)$ are Abelian groups. That follows because for any integer, rational or real numbers $a,b$, we have $a+b=b+a$.}
			\item[2.]{All four of $(\Q^+,\cdot)$, $(\R^+,\cdot)$, $(\R-\{0\},\cdot)$, and $(\Q-\{0\},\cdot)$ are Abelian groups. That follows because for any rational or real numbers $a,b$ (positive, zero, or otherwise), we have $a\cdot b=b\cdot a$.}
			\item[3.]{$\R^n$ with componentwise addition is an Abelian group. Let $(a_1,\ldots,a_n),(b_1,\ldots,b_n)\in\R^n$. Then,
				\begin{align*}
					(a_1,\ldots,a_n)*(b_1,\ldots,b_n)=(a_1+b_1,\ldots,a_n+b_n)&=(b_1+a_1,\ldots,b_n+a_n)\\
					&=(b_1,\ldots,b_n)*(a_1,\ldots,a_n),
				\end{align*}
			where the second equality follows because addition of real numbers is commutative.}
			\item[4.]{$\R-\{0\}$ with the binary operation $a*b=2ab$ is an Abelian group. Let $a,b\in\R-\{0\}$. Then, $a*b=2ab=2ba=b*a$, where the second equality follows because multiplication of real numbers is commutative.}
			\item[5.]{This is not an Abelian group. Consider $A=\begin{pmatrix}0 & 1\\ 1 & 0 \end{pmatrix}$ and $B=\begin{pmatrix}1 & 2\\ 3 & 4 \end{pmatrix}$. Notice that $\det{A}=0\cdot0-1\cdot1=-1$ and $\det{B}=1\cdot4-2\cdot3=4-6=-2$ so both $A$ and $B$ are members of $GL(2,\R)$. However,
				\begin{align*}
					A\cdot B = \begin{pmatrix}0 & 1\\ 1 & 0 \end{pmatrix} \cdot \begin{pmatrix}1 & 2\\ 3 & 4 \end{pmatrix} = \begin{pmatrix}3 & 4\\ 1 & 2 \end{pmatrix},
				\end{align*}
			while
				\begin{align*}
					B\cdot A =  \begin{pmatrix}1 & 2\\ 3 & 4 \end{pmatrix} \cdot \begin{pmatrix}0 & 1\\ 1 & 0 \end{pmatrix}= \begin{pmatrix}2 & 1\\ 4 & 3 \end{pmatrix},
			\end{align*}
			so $A\cdot B \neq B\cdot A$.}
			\item[6.]{This is an Abelian group. Let $f,g\in G$. To show that $G$ is Abelian we must show that $f+g=g+f$. To show the functions $f+g$ and $g+f$ are equal requires us to show that $(f+g)(x)=(g+f)(x)$ for all $x\in\R$. To wit,
				\begin{align*}
					(f+g)(x)=f(x)+g(x)=g(x)+f(x)=(g+f)(x),
				\end{align*}
			where the second equality follows because addition of real numbers is commutative.}
			\item[7.]{This is an Abelian group. Let $A,B\in P(X)$. Then, using the second equivalent definition of symmetric difference on page 12 of the textbook, we have
				\begin{align*}
					A\triangle B = A\cup B - A \cap B = B\cup A - B\cap A = B\triangle A,
				\end{align*}
			where the second equality follows because unions and intersections of sets are commutative operations\footnote{Unlike the prior uses of commutativity of e.g. real numbers, this one is not axiomatic, but still pretty obvious. We provide a heuristic proof here. $A\cup B$ is the set of elements either in $A$ or $B$. The set of elements either in $A$ or $B$ is the same as the set of elements in $B$ or $A$, which is $B\cup A$. $A\cap B$ is the set of elements in both $A$ and $B$. The set of elements in both $A$ and $B$ is the same as the set of elements in $B$ and $A$, which is $B\cap A$.}.}
			\item[8.]{This is an Abelian group. Let $a,b\in\Z_n$. Then suppose that $a+b=kn+c$, where $c\in\Z_n$ (i.e. $0\le c < n$). It follows that $a\bigoplus b = c$, where $\bigoplus$ indicates we are adding modulo $n$. Observe that $b+a=a+b=kn+c$ so it follows that $b\bigoplus a = c$ and hence, $a\bigoplus b=c=b\bigoplus a$.}
		\end{itemize}}
\end{itemize}



\subsection{Q3}

\subsection{Q4}

\begin{itemize}
	\item[(a)]{The table is as follows, each element can be computed directly.

\begin{align*}
	\begin{tabular}{c|cccc}
		& 0 & 1 & 2 & 3 \\
		\hline
		0 & 0 & 1 & 2 & 3 \\
		1 & 1 & 2 & 3 & 0 \\
		2 & 2 & 3 & 0 & 1 \\
		3 & 3 & 0 & 1 & 2 \\
	\end{tabular}
\end{align*}
	
}
	\item[(b)]{The table is as follows, each element can be computed directly.

\begin{align*}
	\begin{tabular}{c|ccccc}
		& 0 & 1 & 2 & 3 & 4\\
		\hline
		0 & 0 & 1 & 2 & 3 & 4\\
		1 & 1 & 2 & 3 & 4 & 0\\
		2 & 2 & 3 & 4 & 0 & 1\\
		3 & 3 & 4 & 0 & 1 & 2\\
		4 & 4 & 0 & 1 & 2 & 3\\
	\end{tabular}	
\end{align*}
	
}
	\item[(c)]{The table is as follows, each element can be computed directly.

\begin{align*}
	\begin{tabular}{c|cccccc}
		& 0 & 1 & 2 & 3 & 4 & 5\\
		\hline
		0 & 0 & 1 & 2 & 3 & 4 & 5\\
		1 & 1 & 2 & 3 & 4 & 5 & 0\\
		2 & 2 & 3 & 4 & 5 & 0 & 1\\
		3 & 3 & 4 & 5 & 0 & 1 & 2\\
		4 & 4 & 5 & 0 & 1 & 2 & 3\\
		5 & 5 & 0 & 1 & 2 & 3 & 4\\
	\end{tabular}
\end{align*}

	
}
\end{itemize}



\subsection{Q5}

No it is not a group. Observe that $a$ is an identity element because (1) $a*a=a$, (2) $a*b=b*a=b$, and (3) $a*c=c*a=c$. However, although $a$ is its own inverse, $b$ and $c$ do not have inverses (there do not exist $x$ such that $x*b=b*x=a$ and $x*c=c*x=a$).



\subsection{Q6}

Not it is not a group. This operation is not associative because $b*(c*b)=b*b=a$ and $(b*c)*b=c*b=b$.



\subsection{Q7}

We already know that by writing down a table with elements from the original set, we have a binary operation.\footnote{The table represents the value of $a*b$ for each $(a,b)\in S \times S$, so it is computable. Moreover, as long as each entry in the table is a member of $S$, we know $*$ always returns a member of $S$.} Consider the table below. Frankly, it was chosen because it is ``nice" in that it is pretty simple and seems reasonable since both $a$ and $b$ can be returned. 

\begin{align*}
\begin{tabular}{c|cc}
	& $a$ & $b$ \\
	\hline
	$a$ & $a$ & $b$\\
	$b$ & $b$ & $a$\\
\end{tabular}
\end{align*}

In any case, an identity must satisfy $a*e=e*a=a$ and $b*e=e*b=b$. Observe that $a*a=a$ so $e=a$ meets the condition that $a*e=e*a=a$ (they are both the same condition when $e=a$). Moreover, $b*a=a*b=b$ so $e=a$ meets the condition that $b*e=e*b=b$. Thus, $e$ serves as an identify. Furthermore, we see that $a$ is its own inverse since $a*a=a$. As for $b$ we need an $x$ such that $b*x=x*b=a$. We know $b*b=a$ from the table so $b$ is also its own inverse (again, the required conditions are the same when $x=b$). Finally for associativity, we need to check for all $x,y,z\in \{a,b\}$ that $(x*y)*z=x*(z*y)$. That is fairly manageable to check directly.

\begin{align*}
	(a*a)*a=a*(a*a) \iff a*a=a*a \iff a = a\\
	(a*a)*b=a*(a*b) \iff a*b=a*b \iff b = b\\
	(a*b)*a=a*(b*a) \iff b*a=a*b \iff b = b\\
	(a*b)*b=a*(b*b) \iff b*b=a*a \iff a = a\\
	(b*a)*a=b*(a*a) \iff b*a=b*a \iff b = b\\
	(b*a)*b=b*(a*b) \iff b*b=b*b \iff a = a\\
	(b*b)*a=b*(b*a) \iff a*a=b*b \iff a = a\\
	(b*b)*b=b*(b*b) \iff a*b=b*a \iff b = b\\
\end{align*}



\subsection{Q8}

Yes this is a group. First, the binary operation can be computed for each pair of functions $(f,g)$ as we simply need to return that function whose value at each point $x\in \R$ is $f(x)\cdot g(x)$, which can be performed by computing the product of real numbers $f(x)$ and $g(x)$. Moreover, this defines a function from $\R$ to $\R$. Moreover, $\times$ is associative because for any $f,g,h\in G$, $(f\times g)\times h=f\times(g\times h)$ if and only if for all $x$ 
\begin{align*}
	(f\times g)(x)\cdot h(x) = f(x)\cdot (g\times h)(x). 
\end{align*}
Of course, this is true because $(f\times g)(x)=f(x)\cdot g(x)$ and $(g\times h)(x)=g(x)\cdot h(x)$. Thus, both the left and right hand sides are $f(x)\cdot g(x) \cdot h(x)$ so $\times$ is associative. 

For there to be an identity element $e$ we would need for all $f\in G$ that $f\times e = f$. That means for all $x\in \R$ that $(f\times e)(x)=f(x)$ or $f(x)\cdot e(x)=f(x)$. Since we require $f(x)\neq 0$ for all $x\in\R$, we obtain that $e(x)=1$ for all $x\in\R$. To completely verify that $e$ is an identity we must also check that $e\times f = f$ for all $f\in G$ (for our candidate $e$). This is so because for all $x\in \R$ we have $f(x)=f(x)$. The left hand side can be re-written as $1\cdot f(x)=f(x)$ and again as $e(x)\cdot f(x)=f(x)$, for all $x\in \R$. This is the meaning of $e\times f = f$ so $e$ is indeed an identity. 

Finally, for each $f\in G$ we need an inverse function $f^\prime$ such that $f\times f^\prime = f^\prime \times f = e$. A natural guess for $f^\prime$ is that function defined such that $f^\prime(x)=\frac{1}{f(x)}$ for all $x\in \R$ (which is well-defined because $f(x)\neq0$ for all $x\in \R$). For this $f^\prime$ we have $f(x)\cdot f^\prime(x)=f(x)\cdot\frac{1}{f(x)}=1=e(x)$, for all $x\in \R$. Moreover, we have $f^\prime(x)\cdot f(x)=\frac{1}{f(x)}\cdot f(x)=1=e(x)$, for all $x\in \R$. Thus, $f^\prime$ is an inverse for each $x$. 



\subsection{Q9}

\begin{itemize}
	\item[(a)]{Let $A=\begin{pmatrix}a_1 & a_2\\ a_3 & a_4 \end{pmatrix}$ and $B=\begin{pmatrix}b_1 & b_2\\ b_3 & b_4 \end{pmatrix}$. Then of course, $\text{det}(A)=a_1a_4-a_2a_3$ and $\text{det}(B)=b_1b_4-b_2b_3$ and
		\begin{align*}
			A\cdot B = \begin{pmatrix}a_1 & a_2\\ a_3 & a_4 \end{pmatrix}\cdot \begin{pmatrix}b_1 & b_2\\ b_3 & b_4 \end{pmatrix} = \begin{pmatrix}a_1 b_1+a_2b_3& a_1b_2+a_2b_4\\ a_3b_1+a_4b_3 & a_3b_2+a_4b_4 \end{pmatrix}.
		\end{align*}
	Thus, 	
		\begin{align*}
			\text{det}(A\cdot B) & = (a_1 b_1+a_2b_3)(a_3b_2+a_4b_4) - (a_1b_2+a_2b_4) (a_3b_1+a_4b_3) \\
			& = a_1b_1a_3b_2+a_1b_1a_4b_4+a_2b_3a_3b_2+a_2b_3a_4b_4-a_1b_2a_3b_1-a_1b_2a_4b_3-a_2b_4a_3b_1-a_2b_4a_4b_3\\
			& = (a_1a_4b_1b_4-a_1a_4b_2b_3)+(a_2a_3b_2b_3-a_2a_3b_1b_4)\\
			&\qquad\qquad\qquad\qquad\qquad\qquad+(a_1a_3b_1b_2-a_1a_3b_1b_2)+(a_2a_4b_3b_4-a_2a_4b_3b_4)\\
			& = a_1a_4(b_1b_4-b_2b_3)-a_2a_3(b_1b_4-b_2b_3)\\
			& = (a_1a_4-a_2a_3)(b_1b_4-b_2b_3)\\
			& = \text{det}(A)\cdot\text{det}(B),\\
		\end{align*}
	exactly as desired.}
\end{itemize}



\subsection{Q10}

Let $A=\begin{pmatrix}a_1 & a_2 \\ -a_2 & a_1 \end{pmatrix}$ and $B=\begin{pmatrix}b_1 & b_2 \\ -b_2 & b_1 \end{pmatrix}$ with both $a_1^2+a_2^2\neq 0$ and $b_1^2+b_2^2\neq 0$. Then,
\begin{align*}
	A\cdot B = \begin{pmatrix}a_1 & a_2 \\ -a_2 & a_1 \end{pmatrix} \cdot \begin{pmatrix}b_1 & b_2 \\ -b_2 & b_1 \end{pmatrix} = \begin{pmatrix}a_1b_1-a_2b_2 & a_1b_2+a_2b_1 \\ -a_2b_1-a_1b_2 & -a_2b_2+a_1b_1 \end{pmatrix}.
\end{align*}
Indeed $A\cdot B$ is in this group because (i) the diagonal entries are equal, (ii) the off-diagonal entries are negatives of each other and (iii) we have
\begin{align*}
	\det{A\cdot B} & = \det{\begin{pmatrix}a_1b_1-a_2b_2 & a_1b_2+a_2b_1 \\ -a_2b_1-a_1b_2 & -a_2b_2+a_1b_1 \end{pmatrix}} \\
	& = (a_1b_1-a_2b_2)(-a_2b_2+a_1b_1)-(-a_2b_1-a_1b_2)(-a_2b_2+a_1b_1)\\
	& = (a_1b_1-a_2b_2)^2+(-a_2b_1-a_1b_2)^2\\
	& = a_1^2b_1^2-2a_1b_1a_2b_2+a_2^2b_2^2+a_2^2b_1^2+2a_ab_1a_2b_2+a_1^2b_2^2\\
	& = a_1^2b_1^2+a_2^2b_2^2+a_2^2b_1^2+a_1^2b_2^2\\
	& = a_1^2b_1^2+a_2^2b_1^2+a_2^2b_2^2+a_1^2b_2^2\\
	& = b_1^2(a_1^2+a_2^2)+b_2^2(a_2^2+a_1^2)\\
	& = (b_1^2+b_2^2)(a_2^2+a_1^2),\\
\end{align*}
which is non-zero because both terms in the product are non-zero by assumption. Thus, we have a valid binary operation on this set of matrices. 



\subsection{Q11}

\subsection{Q12}

\subsection{Q13}

\subsection{Q14}

When $n=1$ we have one set in $A_1\triangle A_2\ldots \triangle A_n$ so we simply have $A_1\triangle A_2\ldots \triangle A_n=A_1$. Thus, if $x\in A_1\triangle A_2\ldots \triangle A_n=A_1$ then $x\in A_1$, which is an odd number of sets. When $n=2$ we have $A_1\triangle A_2$. We have seen that if $x\in A_1\triangle A_2$ it is in \emph{exactly} one of $A_1$ or $A_2$, which is again an odd number.

Suppose that for fixed $n\ge 1$, if $x\in A_1\triangle A_2\ldots \triangle A_n$ then $x\in A_j$ for an odd number of $j$s. Now consider $A_1\triangle A_2\ldots \triangle A_n\triangle A_{n+1}$. Although we can write this unambiguously without parentheses, it is helpful to write this as $(A_1\triangle A_2\ldots \triangle A_n) \triangle A_{n+1}$. Now suppose $x\in(A_1\triangle A_2\ldots \triangle A_n) \triangle A_{n+1}$. Then, we know $x$ is in \emph{exactly} one of $A_1\triangle A_2\ldots \triangle A_n$ or $A_{n+1}$. In the first case, we know $x\notin A_{n+1}$ and $x\in A_1\triangle A_2\ldots \triangle A_n$. by the inductive hypothesis that $x$ is in $A_j$ for an odd number of $j$. In the second case, we know 





\newpage

\section{Fundamental Theorems about Groups}

\subsection{Q1}

In $(\Z_n,\oplus)$, the inverse of $x\neq0$ is $n-x$ and the identity is $0$ (cf. page 22 of the textbook.) We use this fact to cancel the constants on either side of $x$ in the given equation, $2 \oplus x \oplus 7 = 1$. Recalling that we can group the operations in any way we like (cf. Problem 2.13), we begin by canceling the $2$ by adding $10$ to both sides of the equation on the left as
\begin{align*}
	10 \oplus 2 \oplus x \oplus 7 = 10 \oplus 1 \iff 0 \oplus x \oplus 7 = 11 \iff x \oplus 7 = 11.
\end{align*}
Then, we can cancel the $7$ by adding $5$ to both sides of the equation on the right as 
\begin{align*}
	x \oplus 7 \oplus 5 = 11 \oplus 5 \iff x \oplus 0 = 4 \iff x = 4.
\end{align*}
As a check, $2 \oplus 4 \oplus 7 = 6 \oplus 7 = 1$. 



\subsection{Q2}

In $\left(P(X),\triangle\right)$, the inverse of $A$ is $A$ and the identity is $\emptyset$ (cf. page 20 of the textbook.) Using this fact, we can can cancel $A$ from the left hand side of the given equation, $A*x=B$ as
\begin{align*}
	A*A*x=A*B\iff\emptyset * x= A*B\iff x = A*B & = A \triangle B \\
	& = A \cup B - A \cap B \\
	& = \left\{1,4,5,7,8\right\}\cup\left\{2,4,6\right\} - \left\{1,4,5,7,8\right\}\cap\left\{2,4,6\right\} \\
	& = \left\{1,2,4,5,6,7,8\right\}- \left\{4\right\}\\
	& = \left\{1,2,5,6,7,8\right\}.
\end{align*}
As a check 
\begin{align*}
	A*\left\{1,2,5,6,7,8\right\} = A\triangle\left\{1,2,5,6,7,8\right\}  & = A\cup\left\{1,2,5,6,7,8\right\} - A\cap\left\{1,2,5,6,7,8\right\}\\
	& = \left\{1,4,5,7,8\right\}\cup\left\{1,2,5,6,7,8\right\} - \left\{1,4,5,7,8\right\}\cap\left\{1,2,5,6,7,8\right\}\\
	& = \left\{1,2,4,5,6,7,8\right\} - \left\{1,5,7,8\right\}\\
	& = \left\{2,4,6\right\} = B.\\
\end{align*}



\subsection{Q3}

Consider $A = \begin{pmatrix}1 & 2 \\3 & 4\end{pmatrix}$, $B = \begin{pmatrix}0 & 1\\1 & 0\end{pmatrix}$ and $C = \begin{pmatrix}4 & 3 \\2 & 1\end{pmatrix}$. Then,
\begin{align*}
	AB = \begin{pmatrix}1 & 2 \\3 & 4\end{pmatrix} \cdot \begin{pmatrix}0 & 1\\1 & 0\end{pmatrix} = \begin{pmatrix}2 & 1\\4 & 3\end{pmatrix},
\end{align*}
and
\begin{align*}
	BC = \begin{pmatrix}0 & 1\\1 & 0\end{pmatrix} \cdot \begin{pmatrix}4 & 3 \\2 & 1\end{pmatrix}= \begin{pmatrix}2 & 1\\4 & 3\end{pmatrix}.
\end{align*}
Thus, for these three matrices $A,B,C\in GL(2\R)$ we have $AB=BC$, yet $A\neq C$. 

\vspace{\baselineskip}

\noindent (*) One may wonder how to ``derive" these matrices from first principles. There is no direct way to do this, but we can motivate how to think about the above. First, we know $B$ is the most important matrix to get the property $AB=BC$ to hold so we focus on that. To keep it simple, one first considers matrices with 0s and 1s as entries. The identity matrix immediately comes to mind. However, this is not a choice for $B$ because then $AB=BC$ \emph{implies} $A=C$. To try to retain the simplicity, we try flipping and using a matrix with 1s only on the off diagonal (i.e. $B$ above). Finally, if one considers arbitrary matrices $A$ and $C$ and explicitly writes out the four equations derived from $AB=BC$ we find a set of conditions on $C$, given the values of $A$. Specifically, $a_1=c_4$, $a_2=c_3$, $a_3=c_2$, and $a_4=c_1$.  Thus, $A$ can be any matrix you like, and you know immediately what $C$ must be. 



\subsection{Q4}

Because $G$ is a group, each element $x\in G$ has an inverse, $x^{-1}$. Thus, we can left multiply both sides of the equation $x*g=x$ by $x^{-1}$ to obtain 
\begin{align*}
	x^{-1}*x*g=x^{-1}*x\iff e*g=e \iff g = e. 
\end{align*}



\subsection{Q5}

First, we apply Theorem 3.4 with $x:=x$ and $y:=y*z$. We have
\begin{align*}
	\left(x*y*z\right)^{-1}=\left(x*(y*z)\right)^{-1}=\left(y*z\right)^{-1}*x^{-1}.
\end{align*}
Second, applying Theorem 3.4 with $x:=y$ and $y:=z$. We have
\begin{align*}
	\left(y*z\right)^{-1}=z^{-1}*y^{-1}.
\end{align*}
Finally, putting these two equations together we have
\begin{align*}
	\left(x*y*z\right)^{-1}=\left(y*z\right)^{-1}*x^{-1}=\left(z^{-1}*y^{-1}\right)*x^{-1}=z^{-1}*y^{-1}*x^{-1}.
\end{align*}



\subsection{Q6}

Let $(G,*)$ be a group and $x,y,z\in G$. To prove (i) we assume $x*y=x*z$ and we must show $y=z$. Because $x\in G$ there exists $x^{-1}$ such that $x*x^{-1}=x^{-1}*x=e$, where $e$ is the identity element in $G$. From $x*y=x*z$ we can left multiply both sides by $x^{-1}$ to obtain $x^{-1}*(x*y)=x^{-1}*(x*z)$. Using associativity we obtain $(x^{-1}*x)*y=(x^{-1}*x)*z$. That implies $e*y=e*z$, which in turn implies $y=z$, exactly as desired. To prove (ii) we assume $y*x=z*x$. This time, we can right multiply both sides by $x^{-1}$ to obtain $(y*x)*x^{-1}=(z*x)*x^{-1}$. Using associativity we obtain $y*(x*x^{-1})=z*(x*x^{-1})$. That implies $y*e=z*e$, which in turn implies $y=z$, exactly as desired. 



\subsection{Q7}

Suppose for some row there is an element, say $x$, such that it appears more than once in that row. Further suppose that this row represents the left multiplication table for $a$. I.e. reading this row gives you the values of $a*g$ for all $g$ in $G$. Thus, the assumption that $x$ appears more than once means there must be more than one solution $g$ to $a*g=x$. This implies there must exist elements $y$ and $z$ with $y\neq z$ such that $a*y=a*z$ because both sides equal $x$. However, the left Cancellation Law implies $y=z$ contradicting our assumption that for some row there is an element appearing more than once. Now suppose for some row there is an element that never appears in that row. By the Pigeonhole Principle there must be some other element that appears more than once in that row and we have already seen that is not possible. Thus, it also cannot be possible that for some row there is an element that never appears in that row or appears more than once. Finally, we can conclude that every element of $G$ occurs precisely once in each row of the table. 

The proof that every element of $G$ occurs precisely once in each column of the table is exactly the same as the above proof replacing the word ``row" with the word ``column." These two facts together comprise our desired result.

\vspace{\baselineskip}

\noindent (*) We can spell out the application of the Pigeonhole Principle more carefully. Suppose $|G|=n$. We are assumping for some row there is an element that never appears in that row. Suppose further that each of the other $n-1$ elements appear at most once in each row. Then there are at most $n-1$ entries to fill a row that needs to contain $n$ total entries. Thus, it cannot be the case that the other $n-1$ elements appear at most once and there is some other element that appears more than once.



\subsection{Q8}

\begin{itemize}
	\item[(a)]{By way of contradiction assume there are two identity elements $e$ and $e^\prime$ with $e\neq e^\prime$. Because $e$ is an identity, for all $x\in G$, $e*x=x*e=x$. Setting $x=e^\prime$ we have (i) $e*e^\prime=e^\prime*e=e^\prime$. Similarly, because $e^\prime$ is an identity, for all $x\in G$, $e^\prime*x=x*e^\prime=x$. Setting $x=e^\prime$ we have (ii) $e^\prime*e^\prime=e^\prime$. Thus, observe that $e*e^\prime=e^\prime*e^\prime$ because both sides are $e^\prime$. (The left hand side equals $e^\prime$ by (i) and the right hand side equals $e^\prime$ by (ii).) Finally using the right Cancellation Law we can cancel $e^\prime$ from both sides to obtain $e=e^\prime$, contradicting that $e\neq e^\prime$. Thus, there is only one identity element in $G$.}
	\item[(b)]{Suppose $x,y\in G$. Then there exist $x^{-1},y^{-1}$ such that $x*x^{-1}=x^{-1}*x=e$ and $y*y^{-1}=y^{-1}*y=e$, where $e$ is the identity element in $G$. Thus, we can write down the equation $x*x^{-1}=y*y^{-1}$ because both sides equal $e$. Now we use the assumption $x^{-1}=y^{-1}$ and denote their common value as $z$. Thus, we can write $x*z=y*z$ and use the right Cancellation Law to cancel $z$ and obtain $x=y$, exactly as desired.}
\end{itemize}



\subsection{Q9}

$\left(\implies\right)$ 

We know from Theorem 3.4 that for any $x,y\in G$, $\left(x*y\right)^{-1}=y^{-1}*x^{-1}$. Since $x^{-1},y^{-1}\in G$, the fact that $(G,*)$ is abelian implies $y^{-1}*x^{-1}=x^{-1}*y^{-1}$. Thus, the fact that $(G,*)$ is abelian implies for all $x,y\in G$, $\left(x*y\right)^{-1}=y^{-1}*x^{-1}=x^{-1}*y^{-1}.$

\noindent $\left(\impliedby\right)$ 

Let $x,y\in G$ then we have $\left(x*y\right)^{-1}=x^{-1}*y^{-1}$ (by assumption). However, by Theorem 3.4, the left hand side expands as $\left(x*y\right)^{-1}=y^{-1}*x^{-1}$. We conclude that for all $x,y\in G$ that $x^{-1}*y^{-1}=y^{-1}*x^{-1}$ (both being equal to $\left(x*y\right)^{-1}$.) By Theorem 3.2, the inverse is unique so we can take the inverse of both sides to obtain 
\begin{align*}
	\left(x^{-1}*y^{-1}\right)^{-1}=\left(y^{-1}*x^{-1}\right)^{-1} & \implies \left(y^{-1}\right)^{-1}* \left(x^{-1}\right)^{-1}=\left(x^{-1}\right)^{-1}* \left(y^{-1}\right)^{-1}\\
	&\implies y*x = x*y.
\end{align*}
The first implication is due to Theorem 3.4, while the second is due to Theorem 3.3. Thus, we have shown for all $x,y\in G$ we have $x*y=y*x$. I.e. $(G,*)$ is abelian. 

\subsection{Q10}

Let $(G,*)$ be a group and $g\in G$ be a fixed element. Denote $\widetilde{G}:=\{g*x:x\in G\}$. We will show $G=\widetilde{G}$ by showing (i) $G\subseteq\widetilde{G}$ and  (ii) $\widetilde{G}\subseteq G$. 
\begin{itemize}
	\item[(i)]{Suppose $y\in G$. To show that $y\in\widetilde{G}$, we must find an $x\in G$ such that $y=g*x$. Because $g\in G$, we know there is a $g^{-1}\in G$ such that $g*g^{-1}=g^{-1}*g=e$, where $e$ is the identity element in $G$. Left multiplying the equation $y=g*x$ by $g^{-1}$ yields $g^{-1}*y=g^{-1}*(g*x)=(g^{-1}*g)*x=e*x=x$. Thus, for $x=g^{-1}*y$, we can write $y=g*x$. Thus, $y\in\widetilde{G}$.}
	\item[(ii)]{Suppose $y\in\widetilde{G}$. By definition of $\widetilde{G}$, there is an $x\in G$ such that $y=x*g$. However, because $*$ is a binary operation and $x,g\in G$ we know $y=x*g\in G$.}
\end{itemize}



\subsection{Q11}

Let $x,y\in G$. Then, by definition of $G$ we have $x*x=y*y=e$. Observe that this fact immediately implies $x^{-1}=x$ and $y^{-1}=y$. To show that $x*y=y*x$ it is equivalent to show that $(x*y)^{-1}*(y*x)=e$. By Theorem 3.4, for all $x,y\in G$, $(x*y)^{-1}=y^{-1}*x^{-1}$. Thus, we can expand the left hand side as $(x*y)^{-1}*(y*x)=(y^{-1}*x^{-1})*(y*x)=(y*x)*(y*x)=(y*x)^2=e$. The final equality holds because $*$ is a binary operation so $z:=x*y\in G$ and by definition of $G$, $z^2=(y*x)^2=e$.



\subsection{Q12}

($\implies$) Suppose $x,y\in G$ and that $G$ is Abelian. Then,
\begin{align*}
	(x*y)^2=(x*y)*(x*y)=x*(y*x)*y=x*(x*y)*y=(x*x)*(y*y)=x^2*y^2,
\end{align*}
exactly as desired. (Note the use of $G$ being Abelian in the third equality.)

\vspace{\baselineskip}

\noindent ($\impliedby$) Suppose $x,y\in G$. By assumption $(x*y)^2=x^2*y^2$. Writing this out entirely, we have 
\begin{align*}
	(x*y)*(x*y)=(x*x)*(y*y) \implies ((x*y)*x)*y=((x*x)*y)*y & \implies (x*y)*x=(x*x)*y \\
	& \implies x*(y*x)=x*(x*y) \\
	& \implies y*x=x*y, \\	
\end{align*}
exactly as desired. (Note the use of the Cancellation Laws in the second and fourth implications to (i) right cancel $y$ and (ii) left cancel $x$.)

\vspace{\baselineskip}

\noindent (*) Cf. Problem 2.13 for why we can move parentheses however we like.



\subsection{Q13}

The assumption that there is a left identity, $e$ means for every $g\in G$ we have $e*g=g$. The assumption that every element has a left inverse means for every $g\in G$ there is a $g^{-1}$ such that $g^{-1}*g=e$. Similar to the proof of Theorem 3.7, we must show (i) that $e$ is also a right identity (that is, for every $g\in G$, we have $g*e=g$) and (ii) that $g^{-1}$ is also a right inverse (that is, $g*g^{-1}=e$). 

\begin{itemize}
	\item[(i)]{Letting $g=e$ in the assumption for a left identity means $e*e=e$. Consider an arbitary $x\in G$. Then there is an $x^{-1}$ such that $x^{-1}*x=e$. We can insert this into the equation $e*e=e$ to obtain $(x^{-1}*x)*e=x^{-1}*x$. Using associativity we obtain $x^{-1}*(x*e)=x^{-1}*x$. Since $x^{-1}\in G$ it also has a left inverse, say $y$, such that $y*x^{-1}=e$. Left multiplying by $y$ we have $y*(x^{-1}*(x*e))=y*(x^{-1}*x)$. Again using associativity we obtan $(y*x^{-1})*(x*e)=(y*x^{-1})*x$. Thus we obtain $e*(x*e)=e*x$. Because $e$ is a left identity we have $x*e=x$ for all $x\in G$. I.e., $e$ is a right identity.}
	\item[(ii)]{Consider any $x\in G$ and let $x^{-1}$ be the left inverse. We know that $x^{-1}*x=e$ and we want to show that $x*x^{-1}=e$. We know there is also a left inverse of $x^{-1}$, say $y$, for which we can write $y*x^{-1}=e$. Left multiplying both sides of $x^{-1}*x=e$ by $y$ we obtain $y*(x^{-1}*x)=y$. Using associativity we obtain $(y*x^{-1})*x=y$. Because $y$ is the left inverse of $x^{-1}$ we have $e*x=y$ or $x=y$ (because $e$ is a left identity.) Finally, from the equation $y*x^{-1}=e$ and $y=x$ we have $x*x^{-1}=e$. I.e. $x^{-1}$ is the right inverse of $x$.}
\end{itemize}



\subsection{Q14}

\subsection{Q15}

\subsection{Q16}

An example is $(\Z^{+},\cdot)$. This is the set of positive integers together with multiplication. Clearly $\Z^{+}$ is not finite. Let $x,y,z\in\Z^{+}$, and suppose $x\cdot y = x\cdot z$. Since $x>0$ we can divide both sides by $x$ to obtain $y=z$ so the left Cancellation Law holds. Now suppose $y\cdot x = z\cdot x$. Again, since $x>0$ we can divide both sides by $x$ to obtain $y=z$ so the right Cancellation Law holds. However, as discussed on page 17 of the textbook, ``$(\Z^{+},\cdot)$ is \emph{not} a group, since no element other 1 has an inverse."

\vspace{\baselineskip}

\noindent (*) In trying to solve this problem we should consider examples of sets together with operations that we already know are \emph{not} groups. Specifically here we looked back at the second set of examples on page 17 of the textbook. There are two reasons this set is relevant here. First, the examples are infinite sets so that meets one of the conditions of the problem. Second, the operation is multiplication and this is amenable to thinking about the Cancellation Laws and whether or not they hold. 



\subsection{Q17}

The first condition tells us there is a unique value $e\in G$ such that for all $x\in G$, $x*e=x$ (unique right identity). The second condition tells us for any $x\in G$ there is an $x^\prime\in G$ such that $x^\prime * x = e$ (existence of a left inverse). We will show that $x^\prime$ is also a right inverse. Then Theorem 3.7 tells us that $(G,*)$ is a group. That is because we already have a right identity and our proof will demonstrate that every element has a right inverse and those are the conditions of Theorem 3.7. 

By way of contradiction, assume that $x*x^\prime = z$, where $z\in G$ is not equal to $e$. Left multiplying both sides of this equation by $x^\prime$, we have
\begin{align*}
	x^\prime*(x*x^\prime) = x^\prime*z \implies (x^\prime*x)*x^\prime = x^\prime*z \implies e*x^\prime = x^\prime* z \implies x^\prime * z = x^\prime,
\end{align*}
where the first implication is due to associativity, the second is due to $x^\prime$ being a left inverse, and the final one due to $e$ being the unique right identity and a little re-arrangement. Now, the equation $x^\prime * z = x^\prime$ implies that $z$ is a right identity different from $e$, which is a contradiction of the uniqueness of the right identity. Thus, it must be the case that $x*x^\prime = e$ so that $x^\prime$ is a right inverse, exactly as desired. 



\newpage

\section{Powers of an Element; Cyclic Groups}

We use the following definition for the first two problems. In general, for a group $G$ and an element $g\in G$, we have $\langle g \rangle:=\{g^n:n\in\Z\}$, where $g^n:=\underbrace{g*g*g*\ldots*g}_{n\text{ times}}$.

\subsection{Q1}

In the case of $(\Z_{10},+)$, $\langle g \rangle=\{ng:n\in\Z,\text{ and }0\le ng<10\}$, where the added condition guarantees that we only deal with elements within $\Z_{10}$. Thus,
\begin{itemize}
	\item{$\langle 0 \rangle=\{n\cdot0:n\in\Z,\text{ and }0\le n\cdot0<10\}=\{0:n\in\Z,\text{ and }0\le 0<10\}=\{0:n\in\Z\}=\{0\}$, where the second to last equality follows because the statement $0\le 0 < 10$ is always true, independent of $n$. The final equality is because the typical term preceding the colon does not vary with $n$.}
	\item{$\langle 1 \rangle=\{n\cdot1:n\in\Z,\text{ and }0\le n\cdot1<10\}=\{n:n\in\Z,\text{ and }0\le n<10\}=\{0,1,2,3,4,5,6,7,8,9\}$ because the typical term enumerates all integers, and the second condition requires those integers be between 0 (inclusive) and 10 (exclusive).}
	\item{$\langle 2 \rangle=\{n\cdot2:n\in\Z,\text{ and }0\le n\cdot2<10\}=\{2n:n\in\Z,\text{ and }0\le 2n<10\}=\{0,2,4,6,8\}$ because the typical term enumerates all multiples of $2$ so we list all multiples of $2$ between 0 (inclusive) and 10 (exclusive).}
	\item{$\langle 3 \rangle=\{n\cdot3:n\in\Z,\text{ and }0\le n\cdot3<10\}=\{3n:n\in\Z,\text{ and }0\le 3n<10\}=\{0,3,6,9\}$ because the typical term enumerates all multiples of $3$ so we list all multiples of $3$ between 0 (inclusive) and 10 (exclusive).}
	\item{$\langle 4 \rangle=\{n\cdot4:n\in\Z,\text{ and }0\le n\cdot4<10\}=\{4n:n\in\Z,\text{ and }0\le 4n<10\}=\{0,4,8\}$ because the typical term enumerates all multiples of $4$ so we list all multiples of $4$ between 0 (inclusive) and 10 (exclusive).}
	\item{$\langle 5 \rangle=\{n\cdot5:n\in\Z,\text{ and }0\le n\cdot5<10\}=\{5n:n\in\Z,\text{ and }0\le 5n<10\}=\{0,5\}$ because the typical term enumerates all multiples of $5$ so we list all multiples of $5$ between 0 (inclusive) and 10 (exclusive).}
\end{itemize}

\vspace{\baselineskip}

\noindent (*) Note we could also have simplified some of the expressions to figure out which values of $n$ to use and then list the typical terms based on this. For example, we had that $\langle 2 \rangle=\{2n:n\in\Z,\text{ and }0\le 2n<10\}=$. The second condition can be re-written $0\le n < 5$. Thus, the condition says $n\in\{0,1,2,3,4\}$. We insert those to the typical term of $2n$ to obtain $\{0,2,4,6,8\}$.





\subsection{Q2}

In the case of $(G,+)$, where $G$ is the set of all real valued functions on the real line under addition of functions, $\langle g \rangle:=\{ng:n\in\Z\}$. Here $ng$ denotes a function such that $(ng)(x) = n\cdot g(x)$ for all $x\in\R$.  Therefore, with $g=f$ as given in the problem (defined such that $f(x)=1$ for all $x$), we have $(nf)(x)=n\cdot f(x)=n\cdot1=n$ for all $x$. Thus, the set $\langle f \rangle$ is simply the set of constant, integer valued functions. It follows that the configuration from drawing all such functions on one set of axes would be a set of horizontal straight lines, each spaced 1 unit apart vertically from the closest other lines. 



\subsection{Q3}

\subsection{Q4}

In the case of $(\Z_{30},+)$, we write $g^n$ as $ng$ and the identity element is $0$ (cf. page 22 of the textbook). Thus, we seek the smallest $n>0$ such that $ng\equiv0(\text{mod }{30})$ or equivalently, such that $ng$ is a multiple of 30.
\begin{itemize}
	\item{Setting $g=3$ we have $ng=n\cdot3=3n$. Since $n>0$, $3n>0$ so we consider positive multiples of 30. Can $3n=30$? Yes, for $n=10$. Thus, $o(3)=10$.}
	\item{Setting $g=4$ we have $ng=n\cdot4=4n$. Since $n>0$, $4n>0$ so we consider positive multiples of 30. Can $4n=30$? No, because then $n=15/2$ which is not an integer. How about 60? Yes, for $n=15$. Thus, $o(4)=15$.}
	\item{Setting $g=6$ we have $ng=n\cdot6=6n$. Similar to $g=3$ we observe that 6 divides 30 so certainly it is possible for $6n=30$, and it is for $n=5$. Thus, $o(6)=5$}
	\item{Setting $g=7$ we have $ng=n\cdot7=7n$. In order for $7n$ to be a multiple of . Thus, $o(3)=10$.}
	\item{Setting $g=18$ we have $ng=n\cdot3=3n$. Since $n>0$, $3n>0$ so we consider positive multiples of 30. Can $3n=30$? Yes, for $n=10$. Thus, $o(3)=10$.}
\end{itemize}



\subsection{Q5}

We can apply part (iii) of Theorem 4.4 with $n=18$ and $m$ equal to each of the values given. 
\begin{itemize}
	\item[($m=2$)]{We have $d=(n,m)=(18,2)=2$. Thus, $o(x^2)=n/m=18/2=9$. Moreover, $(x^2)^9=x^{18}=e$.}
	\item[($m=3$)]{We have $d=(n,m)=(18,3)=3$. Thus, $o(x^3)=n/m=18/3=6$. Moreover, $(x^3)^6=x^{18}=e$.}
	\item[($m=4$)]{We have $d=(n,m)=(18,4)=2$. Thus, $o(x^4)=n/m=18/2=9$. Moreover, $(x^4)^9=x^{36}=(x^{18})^2=e^2=e$.}
	\item[($m=5$)]{We have $d=(n,m)=(18,5)=1$. Thus, $o(x^5)=n/m=18/1=18$. Moreover, $(x^5)^{18}=x^{90}=(x^{18})^5=e^5=e$.}
	\item[($m=12$)]{We have $d=(n,m)=(18,12)=6$. Thus, $o(x^{12})=n/m=18/6=3$. Moreover, $(x^{12})^3=x^{36}=(x^{18})^2=e^2=e$.}
\end{itemize}



\subsection{Q6}

An element $g\in\Z_{45}$ has order 15 if $15g=0\mod{45}$. Or put another way, we need $15g=45k$ for some $k\in\Z$. This is equivalent to having $g=3k$ for some $k\in\Z$. Thus, the complete set of elements of order 15 in $\Z_{45}$ are those elements elements $3k$ such that $0\le 3k < 45$. In particular this requires $0\le k < 15$. That gives us the following set of elements of order 15:
\begin{align*}
	\{0,3,6,9,12,15,18,21,24,27,30,33,36,39,42\}.
\end{align*}



\subsection{Q7}

\subsection{Q8}

\subsection{Q9}

The proof of this is very similar to the proof that $(\Q,+)$ is not cyclic on page 39 of the textbook. The only difference is that our special case is $q=1$ rather than $q=0$. $q=1$ could not be a generator of the group because $1^i=1$ for all $i$ so it cannot generate all positive rational numbers. 

Now suppose that some positive $q\neq1$ were a generator for $(\Q^+,\cdot)$. If $q>1$, then $q^i > q$ for all $i>1$ and $q^i < q$ for all $i<1$. In particular $q^2 > q$ so that $\frac{q^2+q}{2}$ is a positive rational number\footnote{This is so because the positive rationals are closed under multiplication so $q^2$ is a positive rational. The sum of two positive rational numbers is a positive rational number so $q+q^2$ is a positive rational number. Finally the positive rational numbers are closed under multiplication so multiplying $q+q^2$ and $\frac{1}{2}$ yields a positive rational number.} halfway between $q$ and $q^2$. Thus, it is obviously not equal to $q^i$ for $i\in\{1,2\}$. Moreover, since $q^i\ge q^2>\frac{q^2+q}{2}$ for all $i\ge 2$, $\frac{q^2+q}{2}$ cannot be equal to $q^i$ for any $i\ge 1$. Finally, since $q^i \le q < \frac{q^2+q}{2}$ for all $i\le 1$, $\frac{q^2+q}{2}$ cannot be equal to $q^i$ for any $i\le 1$. Consequently, no $q>1$ can be a generator for $(\Q^+,\cdot)$.

A symmetric argument applies if $q < 1$. In this case $q^i < q$ for all $i>1$ and $q^i > q$ for all $i<1$. Thus, $\frac{q+q^2}{2}$ will be a positive rational halfway between $q^2$ and $q$ (this time with $q^2<q$.) Thus, it is obviously not equal to $q^i$ for $i\in\{1,2\}$. Moreover, since $q^i \le q^2$ for all $i\ge 2$, $\frac{q+q^2}{2}$ cannot be equal to $q^i$ for any $i\ge 1$. Finally, since $q^i \ge q > \frac{q+q^2}{2}$ for all $i\le 1$, $\frac{q+q^2}{2}$ cannot be equal to $q^i$ for any $i\le 1$. Thus, there can be no generator $q<1$ either and we finally conclude that $(\Q^+,\cdot)$ is not cyclic. 




\subsection{Q10}

\subsection{Q11}

\subsection{Q12}

Consider the sequence of elements $\{a^n\}_{n=-\infty}^\infty$. We have $a^0=e$, the identity element in this group. What is the identity in this group? An identity $e$ must satisfy $a*e=a$ for all $a\in G$. Thus, $a+e-1=a$, which implies $e=1$. Indeed $a*1=a+1-1=a$ and $1*a=1+a-1=a$. Next we ask, what is $a^{-1}$? It must satisfy $a*a^{-1}=1$. Thus, $a+a^{-1}-1=1$, which implies $a^{-1}=2-a$. 




where $a_0=a\in G$ and for all $n\ge1$, $a_{n}=a_{n-1}*a_{n-1}=a_{n-1}+a_{n-1}-1=2a_{n-1}-1$. Observe that
\begin{align*}
	a_n=2a_{n-1}-1=2\left(2a_{n-2}-1\right)-1=4a_{n-2}-2-1&=4\left(2a_{n-3}-1\right)-2-1\\
	&=8a_{n-3}-4-2-1,
\end{align*}
where in the first, second, and fourth equalities we used the recursive definitiona of $a_n$, $a_{n-1}$, and $a_{n-2}$, respectively. The equations suggest that $a_n=2^na-\sum_{j=0}^{n-1}2^j$, which we shall prove by induction. 

When $n=0$ the left hand side is $a_0=a$, while the right hand side is $2^0a-\sum_{j=0}^{0-1}2^j=a$. Suppose that $a_n=2^na-\sum_{j=0}^{n-1}2^j$ for fixed $n$. Then using the recursive definition of $a_n$, we have
\begin{align*}
	a_{n+1}=2a_{n}-1=2\left(2^na-\sum_{j=0}^{n-1}2^j\right)-1=2^{n+1}a-\sum_{j=0}^{n-1}2^{j+1}-1=2^{n+1}a-\sum_{j=1}^{n}2^{j}-1=2^{n+1}a-\sum_{j=0}^{n}2^{j},
\end{align*}
which is exactly the equation for $n+1$. 

Recall also from the formula for a finite geometric series we have $\sum_{j=0}^{n-1}2^j=\frac{1-2^n}{1-2}=2^n-1$. Therefore, the formula for $a_n$ simplifies to $a_n=2^na-\sum_{j=0}^{n-1}2^j=a_n=2^na-(2^n-1)=2^n(a-1)+1$. Finally, we can solve the problem. 

What is 
For $\left(\Z,*\right)$ to be a cyclic group we must be able to find an element $a$ such that $\langle a \rangle=\Z$. As we have just shown, $\langle a \rangle=\left\{a^n:n\in\Z\right\}$. 



\subsection{Q13}

Let $g$ be an arbitrary element of a finite group $G$. Then consider the sequence of elements $g,g^2,g^3,\ldots$ Observe that each $g^i\in G$. This is clearly true when $i=1$ because $g\in G$. Now suppose it is true for arbitrary $i$ that $g^i \in G$. Then, $g^{i+1}:=g*g^i$ is an element of $G$ because the right hand side is the binary operation $*$ applied to a pair of elements, $g$ and $g^i$, both of which are in $G$. Thus, $g^{i+1}\in G$ and the induction is complete. 

With that observation we can now show that there must be a repeat element at some point in the sequence $g,g^2,g^3,\ldots$ In fact, by the pigeonhole principle, $g^{|G|+1}$ must be equal to some $g^i$ with $i\le |G|$. If that were not the case, then $g,g^2,\ldots, g^{|G|},g^{|G|+1}$ would be a set of $|G|+1$ unique elements all of which belong to $G$. However, there are only $|G|$ such unique elements so this cannot be possible. Thus, we have $g^i = g^{|G|+1}$, which we can write as $g^i * e= g^{|G|+1} * e$. We can apply the left Cancellation Law to this equation $i$ times to obtain $e = g^{|G|+1-i} * e$ or $g^{|G|+1-i} = e$ which shows that $g$ has finite order. Because $g$ was arbitrary, we conclude that every element of a finite group must have finite order. 

\vspace{\baselineskip}

\noindent (*) It is worth noting that $|G|+1-i\ge 1$ because $1\le i \le |G|$. That implies that $-|G| \le -i \le -1$, which in turn implies $1 \le |G|+1-i \le |G|$. Thus, we have not only shown that $g$ has finite order (because $|G|+1-i\ge 1$), but we have also shown that the order of $g$ is at most $|G|$.



\subsection{Q14}

\subsection{Q15}

\subsection{Q16}

\subsection{Q17}

\subsection{Q18}

\begin{itemize}
	\item[(ii)]{The statement of this part of the Theorem is equivalent to saying that the inverse of $x^n$ is $x^{-n}$. To wit,
	\begin{align*}
		x^n * x^{-n} = x^{n+(-n)}=x^{n-n}=x^0=e,
	\end{align*}
	and
	\begin{align*}
		x^{-n} * x^n = x^{(-n)+n}=x^{-n+n}=x^0=e,
	\end{align*}
	where we used the first part of the Theorem to add exponents. These equations are sufficient to show that the inverse of $x^n$ is $x^{-n}$.}
	\item[(iii)]{We take a different approach and prove (a) $(x^m)^n=x^{nm}$ (for all $n,m$) and (b) $x^{nm}=(x^n)^m$ (for all $n,m$) by induction on $n$ for an arbitrary $m$.
	\begin{itemize}
		\item[(a)]{When $n=1$ the left hand side is $(x^m)^n=(x^m)^1=x^m$ and the right hand side is $x^{nm}=x^{1\cdot m}=x^m$. Now suppose that $(x^m)^n=x^{nm}$ and multiply both sides by $x^m$. The left hand side simply becomes $(x^m)^{n+1}$ since we have an additional copy of $x^m$. The right hand side is $x^m*x^{nm}=x^{m+nm}=x^{(n+1)m}$. Thus, $(x^m)^{n+1}=x^{(n+1)m}$, which is exactly the statement for $n+1$.}
		\item[(b)]{}
	\end{itemize}}
\end{itemize}

\subsection{Q19}

\subsection{Q20}

\subsection{Q21}

\subsection{Q22}

\subsection{Q23}

\subsection{Q24}

\subsection{Q25}

\subsection{Q26}

\subsection{Q27}

\subsection{Q28}

\subsection{Q29}

\subsection{Q30}

\subsection{Q31}

\subsection{Q32}

\subsection{Q33}

\end{document}